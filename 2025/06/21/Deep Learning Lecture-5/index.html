<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Deep Learning Lecture-5 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Recurrent NetworkSequence Model序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是捕捉序列中的上下文。 Basic PrincipleLocal DependencyLocal Dependency：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元素的。这种依赖关系">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Lecture-5">
<meta property="og:url" content="http://example.com/2025/06/21/Deep%20Learning%20Lecture-5/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Recurrent NetworkSequence Model序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是捕捉序列中的上下文。 Basic PrincipleLocal DependencyLocal Dependency：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元素的。这种依赖关系">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250320225821.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321083321.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321091923.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321091936.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321102900.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321110238.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321130443.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321130842.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321133148.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321135457.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321140454.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321141243.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250321141234.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250322202759.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250322204925.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250322222251.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250323084415.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250323084332.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250323084535.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250323090740.png">
<meta property="og:image" content="d:/software/Git/images/Pasted%20image%2020250323092327.png">
<meta property="article:published_time" content="2025-06-20T16:00:00.000Z">
<meta property="article:modified_time" content="2025-06-24T06:55:00.404Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="d:/software/Git/images/Pasted%20image%2020250320225821.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">





<script>
  MathJax = {
    tex: {
      // vvvvv  请务必确认存在这一行 vvvvv
      packages: {'[+]': ['ams']},
      // ^^^^^  请务必确认存在这一行 ^^^^^
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Deep Learning Lecture-5" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/06/21/Deep%20Learning%20Lecture-5/" class="article-date">
  <time class="dt-published" datetime="2025-06-20T16:00:00.000Z" itemprop="datePublished">2025-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Deep Learning Lecture-5
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<h2 id="Recurrent-Network"><a href="#Recurrent-Network" class="headerlink" title="Recurrent Network"></a>Recurrent Network</h2><h3 id="Sequence-Model"><a href="#Sequence-Model" class="headerlink" title="Sequence Model"></a>Sequence Model</h3><p>序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是<strong>捕捉序列中的上下文</strong>。</p>
<h3 id="Basic-Principle"><a href="#Basic-Principle" class="headerlink" title="Basic Principle"></a>Basic Principle</h3><h4 id="Local-Dependency"><a href="#Local-Dependency" class="headerlink" title="Local Dependency"></a>Local Dependency</h4><p><strong>Local Dependency</strong>：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元素的。这种依赖关系是<strong>局部的</strong>。</p>
<p>$$<br>P(x_1,x_2,\dots,x_T) &#x3D; \prod_{t&#x3D;1}^{T} P(x_t|x_1,\dots,x_{t-1}) &#x3D;<br>\prod_{t&#x3D;1}^{T} g(s_{t-2},x_{t-1})<br>$$</p>
<p>如果引入马尔科夫性，那么损失的信息太多了。因此我们引入一个隐藏状态$s_t$，$s_t$的信息是前面的元素的信息的一个编码。<br>假设第$t$时间的元素的信息都被编码到了$s_{t-2}$和$x_{t-1}$中，$s_{t-2}$是一个向量，$g$是一个函数，$s_{t-2}$是一个隐藏状态。我们认为时间上也存在一个感受野。这个思考过程和人也是类似的，在处理序列问题时，也保存早些时候的一些信息。</p>
<p>另外也有时间上的<strong>平稳性假设</strong>，这个假设是人工智能能预测未来的理论基础。本质是时间上的独立同分布假设。</p>
<h4 id="Parametric-Sharing"><a href="#Parametric-Sharing" class="headerlink" title="Parametric Sharing"></a>Parametric Sharing</h4><p>不同时刻使用的参数是一样的，这样可以大大降低参数量。这样的模型是<strong>循环神经网络</strong>。</p>
<h4 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h4><p>语言中的建模任务为：给定一个句子的前面的单词，预测下一个单词。这个任务被称为语言模型。语言模型的目标是最大化句子的概率。语言模型的输入是一个句子，输出是一个概率分布，表示下一个单词的概率。</p>
<p><strong>向量化表达</strong>：可以使用one-hot向量表示单词，也可以使用词向量表示单词。每个词的表达大概需要1000维度。如果使用MLP，要将每个词的向量拼接起来，然后输入到MLP中。这样的模型不适合处理长序列，参数是非常可怕的。在MLP中丢失了一部分的信息（由于MLP的输入维度是可交换的），丢失了前后序关系。</p>
<h4 id="A-Better-MLP"><a href="#A-Better-MLP" class="headerlink" title="A Better MLP"></a>A Better MLP</h4><p><strong>n-gram</strong>：使用n-gram模型，可以考虑前n个单词的信息。</p>
<p><img src="D:/software/Git/images/Pasted%20image%2020250320225821.png" alt="Pasted image 20250320225821.png"><br>输出的概率分布是一个softmax层，输入是一个向量，这个向量是前n个单词的向量拼接起来的。然后在得到的概率分布上进行采样。<strong>滚动预测</strong>，使用第一个次的预测结果作为第二次的输入，会有<strong>误差累计</strong>的问题。<br>但是上述模型会有一个问题：参数量太大；对于每一个词的预测需要有一个MLP，这样的模型不适合处理长序列。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p><img src="D:/software/Git/images/Pasted%20image%2020250321083321.png" alt="Pasted image 20250321083321.png"></p>
<p>循环神经网络中最重要的内容是中间的隐藏层，构建学习到的时间程度上的特征。在每个时刻输入的都是向量，称为<em>token embedding</em>。<br>上述模型在纵向方向上，从$x_t$到$y_t$就是一个前馈网络<em>feedforward network</em>（包括MLP和CNN）。在横向方向上，从$s_{t-1}$到$s_t$就是一个循环网络<em>recurrent network</em>。</p>
<h4 id="Recurrent-Layer"><a href="#Recurrent-Layer" class="headerlink" title="Recurrent Layer"></a>Recurrent Layer</h4><p>$h_t$用来编码$t$时刻之前的所有信息。对于这样的层，接受的输入是$x_t$和$h_{t-1}$，输出是$h_t$。$h_t$的计算公式如下：<br>$$<br>h_t &#x3D; f_W(h_{t-1},x_t)<br>$$<br>$$<br>h_t &#x3D; \tanh (Wh_{t-1} + Ux_t)<br>$$<br>长期以来使用的是双曲正切作为激活函数，但是使用ReLu可能有更好的梯度性质。<br>$$<br>y_t &#x3D; Vh_t<br>$$<br>可以认为$h_t$包含了之前的所有信息，所以使用$h_t$来预测$y_t$。<strong>通过引入状态变量来使得递推公式在形式上是二阶依赖。</strong></p>
<h4 id="Bidirectional-Deep-RNN"><a href="#Bidirectional-Deep-RNN" class="headerlink" title="Bidirectional&amp;Deep RNN"></a>Bidirectional&amp;Deep RNN</h4><p><img src="D:/software/Git/images/Pasted%20image%2020250321091923.png" alt="Pasted image 20250321091923.png"></p>
<p><img src="D:/software/Git/images/Pasted%20image%2020250321091936.png" alt="Pasted image 20250321091936.png"></p>
<p>这里纵向方向上的前馈网络中的训练难点在前面的MLP与CNN中是一样的，梯度消失和梯度爆炸。<br>上面的图中的$y_{t,c}$是真是标签的独热编码，$C$是此表中的元素个数。</p>
<p>在横向方向上，梯度消失是很严重的。因为$h_t$包含了$h_{t-1}$的信息，所以梯度会在时间上指数级的衰减。解决这个问题的方法是<strong>LSTM</strong>和<strong>GRU</strong>。</p>
<h4 id="RNN-for-LM"><a href="#RNN-for-LM" class="headerlink" title="RNN for LM"></a>RNN for LM</h4><ul>
<li>理论上，可以表达没有边界的时间上的依赖，由于将状态变量编码为$h_t$，$h_t$包含了之前的所有信息。</li>
<li>将序列编码到一个向量中，这个向量包含了整个序列的信息。</li>
<li>参数在时间上是共享的</li>
<li>但是在实际上，很难建模时间上的长时间依赖。对于较早的信息，后面的权重会很小。</li>
</ul>
<p><strong>一个模型是否有效，在于<em>assumptions</em>与实际情况是否匹配。</strong></p>
<h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p><img src="D:/software/Git/images/Pasted%20image%2020250321102900.png" alt="Pasted image 20250321102900.png"></p>
<h5 id="Many-to-One"><a href="#Many-to-One" class="headerlink" title="Many to One"></a>Many to One</h5><p>主要实现的是情感识别、文本分类等任务。在最后一个时间步的输出是最终的输出。最后一个时刻的状态不一定包含有重要的信息（比如上下文中的情感词）。</p>
<h5 id="One-to-Many"><a href="#One-to-Many" class="headerlink" title="One to Many"></a>One to Many</h5><p>可能的输入有，比如输入一个图像的特征向量再输入到这个网络中。对于这个输入，应该是输入每个状态还是输入所有的状态。并没有解决每一个词对应图中的哪一个区域的问题。</p>
<h5 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many to Many"></a>Many to Many</h5><p>有两种情况，输入是每个时刻的语音的因素，输出的是对应的symbol，是输入输出平行<em>parallel</em>的，但是输入和输出是异构的。</p>
<p>LM输入和输出是基本上平行的，但是滞后一个时刻，是自回归<em>autoregressive</em>的。</p>
<h5 id="Sequence-to-Sequence"><a href="#Sequence-to-Sequence" class="headerlink" title="Sequence to Sequence"></a>Sequence to Sequence</h5><p>输入和输出都是序列，输入和输出的长度不一定相同。比如机器翻译、语音识别等任务。这个任务可以分为两个部分：编码器和解码器。编码器将输入序列编码到一个向量中，解码器将这个向量解码到输出序列中。</p>
<p><img src="D:/software/Git/images/Pasted%20image%2020250321110238.png" alt="Pasted image 20250321110238.png"></p>
<p><em>为什么上述序列中的参数$W_1$和$W_2$是不一样的？</em></p>
<p>首先将输入变量$\{x_t\}$编码到状态变量$\{h_t\}$中，然后再将状态变量$\{h_t\}$解码到输出变量$\{y_t\}$中。编码器是没有loss function的，因为输入和输出是异构的。解码器接受的输入是编码器的输出，解码器的输出是一个概率分布。解码器的loss function是交叉熵损失函数。</p>
<p><strong>机器翻译任务中的挑战</strong>：</p>
<ul>
<li>输入和输出是异构的</li>
<li>长序列的处理</li>
</ul>
<p><img src="D:/software/Git/images/Pasted%20image%2020250321130443.png" alt="Pasted image 20250321130443.png"></p>
<p><strong>如何从概率中采样</strong>：</p>
<ul>
<li>选择概率最大的</li>
<li>概率较大的有更大的概率被选择</li>
<li>Beam Search贪心方法进行搜索</li>
</ul>
<p><img src="D:/software/Git/images/Pasted%20image%2020250321130842.png" alt="Pasted image 20250321130842.png"></p>
<p>可以选择概率较大的k个词，然后以这个词为条件计算下一个词的条件概率，类似于构建一个真k叉树，这样的方法是一种贪心的方法。考虑这棵树上所有的路径，选择最大的路径（本质上是一种搜索技术）。</p>
<h3 id="Backpropagation-Through-Time"><a href="#Backpropagation-Through-Time" class="headerlink" title="Backpropagation Through Time"></a>Backpropagation Through Time</h3><p><img src="D:/software/Git/images/Pasted%20image%2020250321133148.png" alt="Pasted image 20250321133148.png"></p>
<p>$$<br>\frac{\partial L}{\partial U}  &#x3D; \sum_{t&#x3D;0}^T \frac{\partial L_{t}}{\partial U} &#x3D; \sum_{t&#x3D;0}^{T} \sum_{s&#x3D;0}^t \frac{\partial L_{t}}{\partial y_t} \frac{\partial y_t}{\partial h_t}  \frac{\partial h_t}{\partial h_s}\frac{\partial h_s}{\partial U}<br>$$<br>前一个求和的意义是对于损失函数的各个部分求和，后面的求和式是对于$h_t$的前面的每一个可能的链求和。<br>其中：<br>$$<br>\frac{\partial h_t}{\partial h_s} &#x3D; \prod_{i&#x3D;s+1}^t \frac{\partial h_i}{\partial h_{i-1}}<br>$$<br>这个式子是一个矩阵乘法，是一个矩阵的连乘。这个矩阵是一个雅可比矩阵。</p>
<p><img src="D:/software/Git/images/Pasted%20image%2020250321135457.png" alt="Pasted image 20250321135457.png"></p>
<p>用<em>Cauchy-Schwarz</em>不等式可以证明：<br>$$<br>\| \frac{\partial h_t}{\partial h_{t-1}} \| \leq \| W^T \| \|diag (f&#39;(h_{t-1}))\| \leq \sigma_{max} \gamma<br>$$<br>这里$\sigma_{max}$是矩阵$W$的最大奇异值，$\gamma$是激活函数的导数的最大值。<br>于是：<br>$$<br>\| \frac{\partial h_t}{\partial h_{s}} \| &#x3D;  \prod_{i&#x3D;s+1}^t \| \frac{\partial h_i}{\partial h_{i-1}} \| \leq (\sigma_{max} \gamma)^{t-s}<br>$$<br>这个式子说明了梯度消失的问题，梯度消失是指梯度在时间上的指数级衰减。或者梯度爆炸的问题，梯度爆炸是指梯度在时间上的指数级增长。</p>
<h4 id="Truncated-BPTT"><a href="#Truncated-BPTT" class="headerlink" title="Truncated BPTT"></a>Truncated BPTT</h4><p><img src="D:/software/Git/images/Pasted%20image%2020250321140454.png" alt="Pasted image 20250321140454.png"></p>
<p>这个方法是将时间上的梯度截断，这样可以减少梯度消失和梯度爆炸的问题。但是这样的方法会导致梯度的估计不准确，因为梯度的估计是基于一个截断的时间窗口的。</p>
<h4 id="Long-Short-Term-Memory"><a href="#Long-Short-Term-Memory" class="headerlink" title="Long Short-Term Memory"></a>Long Short-Term Memory</h4><blockquote>
<p>为什么这样就能实现所谓的LSTM</p>
</blockquote>
<ul>
<li>遗忘，将过去“没用”的信息遗忘</li>
<li>更新，将新的信息更新到状态变量中</li>
<li>输出，输出门控制一部分信息用来进行预测<br><img src="D:/software/Git/images/Pasted%20image%2020250321141243.png" alt="Pasted image 20250321141243.png"><br>$t$时刻的状态变量$h_t$储存的是$t$时刻的信息，$c_t$是$t$时刻的记忆变量，$h_{t-1}$和$x_t$是$t$时刻的输入，$f_t$是遗忘门，$i_t$是输入门，$o_t$是输出门，$g_t$是更新门。</li>
</ul>
<p><img src="D:/software/Git/images/Pasted%20image%2020250321141234.png" alt="Pasted image 20250321141234.png"><br>上述网络构造了一个信息流高速路径，使得梯度能够进行快速的传播。</p>
<p>遗忘门和残差网络的思想是类似的，都是将过去的信息和现在的信息进行融合。这样的网络可以更好的处理长序列的问题。[[Deep Learning Lecture-3#ResNet]]</p>
<h4 id="Gradient-Clipping"><a href="#Gradient-Clipping" class="headerlink" title="Gradient Clipping"></a>Gradient Clipping</h4><p>梯度的大小是由模长决定的，如果梯度的模长过大，可以将梯度的模长进行截断。这样可以避免梯度爆炸的问题。</p>
<h4 id="Variational-Dropout"><a href="#Variational-Dropout" class="headerlink" title="Variational Dropout"></a>Variational Dropout</h4><p>在深度网络中，如果是过拟和的，也就是对于一个含有多个参数的网络。也就是说如果输入的参数小于参数的个数，那么相对应的线性方程组是欠定的。</p>
<p>在RNN中对应的纵向方向上是多层感知机，所以可以采用标准的Dropout方法。但是在横向方向上是一个循环网络，Dropout方法不适用。因为Dropout方法会破坏时间上的连续性，违背了参数共享的原则。<br>采用<strong>步调一致</strong>的方法进行操作，这样可以保持时间上的连续性。这样的方法是<strong>Variational Dropout</strong>。</p>
<h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><p><img src="D:/software/Git/images/Pasted%20image%2020250322202759.png" alt="Pasted image 20250322202759.png"></p>
<p>在CNN中，对于每一个通道的值进行归一化。在这样的每一个通道中计算均值和方差。还是要加入一个平移变量和伸缩变量。</p>
<p>在RNN中，主要的原因是门控结构是相对于每一个序列而言的，所以应该引入一种新的归一化方法<em>Layer Normalization</em>，应该在每一条样本（一个序列）在每一个时刻经过之后的值在$C$个通道上进行归一化操作。<br>最后得到的结果是：将所有的向量放在以原点为球心的单位球面上。</p>
<h4 id="Weight-Normalization"><a href="#Weight-Normalization" class="headerlink" title="Weight Normalization"></a>Weight Normalization</h4><p>对于每层的参数$\mathbf{w}$进行重参数化：<br>$$<br>\mathbf{w} &#x3D; \frac{g}{\|v\|}v<br>$$<br><img src="D:/software/Git/images/Pasted%20image%2020250322204925.png" alt="Pasted image 20250322204925.png"></p>
<p>对右边的式子$\frac{v}{\|v\|}$进行优化是更为容易的，重参数化的意思是，在训练和测试的时候使用不同的参数表达形式，这样是更加容易优化的。</p>
<h4 id="Training-Inference-Shift"><a href="#Training-Inference-Shift" class="headerlink" title="Training-Inference Shift"></a>Training-Inference Shift</h4><p>滚动预测：在训练的时候，使用真实的标签进行预测；在测试的时候，使用的是推理得到的值进行预测。这是一个自回归任务。</p>
<h5 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h5><p>在训练的时候，可以先训练一些简单的任务，然后再训练一些复杂的任务。这样可以更好的训练模型。<br>在实际中，可以对所有的样本计算loss，先计算loss较小的样本，然后再计算loss较大的样本。这就是<strong>自步学习</strong>。<br>这里涉及到选择不同的样本顺序的问题。</p>
<p><em>Scheduled Sampling</em>：在学习刚开始的时候，更多地使用真实的标签进行预测；随着学习的进行，更多地使用模型预测的值进行预测。<strong>是RNN中很重要的技术</strong>。</p>
<h3 id="RNN-with-Attention"><a href="#RNN-with-Attention" class="headerlink" title="RNN with Attention"></a>RNN with Attention</h3><h4 id="Human-Attention"><a href="#Human-Attention" class="headerlink" title="Human Attention"></a>Human Attention</h4><p>人类的注意力：</p>
<ul>
<li>可持续注意力（没有实现）</li>
<li>选择性注意力（人类的选择性注意力复杂得多）</li>
<li>交替式注意力</li>
<li>分配式注意力</li>
</ul>
<h4 id="Attention-in-Deep-Learning"><a href="#Attention-in-Deep-Learning" class="headerlink" title="Attention in Deep Learning"></a>Attention in Deep Learning</h4><blockquote>
<p><em>Allowing the model to dynamically pay attention to only certain parts of the input that help in performing the task at hand effectively.</em></p>
</blockquote>
<p>存在时间<em>temporal Attention</em>和空间<em>Spatial Attention</em>上的注意力。一般而言指的是时间上的注意力。</p>
<h4 id="Auto-Regessive"><a href="#Auto-Regessive" class="headerlink" title="Auto-Regessive"></a>Auto-Regessive</h4><p>[[#Sequence to Sequence]]</p>
<p>最重要的问题是编码器和解码器之间的信息沟通太少了，存在有信息瓶颈。并且在翻译任务中，输入和输出的顺序并不是一致的，大部分的语言的语序是不一样的。<br><strong>希望看到后面的信息</strong>，这和RNN的设计目的是相违背的。<strong>全连接的思想</strong>又回来了，获得全局信息的方法有很多，不只是有MLP的方法。有一种基本思想是<em><strong>Relevance</strong></em>，也就是和当前任务相关的信息。这个思想是在Attention中得到了体现。</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p><img src="D:/software/Git/images/Pasted%20image%2020250322222251.png" alt="Pasted image 20250322222251.png"></p>
<p>计算两个东西的相似度有：计算内积、输入<em>relation network</em>。这样的模型在互联网中有很多的应用，比如推荐系统、搜索引擎等。</p>
<p><img src="D:/software/Git/images/Pasted%20image%2020250323084415.png" alt="Pasted image 20250323084415.png"></p>
<p>注意力的分配是符合概率分布的，所以可以使用上面计算得到的相关性$e_{ij}$使用softmax函数进行归一化。这样得到的分布就是注意力的分布：<br>$$<br>\alpha_{ij} &#x3D; \frac{\exp(e_{ij})}{\sum_{k&#x3D;1}^{T_x} \exp(e_{ik})}<br>$$<br>上述计算式表达的含义是，在状态$i$的时刻分配在$j$上的注意力（对于$j$的求和为1）。继续计算$c_i$：<br>$$<br>c_i &#x3D; \sum_{j&#x3D;1}^{T_x} \alpha_{ij} x_j<br>$$<br>这里$c_i$是对于状态$i$的时刻的注意力向量，是对于$x$的加权和。<br>$$<br>s_i &#x3D; f(s_{i-1},y_{i-1},c_i)<br>$$<br>这里$s_i$是状态变量，$y_{i-1}$是前一个时刻的输出，$c_i$是当前时刻的注意力向量。这里的函数$f$是一个GRU or LSTM。这个模型是一个<strong>Seq2Seq</strong>模型。</p>
<blockquote>
<p>这里的$c_i$和$s_i$的区别是什么，为什么和LSTM有关</p>
</blockquote>
<p><img src="D:/software/Git/images/Pasted%20image%2020250323084332.png" alt="Pasted image 20250323084332.png"><br><img src="D:/software/Git/images/Pasted%20image%2020250323084535.png" alt="Pasted image 20250323084535.png"></p>
<p>在机器翻译的过程中，使用source的上下文信息比直接使用词典来翻译更好。这样的模型可以更好的处理长序列的问题。只要是序列都会使用<strong>滑动窗口</strong>，一般设置为50~100之间。对于较短的情况，可以使用psdding的方法；对于较长的情况会使用截断的方法。希望找一个与序列的长度线性关系的模型。</p>
<h4 id="Attention-vs-MLP"><a href="#Attention-vs-MLP" class="headerlink" title="Attention vs. MLP"></a>Attention vs. MLP</h4><p>相同点：</p>
<ul>
<li>都是全局模型，是对于长序关系的建模。</li>
</ul>
<p>不同点：</p>
<ul>
<li>Attention是基于概率的，MLP是基于全连接的。</li>
<li>Attention引入relevance的思想，能大大减小参数量</li>
</ul>
<h4 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h4><p>先建模词注意力然后再建模句子注意力。</p>
<h4 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h4><p>$$<br>\text{score} &#x3D; \begin{cases}<br>h_t^T \overline{h_s} \\<br>h_t^T W_a \overline{h_s} \\<br>v_a^T \tanh(W_a[h_t;\overline{h_s}])<br>\end{cases}<br>$$<br>发现上面三种计算方式的效率是差不多的。</p>
<p><img src="D:/software/Git/images/Pasted%20image%2020250323090740.png" alt="Pasted image 20250323090740.png"></p>
<h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><h4 id="Human-Memory"><a href="#Human-Memory" class="headerlink" title="Human Memory"></a>Human Memory</h4><ul>
<li>Sensory Memory<ul>
<li>计算机视觉与机器感知</li>
</ul>
</li>
<li>Short-term Memory<ul>
<li>与计算机中的内存是很相近的，LSTM是一种将短期记忆尽量变长的方法。</li>
</ul>
</li>
<li>Long-term Memory<ul>
<li>前面的模型中没有实现这个功能</li>
</ul>
</li>
</ul>
<p>在自然语言中，比较困难的任务是进行对话，这时候需要进行长期记忆。在对话中，需要对话的上下文进行理解。</p>
<h4 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h4><p><img src="D:/software/Git/images/Pasted%20image%2020250323092327.png" alt="Pasted image 20250323092327.png"></p>
<p>在这个模型中，最重要的是对内存1进行寻址的操作，这个操作是一个注意力的操作。<br>对于读的操作，是按照注意力的大小对地址里面的内容进行加权平均。<br>对于写的操作，类似于LSTM中的Forget Gate，先进行擦除之后才进行写入。</p>
<p>对于Internal Memory，最大的问题是可能会遗忘，对于External Memory，是一个外部的存储器，这样的存储是比较稳定的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/06/21/Deep%20Learning%20Lecture-5/" data-id="cmca67x5t0005j4vaef6gaxgu" data-title="Deep Learning Lecture-5" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/" rel="tag">DeepLearning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/06/21/Deep%20Learning%20Lecture-4/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Deep Learning Lecture-4
        
      </div>
    </a>
  
  
    <a href="/2025/06/21/Deep%20Learning%20Lecture-6/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Deep Learning Lecture-6</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/" rel="tag">DeepLearning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/DeepLearning/" style="font-size: 10px;">DeepLearning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/06/21/Deep%20Learning%20Lecture-1/">Deep Learning Lecture-1</a>
          </li>
        
          <li>
            <a href="/2025/06/21/Deep%20Learning%20Lecture-2/">Deep Learning Lecture-2</a>
          </li>
        
          <li>
            <a href="/2025/06/21/Deep%20Learning%20Lecture-3/">Deep Learning Lecture-3</a>
          </li>
        
          <li>
            <a href="/2025/06/21/Deep%20Learning%20Lecture-4/">Deep Learning Lecture-4</a>
          </li>
        
          <li>
            <a href="/2025/06/21/Deep%20Learning%20Lecture-5/">Deep Learning Lecture-5</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>