

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Yima Gu">
  <meta name="keywords" content="">
  
    <meta name="description" content="Recurrent Network Sequence Model 序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是捕捉序列中的上下文。 Basic Principle Local Dependency Local Dependency：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Lecture-5">
<meta property="og:url" content="https://yima-gu.github.io/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-5/index.html">
<meta property="og:site_name" content="Yima Gu\&#39;s Blog">
<meta property="og:description" content="Recurrent Network Sequence Model 序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是捕捉序列中的上下文。 Basic Principle Local Dependency Local Dependency：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250320225821.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321083321.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321091923.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321091936.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321102900.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321110238.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321130443.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321130842.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321133148.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321135457.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321140454.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250321141234.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250322202759.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250322204925.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250322222251.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250323084415.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250323084332.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250323090740.png">
<meta property="og:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250323092327.png">
<meta property="article:published_time" content="2025-06-20T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-03T09:17:14.587Z">
<meta property="article:author" content="Yima Gu">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yima-gu.github.io/images/Pasted%20image%2020250320225821.png">
  
  
  
  <title>Deep Learning Lecture-5 - Yima Gu\&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yima-gu.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Yima Gu&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/post.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Deep Learning Lecture-5"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-06-21 00:00" pubdate>
          2025年6月21日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          33 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Deep Learning Lecture-5</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="recurrent-network">Recurrent Network</h2>
<h3 id="sequence-model">Sequence Model</h3>
<p>序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是<strong>捕捉序列中的上下文</strong>。</p>
<h3 id="basic-principle">Basic Principle</h3>
<h4 id="local-dependency">Local Dependency</h4>
<p><strong>Local
Dependency</strong>：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元素的。这种依赖关系是<strong>局部的</strong>。</p>
<p><span class="math display">$$
P(x_1,x_2,\dots,x_T) = \prod_{t=1}^{T} P(x_t|x_1,\dots,x_{t-1}) =
\prod_{t=1}^{T} g(s_{t-2},x_{t-1})
$$</span></p>
<p>如果引入马尔科夫性，那么损失的信息太多了。因此我们引入一个隐藏状态<span
class="math inline"><em>s</em><sub><em>t</em></sub></span>，<span
class="math inline"><em>s</em><sub><em>t</em></sub></span>的信息是前面的元素的信息的一个编码。
假设第<span
class="math inline"><em>t</em></span>时间的元素的信息都被编码到了<span
class="math inline"><em>s</em><sub><em>t</em> − 2</sub></span>和<span
class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>中，<span
class="math inline"><em>s</em><sub><em>t</em> − 2</sub></span>是一个向量，<span
class="math inline"><em>g</em></span>是一个函数，<span
class="math inline"><em>s</em><sub><em>t</em> − 2</sub></span>是一个隐藏状态。我们认为时间上也存在一个感受野。这个思考过程和人也是类似的，在处理序列问题时，也保存早些时候的一些信息。</p>
<p>另外也有时间上的<strong>平稳性假设</strong>，这个假设是人工智能能预测未来的理论基础。本质是时间上的独立同分布假设。</p>
<h4 id="parametric-sharing">Parametric Sharing</h4>
<p>不同时刻使用的参数是一样的，这样可以大大降低参数量。这样的模型是<strong>循环神经网络</strong>。</p>
<h4 id="language-model">Language Model</h4>
<p>语言中的建模任务为：给定一个句子的前面的单词，预测下一个单词。这个任务被称为语言模型。语言模型的目标是最大化句子的概率。语言模型的输入是一个句子，输出是一个概率分布，表示下一个单词的概率。</p>
<p><strong>向量化表达</strong>：可以使用one-hot向量表示单词，也可以使用词向量表示单词。每个词的表达大概需要1000维度。如果使用MLP，要将每个词的向量拼接起来，然后输入到MLP中。这样的模型不适合处理长序列，参数是非常可怕的。在MLP中丢失了一部分的信息（由于MLP的输入维度是可交换的），丢失了前后序关系。</p>
<h4 id="a-better-mlp">A Better MLP</h4>
<p><strong>n-gram</strong>：使用n-gram模型，可以考虑前n个单词的信息。</p>
<p><img src="/images/Pasted%20image%2020250320225821.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250320225821.png" />
输出的概率分布是一个softmax层，输入是一个向量，这个向量是前n个单词的向量拼接起来的。然后在得到的概率分布上进行采样。<strong>滚动预测</strong>，使用第一个次的预测结果作为第二次的输入，会有<strong>误差累计</strong>的问题。
但是上述模型会有一个问题：参数量太大；对于每一个词的预测需要有一个MLP，这样的模型不适合处理长序列。
### RNN</p>
<figure>
<img src="/images/Pasted%20image%2020250321083321.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321083321.png" />
<figcaption aria-hidden="true">Pasted image
20250321083321.png</figcaption>
</figure>
<p>循环神经网络中最重要的内容是中间的隐藏层，构建学习到的时间程度上的特征。在每个时刻输入的都是向量，称为<em>token
embedding</em>。 上述模型在纵向方向上，从<span
class="math inline"><em>x</em><sub><em>t</em></sub></span>到<span
class="math inline"><em>y</em><sub><em>t</em></sub></span>就是一个前馈网络<em>feedforward
network</em>（包括MLP和CNN）。在横向方向上，从<span
class="math inline"><em>s</em><sub><em>t</em> − 1</sub></span>到<span
class="math inline"><em>s</em><sub><em>t</em></sub></span>就是一个循环网络<em>recurrent
network</em>。 #### Recurrent Layer <span
class="math inline"><em>h</em><sub><em>t</em></sub></span>用来编码<span
class="math inline"><em>t</em></span>时刻之前的所有信息。对于这样的层，接受的输入是<span
class="math inline"><em>x</em><sub><em>t</em></sub></span>和<span
class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>，输出是<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>。<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>的计算公式如下：
<span
class="math display"><em>h</em><sub><em>t</em></sub> = <em>f</em><sub><em>W</em></sub>(<em>h</em><sub><em>t</em> − 1</sub>, <em>x</em><sub><em>t</em></sub>)</span>
<span
class="math display"><em>h</em><sub><em>t</em></sub> = tanh (<em>W</em><em>h</em><sub><em>t</em> − 1</sub> + <em>U</em><em>x</em><sub><em>t</em></sub>)</span>
长期以来使用的是双曲正切作为激活函数，但是使用ReLu可能有更好的梯度性质。
<span
class="math display"><em>y</em><sub><em>t</em></sub> = <em>V</em><em>h</em><sub><em>t</em></sub></span>
可以认为<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>包含了之前的所有信息，所以使用<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>来预测<span
class="math inline"><em>y</em><sub><em>t</em></sub></span>。<strong>通过引入状态变量来使得递推公式在形式上是二阶依赖。</strong></p>
<h4 id="bidirectionaldeep-rnn">Bidirectional&amp;Deep RNN</h4>
<figure>
<img src="/images/Pasted%20image%2020250321091923.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321091923.png" />
<figcaption aria-hidden="true">Pasted image
20250321091923.png</figcaption>
</figure>
<figure>
<img src="/images/Pasted%20image%2020250321091936.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321091936.png" />
<figcaption aria-hidden="true">Pasted image
20250321091936.png</figcaption>
</figure>
<p>这里纵向方向上的前馈网络中的训练难点在前面的MLP与CNN中是一样的，梯度消失和梯度爆炸。
上面的图中的<span
class="math inline"><em>y</em><sub><em>t</em>, <em>c</em></sub></span>是真是标签的独热编码，<span
class="math inline"><em>C</em></span>是此表中的元素个数。</p>
<p>在横向方向上，梯度消失是很严重的。因为<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>包含了<span
class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>的信息，所以梯度会在时间上指数级的衰减。解决这个问题的方法是<strong>LSTM</strong>和<strong>GRU</strong>。</p>
<h4 id="rnn-for-lm">RNN for LM</h4>
<ul>
<li>理论上，可以表达没有边界的时间上的依赖，由于将状态变量编码为<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>，<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>包含了之前的所有信息。</li>
<li>将序列编码到一个向量中，这个向量包含了整个序列的信息。</li>
<li>参数在时间上是共享的</li>
<li>但是在实际上，很难建模时间上的长时间依赖。对于较早的信息，后面的权重会很小。</li>
</ul>
<p><strong>一个模型是否有效，在于<em>assumptions</em>与实际情况是否匹配。</strong></p>
<h4 id="architecture">Architecture</h4>
<figure>
<img src="/images/Pasted%20image%2020250321102900.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321102900.png" />
<figcaption aria-hidden="true">Pasted image
20250321102900.png</figcaption>
</figure>
<h5 id="many-to-one">Many to One</h5>
<p>主要实现的是情感识别、文本分类等任务。在最后一个时间步的输出是最终的输出。最后一个时刻的状态不一定包含有重要的信息（比如上下文中的情感词）。</p>
<h5 id="one-to-many">One to Many</h5>
<p>可能的输入有，比如输入一个图像的特征向量再输入到这个网络中。对于这个输入，应该是输入每个状态还是输入所有的状态。并没有解决每一个词对应图中的哪一个区域的问题。</p>
<h5 id="many-to-many">Many to Many</h5>
<p>有两种情况，输入是每个时刻的语音的因素，输出的是对应的symbol，是输入输出平行<em>parallel</em>的，但是输入和输出是异构的。</p>
<p>LM输入和输出是基本上平行的，但是滞后一个时刻，是自回归<em>autoregressive</em>的。
##### Sequence to Sequence</p>
<p>输入和输出都是序列，输入和输出的长度不一定相同。比如机器翻译、语音识别等任务。这个任务可以分为两个部分：编码器和解码器。编码器将输入序列编码到一个向量中，解码器将这个向量解码到输出序列中。</p>
<figure>
<img src="/images/Pasted%20image%2020250321110238.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321110238.png" />
<figcaption aria-hidden="true">Pasted image
20250321110238.png</figcaption>
</figure>
<p><em>为什么上述序列中的参数<span
class="math inline"><em>W</em><sub>1</sub></span>和<span
class="math inline"><em>W</em><sub>2</sub></span>是不一样的？</em></p>
<p>首先将输入变量<span
class="math inline">{<em>x</em><sub><em>t</em></sub>}</span>编码到状态变量<span
class="math inline">{<em>h</em><sub><em>t</em></sub>}</span>中，然后再将状态变量<span
class="math inline">{<em>h</em><sub><em>t</em></sub>}</span>解码到输出变量<span
class="math inline">{<em>y</em><sub><em>t</em></sub>}</span>中。编码器是没有loss
function的，因为输入和输出是异构的。解码器接受的输入是编码器的输出，解码器的输出是一个概率分布。解码器的loss
function是交叉熵损失函数。</p>
<p><strong>机器翻译任务中的挑战</strong>： - 输入和输出是异构的 -
长序列的处理</p>
<figure>
<img src="/images/Pasted%20image%2020250321130443.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321130443.png" />
<figcaption aria-hidden="true">Pasted image
20250321130443.png</figcaption>
</figure>
<p><strong>如何从概率中采样</strong>： - 选择概率最大的 -
概率较大的有更大的概率被选择 - Beam Search贪心方法进行搜索</p>
<figure>
<img src="/images/Pasted%20image%2020250321130842.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321130842.png" />
<figcaption aria-hidden="true">Pasted image
20250321130842.png</figcaption>
</figure>
<p>可以选择概率较大的k个词，然后以这个词为条件计算下一个词的条件概率，类似于构建一个真k叉树，这样的方法是一种贪心的方法。考虑这棵树上所有的路径，选择最大的路径（本质上是一种搜索技术）。</p>
<h3 id="backpropagation-through-time">Backpropagation Through Time</h3>
<figure>
<img src="/images/Pasted%20image%2020250321133148.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321133148.png" />
<figcaption aria-hidden="true">Pasted image
20250321133148.png</figcaption>
</figure>
<p><span class="math display">$$
\frac{\partial L}{\partial U}  = \sum_{t=0}^T \frac{\partial
L_{t}}{\partial U} = \sum_{t=0}^{T} \sum_{s=0}^t \frac{\partial
L_{t}}{\partial y_t} \frac{\partial y_t}{\partial h_t}  \frac{\partial
h_t}{\partial h_s}\frac{\partial h_s}{\partial U}
$$</span>
前一个求和的意义是对于损失函数的各个部分求和，后面的求和式是对于<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>的前面的每一个可能的链求和。
其中： <span class="math display">$$
\frac{\partial h_t}{\partial h_s} = \prod_{i=s+1}^t \frac{\partial
h_i}{\partial h_{i-1}}
$$</span>
这个式子是一个矩阵乘法，是一个矩阵的连乘。这个矩阵是一个雅可比矩阵。</p>
<figure>
<img src="/images/Pasted%20image%2020250321135457.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321135457.png" />
<figcaption aria-hidden="true">Pasted image
20250321135457.png</figcaption>
</figure>
<p>用<em>Cauchy-Schwarz</em>不等式可以证明： <span
class="math display">$$
\| \frac{\partial h_t}{\partial h_{t-1}} \| \leq \| W^T \| \|diag
(f'(h_{t-1}))\| \leq \sigma_{max} \gamma
$$</span> 这里<span
class="math inline"><em>σ</em><sub><em>m</em><em>a</em><em>x</em></sub></span>是矩阵<span
class="math inline"><em>W</em></span>的最大奇异值，<span
class="math inline"><em>γ</em></span>是激活函数的导数的最大值。 于是：
<span class="math display">$$
\| \frac{\partial h_t}{\partial h_{s}} \| =  \prod_{i=s+1}^t \|
\frac{\partial h_i}{\partial h_{i-1}} \| \leq (\sigma_{max}
\gamma)^{t-s}
$$</span>
这个式子说明了梯度消失的问题，梯度消失是指梯度在时间上的指数级衰减。或者梯度爆炸的问题，梯度爆炸是指梯度在时间上的指数级增长。</p>
<h4 id="truncated-bptt">Truncated BPTT</h4>
<figure>
<img src="/images/Pasted%20image%2020250321140454.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321140454.png" />
<figcaption aria-hidden="true">Pasted image
20250321140454.png</figcaption>
</figure>
<p>这个方法是将时间上的梯度截断，这样可以减少梯度消失和梯度爆炸的问题。但是这样的方法会导致梯度的估计不准确，因为梯度的估计是基于一个截断的时间窗口的。</p>
<h4 id="long-short-term-memory">Long Short-Term Memory</h4>
<blockquote>
<p>为什么这样就能实现所谓的LSTM</p>
</blockquote>
<ul>
<li>遗忘，将过去“没用”的信息遗忘</li>
<li>更新，将新的信息更新到状态变量中</li>
<li>输出，输出门控制一部分信息用来进行预测 <img
src="/images/Pasted%20image%2020250321141243.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321141243.png" /> <span
class="math inline"><em>t</em></span>时刻的状态变量<span
class="math inline"><em>h</em><sub><em>t</em></sub></span>储存的是<span
class="math inline"><em>t</em></span>时刻的信息，<span
class="math inline"><em>c</em><sub><em>t</em></sub></span>是<span
class="math inline"><em>t</em></span>时刻的记忆变量，<span
class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>和<span
class="math inline"><em>x</em><sub><em>t</em></sub></span>是<span
class="math inline"><em>t</em></span>时刻的输入，<span
class="math inline"><em>f</em><sub><em>t</em></sub></span>是遗忘门，<span
class="math inline"><em>i</em><sub><em>t</em></sub></span>是输入门，<span
class="math inline"><em>o</em><sub><em>t</em></sub></span>是输出门，<span
class="math inline"><em>g</em><sub><em>t</em></sub></span>是更新门。</li>
</ul>
<p><img src="/images/Pasted%20image%2020250321141234.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250321141234.png" />
上述网络构造了一个信息流高速路径，使得梯度能够进行快速的传播。</p>
<p>遗忘门和残差网络的思想是类似的，都是将过去的信息和现在的信息进行融合。这样的网络可以更好的处理长序列的问题。[[Deep
Learning Lecture-3#ResNet]]</p>
<h4 id="gradient-clipping">Gradient Clipping</h4>
<p>梯度的大小是由模长决定的，如果梯度的模长过大，可以将梯度的模长进行截断。这样可以避免梯度爆炸的问题。</p>
<h4 id="variational-dropout">Variational Dropout</h4>
<p>在深度网络中，如果是过拟和的，也就是对于一个含有多个参数的网络。也就是说如果输入的参数小于参数的个数，那么相对应的线性方程组是欠定的。</p>
<p>在RNN中对应的纵向方向上是多层感知机，所以可以采用标准的Dropout方法。但是在横向方向上是一个循环网络，Dropout方法不适用。因为Dropout方法会破坏时间上的连续性，违背了参数共享的原则。
采用<strong>步调一致</strong>的方法进行操作，这样可以保持时间上的连续性。这样的方法是<strong>Variational
Dropout</strong>。</p>
<h4 id="layer-normalization">Layer Normalization</h4>
<figure>
<img src="/images/Pasted%20image%2020250322202759.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250322202759.png" />
<figcaption aria-hidden="true">Pasted image
20250322202759.png</figcaption>
</figure>
<p>在CNN中，对于每一个通道的值进行归一化。在这样的每一个通道中计算均值和方差。还是要加入一个平移变量和伸缩变量。</p>
<p>在RNN中，主要的原因是门控结构是相对于每一个序列而言的，所以应该引入一种新的归一化方法<em>Layer
Normalization</em>，应该在每一条样本（一个序列）在每一个时刻经过之后的值在<span
class="math inline"><em>C</em></span>个通道上进行归一化操作。
最后得到的结果是：将所有的向量放在以原点为球心的单位球面上。</p>
<h4 id="weight-normalization">Weight Normalization</h4>
<p>对于每层的参数<span
class="math inline"><strong>w</strong></span>进行重参数化： <span
class="math display">$$
\mathbf{w} = \frac{g}{\|v\|}v
$$</span> <img src="/images/Pasted%20image%2020250322204925.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250322204925.png" /></p>
<p>对右边的式子<span
class="math inline">$\frac{v}{\|v\|}$</span>进行优化是更为容易的，重参数化的意思是，在训练和测试的时候使用不同的参数表达形式，这样是更加容易优化的。</p>
<h4 id="training-inference-shift">Training-Inference Shift</h4>
<p>滚动预测：在训练的时候，使用真实的标签进行预测；在测试的时候，使用的是推理得到的值进行预测。这是一个自回归任务。</p>
<h5 id="curriculum-learning">Curriculum Learning</h5>
<p>在训练的时候，可以先训练一些简单的任务，然后再训练一些复杂的任务。这样可以更好的训练模型。
在实际中，可以对所有的样本计算loss，先计算loss较小的样本，然后再计算loss较大的样本。这就是<strong>自步学习</strong>。
这里涉及到选择不同的样本顺序的问题。</p>
<p><em>Scheduled
Sampling</em>：在学习刚开始的时候，更多地使用真实的标签进行预测；随着学习的进行，更多地使用模型预测的值进行预测。<strong>是RNN中很重要的技术</strong>。</p>
<h3 id="rnn-with-attention">RNN with Attention</h3>
<h4 id="human-attention">Human Attention</h4>
<p>人类的注意力： - 可持续注意力（没有实现） -
选择性注意力（人类的选择性注意力复杂得多） - 交替式注意力 -
分配式注意力</p>
<h4 id="attention-in-deep-learning">Attention in Deep Learning</h4>
<blockquote>
<p><em>Allowing the model to dynamically pay attention to only certain
parts of the input that help in performing the task at hand
effectively.</em></p>
</blockquote>
<p>存在时间<em>temporal Attention</em>和空间<em>Spatial
Attention</em>上的注意力。一般而言指的是时间上的注意力。</p>
<h4 id="auto-regessive">Auto-Regessive</h4>
<p>[[#Sequence to Sequence]]</p>
<p>最重要的问题是编码器和解码器之间的信息沟通太少了，存在有信息瓶颈。并且在翻译任务中，输入和输出的顺序并不是一致的，大部分的语言的语序是不一样的。
<strong>希望看到后面的信息</strong>，这和RNN的设计目的是相违背的。<strong>全连接的思想</strong>又回来了，获得全局信息的方法有很多，不只是有MLP的方法。有一种基本思想是<strong><em>Relevance</em></strong>，也就是和当前任务相关的信息。这个思想是在Attention中得到了体现。</p>
<h4 id="attention">Attention</h4>
<figure>
<img src="/images/Pasted%20image%2020250322222251.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250322222251.png" />
<figcaption aria-hidden="true">Pasted image
20250322222251.png</figcaption>
</figure>
<p>计算两个东西的相似度有：计算内积、输入<em>relation
network</em>。这样的模型在互联网中有很多的应用，比如推荐系统、搜索引擎等。</p>
<figure>
<img src="/images/Pasted%20image%2020250323084415.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250323084415.png" />
<figcaption aria-hidden="true">Pasted image
20250323084415.png</figcaption>
</figure>
<p>注意力的分配是符合概率分布的，所以可以使用上面计算得到的相关性<span
class="math inline"><em>e</em><sub><em>i</em><em>j</em></sub></span>使用softmax函数进行归一化。这样得到的分布就是注意力的分布：
<span class="math display">$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$</span> 上述计算式表达的含义是，在状态<span
class="math inline"><em>i</em></span>的时刻分配在<span
class="math inline"><em>j</em></span>上的注意力（对于<span
class="math inline"><em>j</em></span>的求和为1）。继续计算<span
class="math inline"><em>c</em><sub><em>i</em></sub></span>： <span
class="math display">$$
c_i = \sum_{j=1}^{T_x} \alpha_{ij} x_j
$$</span> 这里<span
class="math inline"><em>c</em><sub><em>i</em></sub></span>是对于状态<span
class="math inline"><em>i</em></span>的时刻的注意力向量，是对于<span
class="math inline"><em>x</em></span>的加权和。 <span
class="math display"><em>s</em><sub><em>i</em></sub> = <em>f</em>(<em>s</em><sub><em>i</em> − 1</sub>, <em>y</em><sub><em>i</em> − 1</sub>, <em>c</em><sub><em>i</em></sub>)</span>
这里<span
class="math inline"><em>s</em><sub><em>i</em></sub></span>是状态变量，<span
class="math inline"><em>y</em><sub><em>i</em> − 1</sub></span>是前一个时刻的输出，<span
class="math inline"><em>c</em><sub><em>i</em></sub></span>是当前时刻的注意力向量。这里的函数<span
class="math inline"><em>f</em></span>是一个GRU or
LSTM。这个模型是一个<strong>Seq2Seq</strong>模型。</p>
<blockquote>
<p>这里的<span
class="math inline"><em>c</em><sub><em>i</em></sub></span>和<span
class="math inline"><em>s</em><sub><em>i</em></sub></span>的区别是什么，为什么和LSTM有关</p>
</blockquote>
<p><img src="/images/Pasted%20image%2020250323084332.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250323084332.png" /> <img
src="/images/Pasted%20image%2020250323084535.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250323084535.png" /></p>
<p>在机器翻译的过程中，使用source的上下文信息比直接使用词典来翻译更好。这样的模型可以更好的处理长序列的问题。只要是序列都会使用<strong>滑动窗口</strong>，一般设置为50~100之间。对于较短的情况，可以使用psdding的方法；对于较长的情况会使用截断的方法。希望找一个与序列的长度线性关系的模型。</p>
<h4 id="attention-vs.-mlp">Attention vs. MLP</h4>
<p>相同点： - 都是全局模型，是对于长序关系的建模。</p>
<p>不同点： - Attention是基于概率的，MLP是基于全连接的。 -
Attention引入relevance的思想，能大大减小参数量</p>
<h4 id="hierarchical-attention">Hierarchical Attention</h4>
<p>先建模词注意力然后再建模句子注意力。</p>
<h4 id="global-attention">Global Attention</h4>
<p><span class="math display">$$
\text{score} = \begin{cases}
h_t^T \overline{h_s} \\
h_t^T W_a \overline{h_s} \\
v_a^T \tanh(W_a[h_t;\overline{h_s}])
\end{cases}
$$</span> 发现上面三种计算方式的效率是差不多的。</p>
<figure>
<img src="/images/Pasted%20image%2020250323090740.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250323090740.png" />
<figcaption aria-hidden="true">Pasted image
20250323090740.png</figcaption>
</figure>
<h3 id="memory">Memory</h3>
<h4 id="human-memory">Human Memory</h4>
<ul>
<li>Sensory Memory
<ul>
<li>计算机视觉与机器感知</li>
</ul></li>
<li>Short-term Memory
<ul>
<li>与计算机中的内存是很相近的，LSTM是一种将短期记忆尽量变长的方法。</li>
</ul></li>
<li>Long-term Memory
<ul>
<li>前面的模型中没有实现这个功能</li>
</ul></li>
</ul>
<p>在自然语言中，比较困难的任务是进行对话，这时候需要进行长期记忆。在对话中，需要对话的上下文进行理解。</p>
<h4 id="neural-turing-machine">Neural Turing Machine</h4>
<figure>
<img src="/images/Pasted%20image%2020250323092327.png" srcset="/img/loading.gif" lazyload
alt="Pasted image 20250323092327.png" />
<figcaption aria-hidden="true">Pasted image
20250323092327.png</figcaption>
</figure>
<p>在这个模型中，最重要的是对内存1进行寻址的操作，这个操作是一个注意力的操作。
对于读的操作，是按照注意力的大小对地址里面的内容进行加权平均。
对于写的操作，类似于LSTM中的Forget Gate，先进行擦除之后才进行写入。</p>
<p>对于Internal Memory，最大的问题是可能会遗忘，对于External
Memory，是一个外部的存储器，这样的存储是比较稳定的。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/DeepLearning/" class="print-no-link">#DeepLearning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Deep Learning Lecture-5</div>
      <div>https://yima-gu.github.io/2025/06/21/Deep Learning/Deep Learning Lecture-5/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Yima Gu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年6月21日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-3/" title="Deep Learning Lecture-3">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Deep Learning Lecture-3</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-7/" title="Deep Learning Lecture-7">
                        <span class="hidden-mobile">Deep Learning Lecture-7</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
