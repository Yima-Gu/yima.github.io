<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>从 Obsidian 到 Hexo：打造完美的数学公式与图片发布流</title>
    <link href="/2025/06/24/blog/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"/>
    <url>/2025/06/24/blog/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<p>对于许多技术爱好者和研究者来说，Obsidian是无与伦比的个人知识管理（PKM）神器，而 Hexo则是搭建静态博客的绝佳选择。然而，当我们试图将两者结合，打造一个从“笔记”到“发布”的流畅工作流时，常常会遇到一个巨大的障碍：Obsidian中便捷的语法（尤其是数学公式和图片链接）与 Hexo的标准渲染流程存在严重冲突。</p><p>本文将记录一次完整的“踩坑”与“填坑”过程，最终形成一套强大、可靠的自动化解决方案，让你只需一条命令，即可将Obsidian 笔记完美发布到 Hexo 博客。</p><p>对于网站的搭建可以参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://zhuanlan.zhihu.com/p/392994381">[1]</span></a></sup></p><h3id="问题所在便利性与标准化的冲突">问题所在：便利性与标准化的冲突</h3><p>我们的核心痛点在于，Obsidian 为了用户体验，使用了许多非标准的Markdown 语法，而 Hexo 依赖于标准的渲染器。</p><table><thead><tr><th style="text-align: left;">功能</th><th style="text-align: left;">你在 Obsidian 中的写法</th><th style="text-align: left;">标准 Markdown / HTML</th><th style="text-align: left;">冲突点</th></tr></thead><tbody><tr><td style="text-align: left;"><strong>下标</strong></td><td style="text-align: left;"><code>$x\_i$</code></td><td style="text-align: left;"><code>&lt;em&gt;</code> 标签</td><td style="text-align: left;"><code>_</code> 被 Markdown错误地解析为斜体。</td></tr><tr><td style="text-align: left;"><strong>图片嵌入</strong></td><tdstyle="text-align: left;"><code>![my_image.png](/images/my_image.png)</code></td><tdstyle="text-align: left;"><code>![alt](/path/to/image.png)</code></td><td style="text-align: left;">Hexo 完全不认识<code>![...](/images/...)</code> 这种 Wikilink 语法。</td></tr><tr><td style="text-align: left;"><strong>图片粘贴</strong></td><td style="text-align: left;"><code>![](/images/image.png)</code></td><td style="text-align: left;"><code>![](/web/path/image.png)</code></td><td style="text-align: left;">Obsidian可能会生成指向你本地硬盘的绝对路径，这在网站上无效。</td></tr><tr><td style="text-align: left;"><strong>LaTeX 命令</strong></td><td style="text-align: left;"><code>\frac</code>,<code>\begin&#123;aligned&#125;</code></td><td style="text-align: left;"><code>\</code></td><td style="text-align: left;"><code>\</code>是特殊转义符，在处理中可能被“吃掉”。</td></tr><tr><td style="text-align: left;"><strong>公式换行</strong></td><td style="text-align: left;"><code>\\</code></td><td style="text-align: left;"><code>\</code></td><td style="text-align: left;">两个 <code>\</code>可能在处理后只剩一个，导致换行失败。</td></tr></tbody></table><p>手动去修改每一篇文章中的这些问题，无疑是一场灾难。我们的目标是：<strong>在Obsidian 中自由写作，用一个自动化工具处理所有兼容性问题。</strong></p><h3 id="第一步配置-hexo为数学公式提供最佳渲染环境">第一步：配置Hexo，为数学公式提供最佳渲染环境</h3><p>在解决兼容性问题之前，我们首先要确保 Hexo本身具备渲染高质量数学公式的能力。放弃在各种 Hexo插件之间挣扎的曲折道路，我们直接采用业界最稳定、质量最高的方案：<strong>客户端MathJax</strong>。</p><ol type="1"><li><p><strong>安装一个纯粹的 Markdown 渲染器</strong>我们需要一个不会“自作主张”破坏我们 LaTeX代码的渲染器。<code>hexo-renderer-marked</code> 是一个好的选择。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-renderer-marked<br></code></pre></td></tr></table></figure></p></li><li><p><strong>配置 <code>_config.yml</code></strong>在你的博客根目录下的 <code>_config.yml</code>文件中，添加以下配置，以阻止 <code>marked</code>渲染器处理数学公式中的特殊字符： <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">marked:</span><br>  <span class="hljs-attr">gfm:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">pedantic:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">sanitize:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">tables:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">breaks:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">smartLists:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">smartypants:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">mangle:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure></p></li><li><p><strong>在主题中引入 MathJax</strong>这是最关键的一步。我们需要在每个页面中加载 MathJax的官方脚本。编辑你主题的 <code>head</code> 模板文件（例如，对于<code>landscape</code> 主题，是<code>themes/landscape/layout/_partial/head.ejs</code>），在<code>&lt;/head&gt;</code> 标签前加入以下代码： <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs html">&lt;%# MathJax Client-side Rendering Script with AMS package %&gt;<br><span class="hljs-tag">&lt;<span class="hljs-name">script</span>&gt;</span><span class="language-javascript"></span><br><span class="language-javascript">  <span class="hljs-title class_">MathJax</span> = &#123;</span><br><span class="language-javascript">    <span class="hljs-attr">tex</span>: &#123;</span><br><span class="language-javascript">      <span class="hljs-attr">packages</span>: &#123;<span class="hljs-string">&#x27;[+]&#x27;</span>: [<span class="hljs-string">&#x27;ams&#x27;</span>]&#125;, <span class="hljs-comment">// 加载 AMS 宏包，以支持 aligned 等高级环境</span></span><br><span class="language-javascript">      <span class="hljs-attr">inlineMath</span>: [[<span class="hljs-string">&#x27;$&#x27;</span>, <span class="hljs-string">&#x27;$&#x27;</span>], [<span class="hljs-string">&#x27;\\(&#x27;</span>, <span class="hljs-string">&#x27;\\)&#x27;</span>]],</span><br><span class="language-javascript">      <span class="hljs-attr">displayMath</span>: [[<span class="hljs-string">&#x27;$$&#x27;</span>, <span class="hljs-string">&#x27;$$&#x27;</span>], [<span class="hljs-string">&#x27;\\[&#x27;</span>, <span class="hljs-string">&#x27;\\]&#x27;</span>]],</span><br><span class="language-javascript">      <span class="hljs-attr">processEscapes</span>: <span class="hljs-literal">true</span>,</span><br><span class="language-javascript">      <span class="hljs-attr">processEnvironments</span>: <span class="hljs-literal">true</span></span><br><span class="language-javascript">    &#125;,</span><br><span class="language-javascript">    <span class="hljs-attr">options</span>: &#123;</span><br><span class="language-javascript">      <span class="hljs-attr">skipHtmlTags</span>: [<span class="hljs-string">&#x27;script&#x27;</span>, <span class="hljs-string">&#x27;noscript&#x27;</span>, <span class="hljs-string">&#x27;style&#x27;</span>, <span class="hljs-string">&#x27;textarea&#x27;</span>, <span class="hljs-string">&#x27;pre&#x27;</span>]</span><br><span class="language-javascript">    &#125;</span><br><span class="language-javascript">  &#125;;</span><br><span class="language-javascript"></span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;MathJax-script&quot;</span> <span class="hljs-attr">async</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;[https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js](https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js)&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br></code></pre></td></tr></table></figure></p></li></ol><p>完成这一步，你的 Hexo 博客就已经拥有了渲染任何标准 LaTeX公式的能力。现在，我们来解决如何将 Obsidian的“方言”转换成“标准普通话”。</p><h3id="第二步打造你的专属智能翻译官-python-自动化脚本">第二步：打造你的专属“智能翻译官”——Python 自动化脚本</h3><p>我们将编写一个 Python脚本，它将作为我们工作流的核心。它的任务是读取我们从 Obsidian 写好的Markdown 文件，并自动完成所有必需的修复和转义。</p><p>在你的博客根目录下，创建一个名为 <code>converter.py</code>的文件，并将以下代码粘贴进去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># converter.py (Production-ready version of the script)</span><br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> shutil<br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">import</span> urllib.parse<br><br><span class="hljs-comment"># --- CONFIGURATION ---</span><br><span class="hljs-comment"># 在这里设置你的图片文件夹在网站上的绝对路径。</span><br><span class="hljs-comment"># - 如果你的图片存放在 Hexo 项目的 `source/images/` 目录下，就使用 &quot;/images/&quot;</span><br><span class="hljs-comment"># - 如果你的图片存放在 `source/assets/` 目录下，就把它改成 &quot;/assets/&quot;</span><br><span class="hljs-comment"># - 请确保路径以 &quot;/&quot; 开头和结尾。</span><br>IMAGE_FOLDER_PATH = <span class="hljs-string">&quot;/images/&quot;</span><br><span class="hljs-comment"># --- END CONFIGURATION ---</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_image_links</span>(<span class="hljs-params">content, image_prefix</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fix_local_path</span>(<span class="hljs-params"><span class="hljs-keyword">match</span></span>):<br>        alt_text = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">1</span>)<br>        local_path_raw = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">2</span>)<br>        local_path = urllib.parse.unquote(local_path_raw)<br>        filename = os.path.basename(local_path)<br>        encoded_filename = urllib.parse.quote(filename)<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;![<span class="hljs-subst">&#123;alt_text&#125;</span>](<span class="hljs-subst">&#123;image_prefix&#125;</span><span class="hljs-subst">&#123;encoded_filename&#125;</span>)&quot;</span><br><br>    local_path_regex = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&#x27;!\[(.*?)\]\(((?:[a-zA-Z]:|file:)[\\/].*?)\)&#x27;</span>)<br>    content = local_path_regex.sub(fix_local_path, content)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_wikilink</span>(<span class="hljs-params"><span class="hljs-keyword">match</span></span>):<br>        filename = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">1</span>).strip()<br>        encoded_filename = urllib.parse.quote(filename)<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;![<span class="hljs-subst">&#123;filename&#125;</span>](<span class="hljs-subst">&#123;image_prefix&#125;</span><span class="hljs-subst">&#123;encoded_filename&#125;</span>)&quot;</span><br><br>    wikilink_regex = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&#x27;!\[\[(.*?)\]\]&#x27;</span>)<br>    content = wikilink_regex.sub(replace_wikilink, content)<br>    <span class="hljs-keyword">return</span> content<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">remove_blank_lines_in_env</span>(<span class="hljs-params"><span class="hljs-keyword">match</span></span>):<br>    env_content = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">0</span>)<br>    lines = env_content.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>    non_blank_lines = [line <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines <span class="hljs-keyword">if</span> line.strip()]<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;\n&#x27;</span>.join(non_blank_lines)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">escape_latex_math_part</span>(<span class="hljs-params">content</span>):<br>    content = content.replace(<span class="hljs-string">&#x27;\\&#x27;</span>, <span class="hljs-string">&#x27;\\\\&#x27;</span>)<br>    content = content.replace(<span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">r&#x27;\_&#x27;</span>)<br>    content = content.replace(<span class="hljs-string">&#x27;*&#x27;</span>, <span class="hljs-string">r&#x27;\*&#x27;</span>)<br>    content = content.replace(<span class="hljs-string">&#x27;[&#x27;</span>, <span class="hljs-string">r&#x27;\[&#x27;</span>)<br>    content = content.replace(<span class="hljs-string">&#x27;]&#x27;</span>, <span class="hljs-string">r&#x27;\]&#x27;</span>)<br>    <span class="hljs-keyword">return</span> content<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_math_content</span>(<span class="hljs-params">content</span>):<br>    aligned_regex = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&quot;(\\begin\&#123;aligned\&#125;.*?\\end\&#123;aligned\&#125;)&quot;</span>, re.DOTALL)<br>    content = aligned_regex.sub(remove_blank_lines_in_env, content)<br>    text_block_regex = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&#x27;(\\text\&#123;.*?\&#125;)&#x27;</span>)<br>    parts = text_block_regex.split(content)<br>    new_parts = []<br>    <span class="hljs-keyword">for</span> part <span class="hljs-keyword">in</span> parts:<br>        <span class="hljs-keyword">if</span> part.startswith(<span class="hljs-string">&#x27;\\text&#123;&#x27;</span>):<br>            new_parts.append(part.replace(<span class="hljs-string">&#x27;\\&#x27;</span>, <span class="hljs-string">&#x27;\\\\&#x27;</span>, <span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">else</span>:<br>            new_parts.append(escape_latex_math_part(part))<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>.join(new_parts)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_math_match</span>(<span class="hljs-params"><span class="hljs-keyword">match</span></span>):<br>    delimiter = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">1</span>)<br>    content = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">2</span>)<br>    escaped_content = process_math_content(content)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;delimiter&#125;</span><span class="hljs-subst">&#123;escaped_content&#125;</span><span class="hljs-subst">&#123;delimiter&#125;</span>&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_file_content</span>(<span class="hljs-params">text, image_prefix</span>):<br>    text = convert_image_links(text, image_prefix)<br>    math_block_regex = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&quot;(\&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\$\\|\\\\&lt;/span&gt;)(.\*?)\\1&quot;</span>, re.DOTALL)<br>    converted\_text = math\_block\_regex.sub(process\_math\_<span class="hljs-keyword">match</span>, text)<br>    <span class="hljs-keyword">return</span> converted\_text<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>\_single\_file(filepath, create\_backup, image\_prefix):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Checking: <span class="hljs-subst">&#123;filepath&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            original\_content = f.read()<br>        fm\_<span class="hljs-keyword">match</span> = re.<span class="hljs-keyword">match</span>(<span class="hljs-string">r&#x27;---\\s\*?\\n(.\*?)\\n---\\s\*?\\n&#x27;</span>, original\_content, re.DOTALL)<br>        <span class="hljs-keyword">if</span> fm\_<span class="hljs-keyword">match</span> <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;escaped: true&#x27;</span> <span class="hljs-keyword">in</span> fm\_<span class="hljs-keyword">match</span>.group(<span class="hljs-number">1</span>):<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;  -&gt; Skipping: Already marked as converted.&quot;</span>)<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-keyword">if</span> create\_backup:<br>            backup\_path = filepath + <span class="hljs-string">&quot;.bak&quot;</span><br>            shutil.copy2(filepath, backup\_path)<br>        converted\_content = convert\_file\_content(original\_content, image\_prefix)<br>        <span class="hljs-keyword">if</span> fm\_<span class="hljs-keyword">match</span>:<br>            front\_matter\_content = fm\_<span class="hljs-keyword">match</span>.group(<span class="hljs-number">1</span>)<br>            new\_front\_matter = front\_matter\_content.strip() + <span class="hljs-string">&quot;\\nescaped: true\\n&quot;</span><br>            main\_content\_start\_index = fm\_<span class="hljs-keyword">match</span>.end()<br>            final\_content = original\_content\[:main\_content\_start\_index\].replace(front\_matter\_content, new\_front\_matter) + converted\_content\[main\_content\_start\_index:\]<br>        <span class="hljs-keyword">else</span>:<br>            new\_front\_matter = <span class="hljs-string">&quot;escaped: true\\n&quot;</span><br>            final\_content = <span class="hljs-string">f&quot;---\\n<span class="hljs-subst">&#123;new\_front\_matter&#125;</span>---\\n\\n<span class="hljs-subst">&#123;converted\_content&#125;</span>&quot;</span><br>        <span class="hljs-keyword">if</span> original\_content != final\_content:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f: f.write(final\_content)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;  -&gt; File converted and marked as &#x27;escaped&#x27;.&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;  -&gt; No changes needed.&quot;</span>)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  -&gt; Error processing file <span class="hljs-subst">&#123;filepath&#125;</span>: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>, file=sys.stderr)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>\_path(path, create\_backup, image\_prefix):<br>    <span class="hljs-keyword">if</span> os.path.isfile(path):<br>        <span class="hljs-keyword">if</span> path.endswith(<span class="hljs-string">&quot;.md&quot;</span>): process\_single\_file(path, create\_backup, image\_prefix)<br>        <span class="hljs-keyword">else</span>: <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Skipping non-markdown file: <span class="hljs-subst">&#123;path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">elif</span> os.path.isdir(path):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Processing directory: <span class="hljs-subst">&#123;path&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">for</span> root, \_, files <span class="hljs-keyword">in</span> os.walk(path):<br>            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                    file\_path = os.path.join(root, file)<br>                    process\_single\_file(file\_path, create\_backup, image\_prefix)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error: Path does not exist: <span class="hljs-subst">&#123;path&#125;</span>&quot;</span>, file=sys.stderr)<br><br><span class="hljs-keyword">if</span> \_\_name\_\_ == <span class="hljs-string">&quot;\_\_main\_\_&quot;</span>:<br>    parser = argparse.ArgumentParser(description=<span class="hljs-string">&quot;Convert Obsidian/Hexo Markdown for publishing.&quot;</span>)<br>    parser.add\_argument(<span class="hljs-string">&quot;paths&quot;</span>, metavar=<span class="hljs-string">&quot;PATH&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, nargs=<span class="hljs-string">&#x27;+&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;One or more paths to markdown files or directories to process.&quot;</span>)<br>    parser.add\_argument(<span class="hljs-string">&quot;--image-prefix&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;/images/&quot;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;The absolute path prefix for image links from the site root. Default: /images/&quot;</span>)<br>    parser.add\_argument(<span class="hljs-string">&quot;--no-backup&quot;</span>, action=<span class="hljs-string">&quot;store\_false&quot;</span>, dest=<span class="hljs-string">&quot;create\_backup&quot;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;Disable the creation of .bak backup files.&quot;</span>)<br>    args = parser.parse\_args()<br>    <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> args.paths:<br>        process\_path(path, args.create\_backup, args.image\_prefix)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\\nConversion process finished.&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="第三步封装一切的总指挥-shell-脚本">第三步：封装一切的“总指挥”——Shell 脚本</h3><p>为了实现“一键发布”，我们编写一个 shell 脚本来调用 Python脚本，并执行所有 Hexo 命令。</p><p>在你的博客根目录下，创建一个名为 <code>publish.sh</code>的文件，并粘贴以下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># publish.sh - A smart script to convert, clean, generate, and deploy the Hexo blog.</span><br><br><span class="hljs-comment"># Stop the script immediately if any command fails.</span><br><span class="hljs-built_in">set</span> -e<br><br><span class="hljs-comment"># --- Configuration ---</span><br>PYTHON\_SCRIPT=<span class="hljs-string">&quot;converter.py&quot;</span> <span class="hljs-comment"># 确保这个文件名和你的Python脚本文件名一致</span><br>SOURCE\_DIR=<span class="hljs-string">&quot;source/\_posts/&quot;</span><br>IMAGE\_PREFIX=<span class="hljs-string">&quot;/images/&quot;</span><br><br><span class="hljs-comment"># --- ANSI Color Codes ---</span><br>GREEN=<span class="hljs-string">&#x27;\\033\[0;32m&#x27;</span><br>YELLOW=<span class="hljs-string">&#x27;\\033\[1;33m&#x27;</span><br>CYAN=<span class="hljs-string">&#x27;\\033\[0;36m&#x27;</span><br>NC=<span class="hljs-string">&#x27;\\033\[0m&#x27;</span> <span class="hljs-comment"># No Color</span><br><br><span class="hljs-comment"># --- Argument Parsing Logic ---</span><br><span class="hljs-keyword">while</span> \[\[ <span class="hljs-variable">$#</span> -gt 0 ]]; <span class="hljs-keyword">do</span><br>    key=<span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span><br>    <span class="hljs-keyword">case</span> <span class="hljs-variable">$key</span> <span class="hljs-keyword">in</span><br>        -s|--<span class="hljs-built_in">source</span>) SOURCE_DIR=<span class="hljs-string">&quot;<span class="hljs-variable">$2</span>&quot;</span>; <span class="hljs-built_in">shift</span>; <span class="hljs-built_in">shift</span> ;;<br>        -i|--image-prefix) IMAGE\_PREFIX=<span class="hljs-string">&quot;<span class="hljs-variable">$2</span>&quot;</span>; <span class="hljs-built_in">shift</span>; <span class="hljs-built_in">shift</span> ;;<br>        *) <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Unknown option: &lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;1&quot;</span>; <span class="hljs-built_in">exit</span> 1 ;;<br><span class="hljs-keyword">esac</span><br><span class="hljs-keyword">done</span><br>\# \-\-\- Main Script Logic \-\-\-<br><span class="hljs-built_in">echo</span> \-e <span class="hljs-string">&quot;&lt;/span&gt;&#123;YELLOW&#125;&gt;&gt;&gt; Step 1: Converting Markdown files...&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\&#123;NC\&#125;&quot;</span><br><span class="hljs-built_in">echo</span> \-e <span class="hljs-string">&quot;&lt;/span&gt;&#123;CYAN&#125;Source:           &lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\&#123;SOURCE\_DIR\&#125;&lt;/span&gt;&#123;NC&#125;&quot;</span><br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CYAN&#125;</span>Image Prefix:     &lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;IMAGE\\\_PREFIX\\&#125;&lt;/span&gt;&#123;NC&#125;&quot;</span><br><br><span class="hljs-keyword">if</span> \[ ! -f <span class="hljs-string">&quot;<span class="hljs-variable">$PYTHON_SCRIPT</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Error: Python script &#x27;<span class="hljs-variable">$PYTHON</span>\_SCRIPT&#x27; not found.&quot;</span><br>    <span class="hljs-built_in">exit</span> 1<br><span class="hljs-keyword">fi</span><br>python <span class="hljs-string">&quot;<span class="hljs-variable">$PYTHON_SCRIPT</span>&quot;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$SOURCE</span>\_DIR&quot;</span> --image-prefix <span class="hljs-string">&quot;&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;IMAGE\\\_PREFIX&quot;</span><br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;&lt;/span&gt;&#123;GREEN&#125;--- Markdown conversion complete.&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;\\\\n&lt;/span&gt;&#123;YELLOW&#125;&gt;&gt;&gt; Step 2: Cleaning old Hexo files...&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br>hexo clean<br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;&lt;/span&gt;&#123;GREEN&#125;--- Clean complete.&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;\\\\n&lt;/span&gt;&#123;YELLOW&#125;&gt;&gt;&gt; Step 3: Generating new site files...&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br>hexo g<br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;&lt;/span&gt;&#123;GREEN&#125;--- Generation complete.&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;\\\\n&lt;/span&gt;&#123;YELLOW&#125;&gt;&gt;&gt; Step 4: Deploying to GitHub Pages...&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br>hexo d<br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;&lt;/span&gt;&#123;GREEN&#125;--- Deployment complete.&lt;span class=&quot;</span>math-inline<span class="hljs-string">&quot;&gt;\\&#123;NC\\&#125;&quot;</span><br><span class="hljs-built_in">echo</span> \\-e <span class="hljs-string">&quot;\\\\n&lt;/span&gt;&#123;GREEN&#125;✅ All steps completed successfully! Your blog has been published.<span class="hljs-variable">$&#123;NC&#125;</span>&quot;</span><br></code></pre></td></tr></table></figure><p><strong>别忘了</strong>给这个脚本赋予执行权限（只需做一次）：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> +x publish.sh<br></code></pre></td></tr></table></figure></p><h3 id="你的终极工作流">你的终极工作流</h3><p>现在，从写作到发布的完整流程被简化为了三个简单步骤： 1.<strong>写作</strong>：在 Obsidian 中自由地写作，使用你最喜欢的<code>![...](/images/...)</code> 语法插入图片，用标准 LaTeX 书写公式。2. <strong>同步</strong>：将写好的 <code>.md</code>文件和相关图片分别放入你 Hexo 项目的 <code>source/_posts/</code> 和<code>source/images/</code> 目录。 3.<strong>发布</strong>：在你的博客根目录下，打开终端，运行：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./publish.sh<br></code></pre></td></tr></table></figure>一切都将自动完成。如果你有特殊需求，比如处理草稿箱或使用不同的图片目录，还可以使用参数：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./publish.sh -s <span class="hljs-built_in">source</span>/_drafts/ -i /assets/<br></code></pre></td></tr></table></figure></p><h3 id="结语">结语</h3><p>通过结合 Hexo 的正确配置、一个智能的 Python 转换脚本和一个自动化的Shell 发布脚本，我们最终打通了 Obsidian 和 Hexo之间的“任督二脉”。这套工作流将所有复杂性都封装在了工具背后，让我们可以重新专注于最重要的事——思考与创作。</p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span>https://zhuanlan.zhihu.com/p/392994381<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>#Hexo</tag>
      
      <tag>#Obsidian</tag>
      
      <tag>#MathJax</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-1</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-1/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-1/</url>
    
    <content type="html"><![CDATA[<p><strong>人工智能</strong>：模拟人类的智能<strong>机器学习</strong>：不显式编程来实现<strong>深度学习</strong>：使用神经网络</p><p>对于深度学习总是能找到一些amazing的例子，同时也有一些失败的例子。</p><p>深度学习是在数据中<strong>获取高度抽象</strong>的特征，主要使用神经网络。在深度学习中实现的功能类似于坐标系变换。对于同一个问题进行变换可以进行不同的处理。在机器学习过程中的规则系统仍然是很重要的。</p><p>深度学习的表示为计算图</p><h3 id="framework">Framework</h3><p>在学习算法中一定要指定函数族，然后使用数据来拟合函数族。要找出<strong>最优的函数族</strong>。不同的机器学习模型的区别主要在于<strong>假设空间</strong>（候选函数族）的不同。</p><p>过拟和在指定函数族时已经确定。</p><p>泛化误差公式： <span class="math display">$$\\varepsilon\_{test} \\leq \\hat{\\varepsilon}\_{train} +\\sqrt{\\frac{complexity}{n}}$$</span> 要求：训练误差小，模型复杂度小，数据量大<strong>当且仅当需要的时候才增加模型复杂度</strong></p><h3 id="model">Model</h3><p><span class="math display">$$\\arg \\min O(D;\\theta) = \\sum\_{i=1}^{N} L(y\_i, f(x\_i);\\theta) +\\Omega(\\theta)$$</span> 上述公式要求：在训练样本上预测值与真实值的误差尽可能小。</p><p>对于分类问题而言，经常使用的是交叉熵损失函数（logistic）和hinge损失函数。</p><p><spanclass="math inline">$\\Omega(\\theta)$</span>是惩罚项，希望模型尽量简单。在使得<spanclass="math inline">$\\theta$</span>尽量小的过程中，某些特征的权重会变为0，这样就可以实现特征选择。同时也可以减少模型的复杂性。</p><p><strong>学习类似于统计中的参数估计</strong></p><figure><img src="/images/%7B65C91395-95B8-4EF2-8AF6-3F86705FD81B%7D.png"alt="{65C91395-95B8-4EF2-8AF6-3F86705FD81B}.png" /><figcaptionaria-hidden="true">{65C91395-95B8-4EF2-8AF6-3F86705FD81B}.png</figcaption></figure><p>梯度下降的方法。</p><p>具体例子：线性回归</p><p><span class="math display">$$O(D;\\theta) = \\sum\_{i=1}^{N} (y\_i - \\theta^T x\_i)^2 + \\lambda\\theta^T \\theta = (Y - X\\theta)^T (Y - X\\theta) + \\lambda \\theta^T\\theta$$</span> 上述公式是关于<spanclass="math inline">$\\theta$</span>的二次函数，可以通过求导得到最优解。[[matrixcookbook.pdf]]<span class="math display">$$\\frac{\\partial O(D;\\theta)}{\\partial \\theta} = -2 X^T (Y -X\\theta) + 2 \\lambda \\theta = 0$$</span> <span class="math display">$$\\hat{\\theta} = (X^T X + \\lambda I)^{-1} X^T Y$$</span> 但是上述算法的复杂性为<spanclass="math inline"><em>O</em>(<em>n</em><sup>3</sup>)</span>，不适用于大数据集。在软件工程中，通常使用梯度下降法。</p><p><strong>Logistic Regression</strong></p><p><span class="math display">$$O(D;\\theta) = \\sum\_{i=1}^{n} \\log(1 + \\exp(-y\_i \\theta^T x\_i)) +\\lambda ||\\theta||\_{1} = F(D;\\theta) + \\lambda ||\\theta||\_{1}$$</span> 这里使用的是近端梯度下降法 <em>proximal gradientdescent</em>。其中的1-范数是各个分量的绝对值之和。<ahref="https://www.zhihu.com/tardis/zm/art/82622940?source_id=1005">机器学习| 近端梯度下降法 (proximal gradient descent)</a>在一些不可导的情况下如hinge损失函数，可以使用次梯度。</p><p><strong>Softmax Regression</strong></p><p>实现多分类问题，计算每个类别的概率即可，对于某个样本进行多次打分，取最高分对应的类别。</p><p>Softmax Function: [[Deep Learning Lecture-2#^b5bcbb]] <spanclass="math display">$$P(y|x,\\theta) = \\frac{\\exp(\\theta\_y^T x)}{\\sum\_{r=1}^{C}\\exp(\\theta\_{r}^T x)}$$</span>线性模型，用超平面对样本进行打分，然后使用softmax函数进行归一化。</p><p><span class="math display">$$O(D;\\theta) = -\\sum\_{i=1}^{n} \\log P(y\_i|x\_i;\\theta) + \\lambda||\\theta||\_{1}$$</span>上述式子其实是对数似然函数，最大化对数似然函数等价于最小化交叉熵损失函数。</p><h4 id="aproximation-and-estimation">Aproximation and Estimation</h4><p>假设空间只能表现一个有限的函数集合，有时候真值函数不一定在假设空间中。这时候假设空间中最好的函数与真值函数之间的差距称为<strong>近似误差</strong><em>ApproximationError</em>。学习得到的函数与空间中最好的函数的误差为<strong>估计误差</strong><em>Estimation Error</em>。</p><h3 id="model-selection">Model Selection</h3><p><em>All models are wrong but some are useful.</em></p><p>CASH: Combined Algorithm Selection and Hyperparameter optimization目标是：选择最好的模型和超参数 <span class="math display">$$A^\*\_{\\lambda^\*} = \\arg \\min\_{A \\in \\mathcal{A}}\\min\_{\\lambda \\in \\Lambda^{(i)}}\\mathcal{L}(A\_{\\lambda}^{(i)},D\_{train},D\_{valid})$$</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-2</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-2/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-2/</url>
    
    <content type="html"><![CDATA[<h3 id="brain-and-neuron">Brain and Neuron</h3><p>感知机<em>Perceptron</em>是神经元的一个相当简单的数学模型，包括：输入、权重、激活函数、输出。其实是在空间超平面上嵌入了一个非线性函数。<span class="math display">$$\\hat{y} = g(\\sum\_{i=1}^{n}  x\_i \\theta\_i +\\theta\_0)$$</span>感知机在神经网络中也叫单元<em>unit</em>，是神经网络的基本组成单元。但是这样的单元会比人的神经元简单很多。#### PLA在上述的激活函数中，最先使用的是符号函数，这个函数是不光滑的。可以使用下面的算法来进行训练：<img src="/images/%7B5ABA24F6-F5A5-48C8-AE9F-1E8524E35979%7D.png"alt="{5ABA24F6-F5A5-48C8-AE9F-1E8524E35979}.png" />在理论推导中，可以计算PLA的<strong>收敛率</strong>：（在线性可分的情况下）<span class="math inline">$\\gamma$</span>是最优间隔<em>the best-casemargin</em>，计算的是训练样本与超平面的距离之间的最小值。 <spanclass="math display">$$\\exists v \\in \\mathbb{R}^d \\quad \\text{s.t.}\\,\\gamma \\leq\\frac{y\_i(v\\cdot x\_i)}{||v||}$$</span> <spanclass="math inline"><em>R</em></span>是数据集的半径，即样本数据向量模的最大值，<spanclass="math inline"><em>d</em></span>是数据集的维度。那么PLA的收敛率为：最多经过<spanclass="math inline">$\\frac{R^2}{\\gamma^2}$</span>次迭代就可以收敛。 -<span class="math inline">$\\gamma$</span>越大，收敛越快 - <spanclass="math inline"><em>R</em></span>越大，收敛越慢</p><h5 id="expresiveness-of-perceptron">Expresiveness of Perceptron</h5><p>感知机是一个线性分类器，只能解决线性可分的问题。如果数据不是线性可分的，那么感知机就无法解决（例如异或问题，但其实异或不是基本的布尔运算，可以用与或非表达）。</p><h4 id="multi-layer-perceptron">Multi-layer Perceptron</h4><p>多层感知机<em>Multi-layerPerceptron</em>是感知机的扩展，可以解决非线性问题。多层感知机的结构是：输入层、隐藏层、输出层（输入层并不算一层）。<em>感知机之间的链接方式相比人脑而言也是较为简单的。</em>在表达时，可以发现是<strong>稀疏的</strong>，也就是每一层并不是与前面的所有的感知机相连。多层感知机的表达能力较强，这时需要增加感知机的层数。</p><h5 id="comention">Comention</h5><figure><img src="/images/%7B9A215F21-185B-4C55-B872-413D388E5321%7D.png"alt="{9A215F21-185B-4C55-B872-413D388E5321}.png" /><figcaptionaria-hidden="true">{9A215F21-185B-4C55-B872-413D388E5321}.png</figcaption></figure><figure><img src="/images/%7BE178BAD1-DE9C-44E5-A665-D550AE3A9558%7D.png"alt="{E178BAD1-DE9C-44E5-A665-D550AE3A9558}.png" /><figcaptionaria-hidden="true">{E178BAD1-DE9C-44E5-A665-D550AE3A9558}.png</figcaption></figure><ul><li>用上标表示层数，用下标表示感知机的编号</li><li><span class="math inline">$\\theta\_{ij}^{(l)}$</span>表示第<spanclass="math inline"><em>l</em></span>层的第<spanclass="math inline"><em>i</em></span>个感知机的第<spanclass="math inline"><em>j</em></span>个输入的权重</li><li><spanclass="math inline"><em>b</em>_<em>j</em><sup>(<em>l</em>)</sup></span>表示第<spanclass="math inline"><em>l</em></span>层的第<spanclass="math inline"><em>j</em></span>个感知机的偏置</li><li><spanclass="math inline"><em>a</em>_<em>j</em><sup>(<em>l</em>)</sup></span>表示第<spanclass="math inline"><em>l</em></span>层的第<spanclass="math inline"><em>j</em></span>个感知机的输出（在激活之后的数值）</li><li><spanclass="math inline"><em>z</em>_<em>j</em><sup>(<em>l</em>)</sup></span>表示第<spanclass="math inline"><em>l</em></span>层的第<spanclass="math inline"><em>j</em></span>个感知机的输入（经过线性变换之后的数值）</li><li><span class="math inline">$J(\\theta)$</span>表示损失函数<em>LossFunction</em></li></ul><p>在上面的图中，边的个数就是参数的个数。</p><h5 id="activation-function">Activation Function</h5><ul><li><p><strong>Sigmoid函数</strong>：<span class="math inline">$g(z) =\\sigma(z)= \\frac{1}{1+e^{-z}}$</span>采用有界的函数，可以将输出限制在0-1之间，避免数值爆炸。但是在基于梯度的计算中，会出现梯度消失（梯度饱和），在两侧的范围内梯度会接近于0。</p></li><li><p><strong>ReLU函数</strong>：<spanclass="math inline"><em>g</em>(<em>z</em>) = <em>m</em><em>a</em><em>x</em>(0, <em>z</em>)</span>ReLU函数是一个分段函数，可以避免<strong>梯度消失</strong>的问题。但是在训练时，会出现<strong>神经元死亡</strong>的问题，即神经元的输出一直为0。</p></li><li><p><strong>GeLu函数</strong>：<span class="math inline">$g(z) = z\\cdot \\Phi(z) = z \\cdot \\frac{1}{2} (1 +\\text{erf}(\\frac{z}{\\sqrt{2}}))$</span>用Guass分布的累计函数对上述进行加权。<spanclass="math inline">$\\Phi(z)$</span>是标准正态分布的累计分布函数<em>CDF</em>。在一些较为复杂的模型中（GPT-3、Bert）都有使用。</p></li></ul><p>在网络的输出层，使用的激活函数由问题决定。如果是回归问题，可以使用线性函数；在有界的输出情况下，可以使用Sigmoid函数；在多分类问题中，可以使用Softmax函数。</p><ul><li><strong>Softmax函数</strong>：<span class="math inline">$g(z)\_i =\\frac{e^{z\_i}}{\\sum\_{j=1}^{k} e^{z\_j}}$</span>Softmax函数是一个多分类的激活函数，可以将输出的值转化为概率值。分类问题是随机实验中的伯努利实验<em>CategoricalDistribution</em>。缺点为：“赢者通吃”，即最大的值会被放大，其他的值会被压缩，有<em>overconfidence</em>的问题（即某个分类的概率过大）。同时有数值稳定性问题，即数值计算时可能会出现数值爆炸的问题。改进为： <span class="math display">$$g(z)\_i = \\frac{e^{z\_i - \\max(z)}}{\\sum\_{j=1}^{k} e^{z\_j -\\max(z)}}$$</span> 上述改进能解决数值稳定性问题，但是对于<em>overconfidence</em>问题还是存在。 ^b5bcbb</li></ul><h5 id="cost-function">Cost Function</h5><p>任何一个衡量预测与实际值之间的差异的函数都可以称为损失函数。在这里使用的是交叉熵损失函数<em>CrossEntropy Loss</em>： <span class="math display">$$J(y,\\hat{y}) = - \\sum\_{i=1}^{n} y\_i \\log(\\hat{y}\_i)$$</span> 作代入得到： <span class="math display">$$\\min J(\\theta)= -\\frac{1}{m} \\sum\_{i=1}^{m} \\sum\_{j=1}^{k}\\mathbf{1}\\{y^{(i)}=j\\} \\log\\frac{\\exp{z\_j^{(n\_l)}}}{\\sum\_{j'=1}^{k} \\exp{ z\_{j'}^{(n\_l)}}} \\quad (\*)$$</span></p><p><em>*上述公式中的m为样本数目，k为类别数目，<spanclass="math inline"><em>z</em>_<em>j</em><sup>(<em>n</em>_<em>l</em>)</sup></span>为最后一层的第j个感知机的输入。对于实际类别采用独热编码，即只有在对应类别取值为1。</em></p><h5 id="statistical-view-of-softmax">Statistical View of Softmax</h5><p>考虑投掷m次骰子，其中第<spanclass="math inline"><em>i</em></span>个得到<spanclass="math inline"><em>j</em></span>的概率为<spanclass="math inline"><em>q</em>_<em>i</em><em>j</em></span>。在<em>Softmax</em>中对于概率进行建模（用数据进行估计，对于分类估计的参数进行逼近）：<span class="math display">$$q\_{ij} = P(y\_i = j\\,| \\mathbf{x\_i} ; \\mathbf{W} )$$</span> 在给定的结果<spanclass="math inline">$\\{y\_1,...,y\_m\\}$</span>下，概率值（似然函数）为：<span class="math display">$$\\mathcal{L}(\\mathbf{W};\\mathcal{D})=\\prod\_{i=1}^{m}\\prod\_{j=1}^{k} P(y\_i=j|q\_{ij})^{\\mathbf{1}\\{y\_i = j\\}} =\\prod\_{i=1}^{m} \\prod\_{j=1}^{k}P(y\_i = j\\,| \\mathbf{x\_i} ;\\mathbf{W} ) ^{\\mathbf{1}\\{y\_i = j\\}}$$</span> <em><spanclass="math inline">$\\mathbf{W}$</span>是模型的参数，上面的式子是在这样的建模和数据下得到结果的可能性，也就是统计中的似然函数。这样的过程类似于统计中的参数估计。</em></p><p>做极大似然估计： <span class="math display">$$\\mathcal{L}(\\mathbf{W};\\mathcal{D}) =\\max\_{w\_1 \\dots w\_k}\\prod\_{i=1}^{m} \\prod\_{j=1}^{k} P(y\_i = j\\,| \\mathbf{x\_i} ;\\mathbf{W} ) ^{\\mathbf{1}\\{y\_i = j\\}}$$</span> 取负对数： <span class="math display">$$J(\\mathbf{W}) = \\min\_{w\_1 \\dots w\_k}- \\log\\mathcal{L}(\\mathbf{W};\\mathcal{D}) = - \\sum\_{i=1}^{m}\\sum\_{j=1}^{k} \\mathbf{1}\\{y\_i = j\\} \\log P(y\_i = j\\,|\\mathbf{x\_i} ; \\mathbf{W} )$$</span>上述的式子就是交叉熵损失函数。上面的过程其实是在认为分类是<spanclass="math inline"><em>i</em>.<em>i</em>.<em>d</em>.</span>的伯努利分布的极大似然估计。</p><h3 id="gradient-descent">Gradient Descent</h3><p>对于不是直接依赖的导数的计算较为复杂，对于最后一层的导数计算较为简单（是直接依赖）。对于前面层的参数的导数在这里使用<strong>链式法则</strong>来进行计算。</p><p>对于最后一层的参数的导数计算： <span class="math display">$$\\frac{\\partial J(\\theta ,b)}{\\partial z\_j^{(n\_l)}} = -(\\mathbf{1}\\{y^{(i)}=j\\} -P(y^{(i)}=j|\\mathbf{x}^{(i)};\\theta,b)))$$</span> 可以发现梯度是真是的概率减去预测的概率。</p><h4 id="step-1-forward-propagation">Step 1: Forward Propagation</h4><p>输入样本计算得到的输出值，这个过程是一个前向传播的过程。</p><h4 id="step-2-backward-propagation">Step 2: Backward Propagation</h4><p>将损失函数带有的错误信息向前传播 <span class="math display">$$\\frac{J(\\theta)}{\\theta\_1}= \\frac{\\partial J(\\theta)}{\\partial\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partialz}{\\partial \\theta\_1}$$</span>除了需要求解的导数的参数，其他的都是计算的中间值。BP是一个动态规划算法。</p><h4 id="computing-the-residual">Computing the Residual</h4><p>第<span class="math inline"><em>l</em></span>层的第<spanclass="math inline"><em>i</em></span>个结点的残差<em>Residual</em>的定义为：<span class="math display">$$\\delta\_i^{(l)} = \\frac{\\partial J(\\theta)}{\\partial z\_i^{(l)}}$$</span> 对于最后一层的残差，计算较为简单： <spanclass="math display">$$\\delta\_i^{(n\_l)} = \\frac{\\partial}{\\partial z\_i^{(n\_l)} }J(\\theta) = \\frac{\\partial }{\\partial \\hat{y}\_i}J(\\theta)g'(z\_i^{(n\_l)})$$</span> 利用链式法则对激活函数求导即可。 对于隐藏层的导数计算： <spanclass="math display">$$\\delta\_i^{(l)} = \\frac{\\partial J(\\theta)}{\\partial z\_i^{(l)}} =\\sum\_{j=1}^{n\_{l+1}} \\frac{\\partial J(\\theta)}{\\partialz\_j^{(l+1)}} \\frac{\\partial z\_j^{(l+1)}}{\\partial z\_i^{(l)}} =\\sum\_{j=1}^{n\_{l+1}} \\delta\_j^{(l+1)} \\theta\_{ij}^{(l)}g'(z\_i^{(l)})$$</span> <span class="math display">$$\\delta\_i^{(l)}= \\sum\_{j=1}^{n\_{l+1}} \\delta\_j^{(l+1)}\\theta\_{ji}^{(l)} g'(z\_j^{(l)})$$</span> 上述公式实现了<strong>传递</strong>的过程。</p><h4 id="step-3-update-parameters">Step 3: Update Parameters</h4><p>对于参数更新的过程： <span class="math display">$$\\frac{\\partial J(\\theta)}{\\partial \\theta\_{ij}^{(l)}} =\\frac{\\partial J(\\theta)}{\\partial z\_j^{(l+1)}} \\frac{\\partialz\_j^{(l+1)}}{\\partial \\theta\_{ij}^{(l)}} = \\delta\_j^{(l+1)}a\_i^{(l)}$$</span> <span class="math display">$$\\frac{\\partial J(\\theta)}{\\partial b\_j^{(l)}} = \\delta\_j^{(l+1)}$$</span> ##### Automatic Differentiation在实际的计算中，可以使用自动微分的方法来进行计算。自动微分是一种计算导数的方法，可以分为两种：- <strong>SymbolicDifferentiation</strong>：通过符号的方式来计算导数，这种方法计算的精确度较高，但是计算的速度较慢。- <strong>NumericalDifferentiation</strong>：通过数值的方式来计算导数，这种方法计算的速度较快，但是计算的精确度较低。</p><p>在计算图中，将每一个计算层的反向传播的导数保存在软件包中，这样可以减少计算的时间。实际的应用中，对于计算图进行拓扑排序，然后进行反向传播的计算。</p><h4 id="optimization-in-practice">Optimization in Practice</h4><h5 id="dropout"><strong>Dropout</strong></h5><p>在训练的过程中，随机的将一些神经元的权重置为0（丢弃），这样可以减少过拟合的问题。在操作的过程中，按照一定的概率<spanclass="math inline"><em>p</em></span>对神经元进行丢弃。在某一层未被丢弃的神经元的激活值值乘以<spanclass="math inline">$\\frac{1}{1-p}$</span>，这样可以保持期望值不变。</p><h5 id="weight-initialization">Weight Initialization</h5><p>对于权重的初始化，一般使用Guass分布可以使用一些方法来进行初始化，例如：<strong>Xavier Initialization</strong> ( linear activations )： <spanclass="math display">$$Var(W)= \\frac{1}{n\_{in}}$$</span> 假设输入的数据<spanclass="math inline"><em>x</em>_<em>j</em></span>满足均值为0，方差为<spanclass="math inline">$\\gamma$</span>，<spanclass="math inline"><em>n</em>_<em>i</em><em>n</em></span>是这一个神经元对应的输入的神经元的个数。在线性组合之后，可以得到： <span class="math display">$$h\_i=\\sum\_{j=1}^{n\_{in}} w\_{ij} x\_j$$</span> 可以认为<spanclass="math inline"><em>w</em>_<em>i</em><em>j</em></span>是独立同分布的并且均值为0方差为<spanclass="math inline">$\\sigma^2$</span>那么计算得到： <spanclass="math display">$$\\mathbb{E}\[h\_i\]=0 \\quad \\mathbb{E}\[h\_i^2\] = n\_{in} \\sigma^2\\gamma$$</span>这样在经过一个层之后数据的方差会改变，为了保持方差不变，可以使用上述的初始化方法。</p><p><strong>He Initialization</strong>：(ReLU activations) <spanclass="math display">$$Var(W)= \\frac{2}{n\_{in}}$$</span> [[权重初始化.pdf]] 其中<spanclass="math inline"><em>n</em>_<em>i</em><em>n</em></span>是这一个神经元对应的输入的神经元的个数。</p><h5 id="baby-sitting-learning">Baby Sitting Learning</h5><p>在训练的过程中，首先在较小的数据集上进行过拟和（在这个训练集上的损失函数接近0）</p><p><strong>学习率</strong> -如果一个网络训练的过程中，损失函数不变或变大，那么可能是学习率过大，可以减小学习率。- 学习率较小，可能会导致训练的过程较慢，可以增大学习率。</p><p><strong>数值爆炸</strong>： -尽量使神经元不陷入饱和区，使用上述权重的初始化方法，可以很好缓解。 -使得输入经过一定的归一化处理，可以尽量避免数值爆炸的问题。</p><p>验证误差曲线和训练误差曲线之间的差距较大，可能是过拟合的问题。可以进行早停。现在已经可以使验证误差趋近于渐近线。</p><h5 id="batch-normalization">Batch Normalization</h5><p>对于输入的数据进行归一化处理，可以加快训练的速度，同时可以减少梯度消失的问题。在训练的过程中，对于每一个batch的数据进行归一化处理，可以使得数据的分布更加稳定。<span class="math display">$$\\hat{x} = \\frac{x - \\mu}{\\sigma}$$</span> 这是一个非参数化方法。可以加入可学习的参数： <spanclass="math display">$$y = \\gamma \\hat{x} + \\beta$$</span> 其中<span class="math inline">$\\mu$</span>和<spanclass="math inline">$\\sigma$</span>是对于每一个mini-batch的均值和方差。</p><p>在CNN中，对每一个batch中的n个<span class="math inline">$w \\timesh$</span>的特征图进行归一化处理，可以使得数据的分布更加稳定。</p><p>上述是在训练的过程中使用的，在测试过程中使用不了称为<strong>训练推理失配</strong><em>traininferencemismatch</em>。可以使用EMA（指数滑动平均）的方法来进行替代。</p><p>上述要求n大概是16，在比较大的模型中，可能显存不够。上述方法有一个替代的方法<em>LayerNormalization</em>，对于每一个样本进行归一化处理。</p><p>在使用了<em>BatchNormalization</em>之后，仍然有协变量偏移<em>covariateshift</em>的问题。但是在使用<em>BatchNormalization</em>之后，<em>Lipchitz</em>系数变化更加平稳，海森矩阵也更加稳定。上述可以用数学严格证明。上述操作并不是简单的归一化，而是使得表示的函数族更加光滑，一个光滑的、凸的函数更容易优化。- Lipchitz: <span class="math display">$$\\left\\|\\nabla\_{y\_j} \\hat{\\mathcal{L}}\\right\\|^2 \\leq\\frac{\\gamma^2}{\\sigma\_j^2}\\left(\\left\\|\\nabla\_{y\_j}\\right\\|^2-\\frac{1}{m}\\left(1,\\nabla\_{y\_j}\\mathcal{L}\\right)^2-\\frac{1}{m}\\left(\\nabla\_{y\_j} \\mathcal{L},\\hat{y}\_j\\right)^2\\right)$$</span> - Smoothness: <span class="math display">$$\\gamma&lt;\\sigma \\text { in experiments }$$</span> - Hessian matrix</p><p><span class="math display">$$\\left(\\nabla\_{y\_j} \\hat{\\mathcal{L}}\\right)^T \\frac{\\partial\\hat{\\mathcal{L}}}{\\partial y\_j \\partialy\_j}\\left(\\nabla\_{y\_j} \\hat{\\mathcal{L}}\\right) \\leq\\frac{\\gamma^2}{\\sigma\_j^2}\\left(\\left(\\nabla\_{y\_j}\\mathcal{L}\\right)^T \\frac{\\partial \\mathcal{L}}{\\partial y\_j\\partial y\_j}\\left(\\nabla\_{y\_j}\\mathcal{L}\\right)-\\frac{\\gamma}{m \\sigma^2}\\left(\\nabla\_{y\_j}\\mathcal{L}, \\hat{y}\_j\\right)\\left\\|\\nabla\_{y\_j}\\hat{\\mathcal{L}}\\right\\|^2\\right)$$</span></p><h5 id="group-normalization">Group Normalization</h5><p><img src="/images/%7B139D48AB-F664-4CE8-AC05-97B772908A85%7D.png"alt="{139D48AB-F664-4CE8-AC05-97B772908A85}.png" /> 在<em>GroupNormalization</em>中，对于每一个通道的特征图进行归一化处理，这样可以减少计算的复杂度。是轻量化CNN的方法。在一定数据量较大的情况下可以达到和<em>BatchNormalization</em>差不多的结果。</p><h3 id="generalization-and-capacity">Generalization and Capacity</h3><ul><li>网络结构不同网络效果不同，如相同的层数下，全连接网络的参数量大但是和卷积网络的效果差不多。</li><li>相同的网络结构，参数量不同，参数量多的网络效果更好。</li></ul><h4id="theorem-arbitrarily-large-neural-networks-can-approximate-any-function">Theorem(Arbitrarily large neural networks can approximate any function)</h4><p>理论可以表述为：对于任意的连续函数，存在一个足够大的神经网络可以近似这个函数。<img src="/images/%7B79D51D58-D7B7-485E-9A0F-5F615FE27545%7D.png"alt="{79D51D58-D7B7-485E-9A0F-5F615FE27545}.png" />上面表示两层神经网络可以逼近任意的连续函数，要求这个函数<spanclass="math inline">$\\sigma$</span>不是多项式函数。</p><p><img src="/images/%7B862689B6-2775-4822-8FAC-B0450B360BA0%7D.png"alt="{862689B6-2775-4822-8FAC-B0450B360BA0}.png" />上面的定理表示神经网络的宽度也很重要，可以通过增加神经元的数量来逼近函数。</p><p>在空间折叠的问题中，表明<strong>深度比宽度更加重要</strong>。</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-3</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-3/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-3/</url>
    
    <content type="html"><![CDATA[<h2 id="convolutional-neural-networks">Convolutional NeuralNetworks</h2><p>卷积网络最早是用来处理图像的问题。目前较为成功的研究是物体识别问题，对于物体之间的关系推断依然是计算机视觉的前沿领域。</p><p>在生物的研究中，存在<strong>感受野</strong><em>receptivefield</em>的概念，这是指神经元对于输入的局部区域的敏感程度。在卷积网络中，我们也引入了这个概念。相邻的神经元处理的是相邻的图像区域，这样的设计使得网络能够捕捉到图像的空间结构。在模拟人类的视觉系统中，重要的是<strong>提取不同程度的特征</strong>，这就是卷积网络的核心思想。</p><p>在MLP中，由于对于原图像像素进行展开，会损失部分的空间信息。在处理视角、光照的变化时，MLP的效果会变差。</p><figure><img src="/images/%7B71895E3A-0EFA-438D-8E6C-EDBD4A337F5A%7D.png"alt="{71895E3A-0EFA-438D-8E6C-EDBD4A337F5A}.png" /><figcaptionaria-hidden="true">{71895E3A-0EFA-438D-8E6C-EDBD4A337F5A}.png</figcaption></figure><p>从左到右，逐步提取的是从较低到较高的层次特征。在不同的任务中使用不同的特征。在识别人物中，应该使用较高层次的特征。</p><h5 id="local-connectivity">Local Connectivity</h5><p>每一层的神经元只连接到上一层的局部区域。具体的连接方式为：每一个神经元只关注上一层对应区域的部分神经元。</p><h5 id="parameter-sharing">Parameter Sharing</h5><p>一个神经元对应的某组参数代表的是某种特征，我们认为提取的特征的在不同的位置是一样的。是一种<strong>平移不变性</strong>。</p><h4 id="convolution">Convolution</h4><p><span class="math inline"><em>f</em></span>是输入的图像，<spanclass="math inline"><em>g</em></span>是卷积核，卷积操作定义为：<em>Continuous function</em> : <span class="math display">$$(f\*g)(t) = \\int\_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau$$</span> <em>Discrete function</em> : <span class="math display">$$(f\*g)\[n\] = \\sum\_{m=-\\infty}^{\\infty} f\[m\]g\[n-m\]$$</span> 上述操作的目的为对于输入函数</p><h5 id="cross-correlation">Cross Correlation</h5><p>互相关用来分析两个信号的相似性，定义为： <em>Continous function</em>: <span class="math display">$$(f\\star g)(t) = \\int\_{-\\infty}^{\\infty} \\overline{f(\\tau)}g(t+\\tau)d\\tau$$</span> <em>Discrete function</em> : <span class="math display">$$(f\\star g)\[n\] = \\sum\_{m=-\\infty}^{\\infty} \\overline{f\[m\]}g\[n+m\]$$</span> 对于卷积和互相关的关系为： <span class="math display">$$\[f(t) \\star g(t)\] (t) = \[\\overline{f(-t)} \* g(t)\](t)$$</span> 事实上，在卷积网络中使用的是互相关操作。</p><h5 id="notation">Notation</h5><ul><li>输入的图片一般为三层，分别为RGB。这个<em>Volume</em>是一个张量<em>Tensor</em>。</li><li>卷积核的大小为<span class="math inline">$5 \\times 5 \\times3$</span>（举例），即3个通道，每个通道大小为<span class="math inline">$5\\times5$</span>。然后在整个图片上进行平移，计算卷积操作。（类似于卷积的定义）</li><li>卷积的计算操作为：卷积核在图片上平移，计算对应位置的乘积和。 <imgsrc="/images/%7B7AC9125F-FA2D-42F9-B0CF-085ABFFB668F%7D.png"alt="{7AC9125F-FA2D-42F9-B0CF-085ABFFB668F}.png" />上面的公式是在移动到某个位置<spanclass="math inline">(<em>w</em>, <em>h</em>)</span>的时候，计算对应位置的乘积和。然后将结果存储在一个新的矩阵中。</li></ul><p><em>在实际操作的过程中，并不要将卷积核在图片上“滑动”（这是一个串行算法），可以为每一个为每一个局域来分配一个神经元，这样的操作是并行的，可以加速计算。</em></p><p>得到的是一个Feature Map，这个FeatureMap是一个新的张量，使用6个卷积核得到的图片的特征图的大小为<spanclass="math inline">$28 \\times 28 \\times6$</span>。<em>如何计算的？参数量是多少？</em>使用的卷积核数量是一个超参数，可以调整。</p><p>对于一个<span class="math inline">$w\*h\*c$</span>的输入，使用<spanclass="math inline">$k\*k\*c\*d$</span>个卷积核，得到的输出的特征图的大小为<spanclass="math inline">$(w-k+1)\*(h-k+1)\*d$</span>，参数量为<spanclass="math inline">$(k\*k\*c+1)\*d$</span>。但是CNN在实践过程中的显存占用是较大的，因为需要存储特征图。</p><p><img src="/images/%7B7202EECA-CD25-4EE1-A5C9-02F0263C968C%7D.png"alt="{7202EECA-CD25-4EE1-A5C9-02F0263C968C}.png" /> #### Dilated&amp;Stride Convolution</p><p>在卷积操作中，我们可以使用不同的步长来进行卷积操作，这样的操作会改变输出的大小。在DilatedConvolution中，我们可以使用不同的扩张率来进行卷积操作。扩张率为1的时候<em>1-dilated</em>是正常的卷积操作，扩张率为2的时候，卷积核的间隔为2<em>2-dilated</em>。这样的操作可以<strong>增加卷积核的感受野</strong>，但是减少了输出的大小。</p><p><em>Stride</em>是卷积核的步长，可以让输出的特征图迅速变小。可能出现不匹配的情况，这样可以进行<em>Padding</em>是在边缘填充0，这样的操作可以保持输出的大小。</p><h5 id="activation-functions">Activation Functions</h5><p>在卷积网络中，激活函数可以使用ReLU函数，这样的操作可以避免梯度消失的问题。有时候使用的是LeakyReLU函数，这样的操作在某些模型中可以减少神经元死亡的问题。 <spanclass="math display">$$LeakyReLU(x) = \\begin{cases} x &amp; x&gt;0 \\\\ 0.01x &amp; x \\leq 0\\end{cases}$$</span></p><figure><img src="/images/%7BCFAAFBDA-A81C-4931-9C2C-CBD64F40E29C%7D.png"alt="{CFAAFBDA-A81C-4931-9C2C-CBD64F40E29C}.png" /><figcaptionaria-hidden="true">{CFAAFBDA-A81C-4931-9C2C-CBD64F40E29C}.png</figcaption></figure><h5 id="pooling">Pooling</h5><p>池化操作一般是用来减少空间尺寸，这样的操作可以减少参数量，减少过拟合的问题。</p><ul><li>Max Pooling：取局部区域的最大值</li><li>Average Pooling：取局部区域的平均值 <imgsrc="/images/%7B01688551-C69C-4B62-8660-637A51409678%7D.png"alt="{01688551-C69C-4B62-8660-637A51409678}.png" /></li></ul><p><strong>Spatial PyramidPooling</strong>：对于不同的通道进行池化操作，这样的操作可以增加特征的多样性。这是用来减少多尺度问题的。在现实生活中，多尺度是一个基本的特征，例如在大气系统、气候系统中，有较大的气流和较小部分的湍流。</p><h3 id="back-propagation">Back Propagation</h3><p><ahref="https://www.youtube.com/watch?v=Ilg3gGewQ5U">Backpropagation,step-by-step | DL3</a></p><figure><img src="/images/%7B1197B7EA-3F5B-40C9-8ADF-861B8AACBDC2%7D.png"alt="{1197B7EA-3F5B-40C9-8ADF-861B8AACBDC2}.png" /><figcaptionaria-hidden="true">{1197B7EA-3F5B-40C9-8ADF-861B8AACBDC2}.png</figcaption></figure><p>在卷积网络的前向传播中每一层的计算公式如下： <spanclass="math display">$$z\_d^{(l+1)} = a^{(l)}\\star \\theta^{(l)}\_d + b^{(l)}\_d$$</span></p><p>在一个具体的例子中，可以计算一个<span class="math inline">$3 \\times3 \\times 1$</span>的图像在经过一个<span class="math inline">$2\\times 2\\times 1$</span>的卷积核得到的<span class="math inline">$2 \\times 2\\times 1$</span>的特征图的计算过程。</p><ul><li>Consider single input channel <spanclass="math inline"><em>c</em> = 1</span> : <spanclass="math display">$$\\left\[\\begin{array}{ll}z\_{11}^{(l+1)} &amp; z\_{12}^{(l+1)} \\\\z\_{21}^{(l+1)} &amp; z\_{22}^{(l+1)}\\end{array}\\right\]=\\left\[\\begin{array}{lll}a\_{11}^{(l)} &amp; a\_{12}^{(l)} &amp; a\_{13}^{(l)} \\\\a\_{21}^{(l)} &amp; a\_{22}^{(l)} &amp; a\_{23}^{(l)} \\\\a\_{31}^{(l)} &amp; a\_{32}^{(l)} &amp; a\_{33}^{(l)}\\end{array}\\right\] \\star\\left\[\\begin{array}{ll}\\theta\_{11}^{(l)} &amp; \\theta\_{12}^{(l)} \\\\\\theta\_{21}^{(l)} &amp; \\theta\_{22}^{(l)}\\end{array}\\right\]$$</span></li><li>Expand above to be clearer: <span class="math display">$$\\left\\{\\begin{array}{l}z\_{11}^{(l+1)}=a\_{11}^{(l)} \\theta\_{11}^{(l)}+a\_{12}^{(l)}\\theta\_{12}^{(l)}+a\_{21}^{(l)} \\theta\_{21}^{(l)}+a\_{22}^{(l)}\\theta\_{22}^{(l)} \\\\z\_{12}^{(l+1)}=a\_{12}^{(l)} \\theta\_{11}^{(l)}+a\_{13}^{(l)}\\theta\_{12}^{(l)}+a\_{22}^{(l)} \\theta\_{21}^{(l)}+a\_{23}^{(l)}\\theta\_{22}^{(l)} \\\\z\_{21}^{(l+1)}=a\_{21}^{(l)} \\theta\_{11}^{(l)}+a\_{22}^{(l)}\\theta\_{12}^{(l)}+a\_{31}^{(l)} \\theta\_{21}^{(l)}+a\_{32}^{(l)}\\theta\_{22}^{(l)} \\\\z\_{22}^{(l+1)}=a\_{22}^{(l)} \\theta\_{11}^{(l)}+a\_{23}^{(l)}\\theta\_{12}^{(l)}+a\_{32}^{(l)} \\theta\_{21}^{(l)}+a\_{33}^{(l)}\\theta\_{22}^{(l)}\\end{array}\\right.$$</span></li></ul><p>对于上述的例子计算残差网络的过程为： <span class="math display">$$\\begin{aligned}\\delta\_{ij}^{(l)} &amp; = \\frac{\\partial J(\\theta,b)}{\\partialz\_{ij}^{(l)}}  =  \\frac{\\partial J(\\theta,b)}{\\partiala\_{ij}^{(l)}} \\frac{\\partial a\_{ij}^{(l)}}{\\partialz^{(l)}\_{ij}}\\\\&amp;= \\sum\_{z\_{pq}^{(l+1)} \\in  Pa(a\_{ij}^{(l)})} \\frac{\\partialJ(\\theta,b)}{\\partial z\_{pq}^{(l+1)}} \\frac{\\partialz\_{pq}^{(l+1)}}{\\partial a\_{ij}^{(l)}} \\frac{\\partiala\_{ij}^{(l)}}{\\partial z^{(l)}\_{ij}} \\\\&amp;= \\sum\_{z\_{pq}^{(l+1)} \\in Pa(a\_{ij}^{(l)}) }\\delta\_{pq}^{(l+1)} \\frac{\\partial z\_{pq}^{(l+1)}}{\\partiala\_{ij}^{(l)}} g'(z\_{ij}^{(l)})\\end{aligned}$$</span></p><p>与全连接层的计算公式的不同在于，CNN是一个局部的计算过程，即每一个神经元的输出值并不会对后一层的所有神经元产生影响。</p><p>对于上述计算得到的残差，可以写成一个矩阵： <spanclass="math display">$$\\delta^{(l)} = \\left\[\\begin{array}{lll}\\delta\_{11}^{(l)} &amp; \\cdots &amp; \\delta\_{1n}^{(l)} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\\\delta\_{m1}^{(l)} &amp; \\cdots &amp; \\delta\_{mn}^{(l)}\\end{array}\\right\]$$</span> 在上面的例子中，我们可以计算得到： <spanclass="math display">$$\\left\[\\begin{array}{lll}\\delta\_{11}^{(l)} &amp; \\delta\_{12}^{(l)} &amp; \\delta\_{13}^{(l)}\\\\\\delta\_{21}^{(l)} &amp; \\delta\_{22}^{(l)} &amp; \\delta\_{23}^{(l)}\\\\\\delta\_{31}^{(l)} &amp; \\delta\_{32}^{(l)} &amp; \\delta\_{33}^{(l)}\\end{array}\\right\]=\\left\[\\begin{array}{cccc}0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; \\delta\_{11}^{(l+1)} &amp; \\delta\_{12}^{(l+1)} &amp; 0 \\\\0 &amp; \\delta\_{21}^{(l+1)} &amp; \\delta\_{22}^{(l+1)} &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right\] \\star\\left\[\\begin{array}{cc}\\theta\_{22}^{(l)} &amp; \\theta\_{21}^{(l)} \\\\\\theta\_{12}^{(l)} &amp; \\theta\_{11}^{(l)}\\end{array}\\right\] \\odot g^{\\prime}\\left(z^{(l)}\\right)$$</span> 比较紧凑地写出： <span class="math display">$$\\delta^{(l)} = \\delta^{(l+1)} \\star rot180(\\theta^{(l)}) \\odotg^{\\prime}(z^{(l)})$$</span> 其中<spanclass="math inline"><em>r</em><em>o</em><em>t</em>180</span>是对卷积核进行旋转180度的操作。<spanclass="math inline">$\\odot$</span>对应的是两个矩阵的逐个元素乘</p><p>在有多张特征图的情况下（输出有<spanclass="math inline"><em>d</em></span>个通道），输入的图像有<spanclass="math inline"><em>c</em></span>通道，我们可以将上述的计算过程进行扩展，得到：<span class="math display">$$\\delta^{(l)} = \\sum\_{d} \\delta^{(l+1)}\_d \\starrot180(\\theta^{(l)}) \\odot g^{\\prime}(z^{(l)})$$</span> 在计算目标函数对于参数的导数的过程中，我们可以得到： <spanclass="math display">$$\\frac{\\partial J(\\theta,b)}{\\partial \\theta\_d^{(l)}} =\\frac{\\partial J(\\theta,b)}{\\partial z\_d^{(l+1)}} \\frac{\\partialz\_d^{(l+1)}}{\\partial \\theta\_d^{(l)}}= \\delta\_d^{(l+1)} \\star a^{(l)}\_d$$</span> 可以更加详细地计算： <span class="math display">$$\\begin{aligned}\\frac{\\partial J(\\theta,b)}{\\partial \\theta\_{i,j,k,d}^{(l)}}&amp;=\\sum\_{m,n} \\frac{\\partial J(\\theta,b)}{\\partial z\_{m,n}^{(l+1)}}\\frac{\\partial z\_{m,n}^{(l+1)}}{\\partial \\theta\_{i,j,k,d}^{(l)}}\\\\&amp;= \\sum\_{m,n} \\delta\_{m,n}^{(l+1)} a\_{m+i-1,n+j-1,k}^{(l)} \\\\&amp;= \\delta\_d^{(l+1)} \\star a^{(l)}\_d\\end{aligned}$$</span> 对于偏置项的计算过程为 <span class="math display">$$\\frac{\\partial J(\\theta,b) }{\\partial b\_d^{(l)}} = \\sum\_{m,n}\\frac{\\partial J(\\theta,b)}{\\partial z\_{m,n}^{(l+1)}}\\frac{\\partial z\_{m,n}^{(l+1)}}{\\partial b\_d^{(l)}} = \\sum\_{m,n}\\delta\_{m,n}^{(l+1)}$$</span></p><p>从而可以使用<strong>动量的梯度</strong>下降的方法来进行参数的更新。</p><h3 id="invariance-and-equivariance">Invariance and Equivariance</h3><p>对于不同的任务，有时候需要不变性和等变性。在图像处理中，对于图像的旋转、平移、缩放等操作，神经网络的输出应该是不变的。在神经网络中，可以通过数据增强的方法来进行处理。对于等变性，可以通过卷积神经网络来进行处理。CNN在设计的过程中，引入池化层<em>pooling</em>希望获得不变性。但是并没有获得很好的效果。- <strong>Invariance</strong>：对于输入的变化，输出不变 <spanclass="math display"><em>f</em>(<em>T</em>(<em>X</em>)) = <em>f</em>(<em>x</em>)</span>- <strong>Equivariance</strong>：对于输入的变化，输出也会发生相应的变化<spanclass="math display"><em>f</em>(<em>T</em>(<em>X</em>)) = <em>T</em>(<em>f</em>(<em>x</em>))</span>在实践中，有时候会有噪声、形变、翻转、光照条件、视角变化、遮挡、尺度变化、类内类间差距、奇异等问题。</p><h4 id="data-augmentation">Data Augmentation</h4><p>在实践中，可以通过数据增强的方法来进行处理。对于图像的旋转、平移、缩放、翻转等操作，可以增加数据的多样性。在训练的过程中，可以使用不同的数据增强的方法来进行训练。用这样的方法期望获得一种不变性。</p><p>常用的方法有：裁剪、旋转、翻转、缩放、平移、仿射变换、弹性变换、颜色变换等。CNN识别主要使的是纹理的识别方法，希望将人类的对于形状识别的能力融入到CNN中。可以使用的数据集为<em>Augmentationby Stylization</em>，希望获得模型对于纹理的不变性。CNN对上下文是敏感的，对于经常出现在一起的事物，CNN可以很好地进行识别，但是对于不常见的事物，CNN的效果会变差。一种极端的方式是将不经常出现的东西组合在一起。目前的深度网络一定程度上利用了<em>spuriouscorrelation</em>，从而进行bench mark在数据集上过拟和。</p><h4 id="architecture-revolution">Architecture Revolution</h4><h5 id="maga-making-convolutional-networks-shift-invariant-again">MAGAMaking convolutional networks shift-invariant again</h5><p>在CNN中，在平移过程中，网络很难获得平移不变性。在采用模糊之后的<em>pooling</em>可以获得一定程度上的平移不变性。</p><h5 id="capsule-network">Capsule Network</h5><p>CNN的缺点还有：不能保持物体之间的相对关系。在分类任务中，一般情况下强调的是不变性，而在一些细粒度识别任务中，强调的是等变性。在CapsuleNetwork中，引入了胶囊的概念，这样的操作可以保持物体之间的相对关系。</p><h2 id="cnn-architectures">CNN Architectures</h2><h3 id="alexnet">AlexNet</h3><figure><img src="/images/%7B382F2347-B42F-46CB-83DB-F25DDE49A91E%7D.png"alt="{382F2347-B42F-46CB-83DB-F25DDE49A91E}.png" /><figcaptionaria-hidden="true">{382F2347-B42F-46CB-83DB-F25DDE49A91E}.png</figcaption></figure><ul><li>一般会在输入层使用较大的卷积核，然后在后面的层使用较小的卷积核</li><li>通道数随着网络逐渐增加然后减少</li><li>第一次使用ReLU激活函数</li><li>使用了大量的数据增强<em>Data Augmentation</em></li><li>使用GPU进行训练</li><li>采用SGD with momentum 0.9进行训练</li><li>使用dropout 0.5，一般在MLP层都需要使用dropout</li><li>使用0.01的学习率，然后在训练的过程中逐渐减小学习率，当loss不再下降的时使用学习率的0.1倍#### ZFNet</li></ul><p>在输入层使用了更小的卷积核，一般不要使用小的卷积核（更大的stride）会导致信息的丢失。在中间层数使用了更多的通道数。</p><h3 id="vggnet">VGGNet</h3><p>用更小的卷积核来代替较大的卷积核，这样的操作可以减少参数量，增加网络的深度。在VGGNet中，使用了3个<spanclass="math inline">$3 \\times 3$</span>的卷积核来代替一个<spanclass="math inline">$7 \\times7$</span>的卷积核。这样的操作可以增加网络的深度，减少参数量。<strong>一个较大的感受野可以通过多个较小的感受野来代替</strong>。但是现在又发现事实上没有这么好的效果，所以现在又开始使用较大的卷积核。</p><p>采用了预训练的方法。在训练的过程中，首先在较小的数据集上进行过拟和（在这个训练集上的损失函数接近0），然后在较大的数据集上进行训练。</p><p>在早期的网络中，池化层的使用是较多的，现在已经很少使用。 ### NINNetwork in Network</p><p>在使用全连接层时，会有较多的参数，基于这样的思路提出<spanclass="math inline">$1\\times1$</span>卷积这个操作。这样的操作只改变通道数，不改变空间尺寸，通常在不希望改变空间尺寸而增大通道数的时候可以使用。</p><p>在使用卷积增加步长、使用<em>pooling</em>层时候，会导致信息的丢失。在NIN中希望获得一个尺寸小但是通道数多的特征图。在NIN中使用了<spanclass="math inline">$1\\times1$</span>的卷积核来增加通道数，这样可以使用空间全局池化。例如一个<spanclass="math inline">$256\\times 6 \\times 6$</span>的特征图，使用<spanclass="math inline">$6\\times 6$</span>的全局池化，可以得到一个<spanclass="math inline">$256\\times 1 \\times 1$</span>的特征图。</p><blockquote><p>[!NOTE] -传统CNN末尾通常使用全连接层（FC）进行分类，但全连接层参数量大，易过拟合。- 全局池化可直接将特征图转换为通道维度的向量（如<spanclass="math inline">$1\\times 1 \\times1024$</span> <spanclass="math inline">$\\rightarrow$</span>1024维向量），再接一个分类层，大幅减少参数。</p></blockquote><h4 id="googlenet">GoogLeNet</h4><p><img src="/images/%7B43BF44F4-D211-4165-AE84-257E702582A0%7D.png"alt="{43BF44F4-D211-4165-AE84-257E702582A0}.png" />这是一个较为深的网络，删去了全连接层，参数量是较小的。 -引入Multi-passway，使用多路的卷积核来提取特征。采用了特征增广<em>FeatureAugmentation</em>的方法。 -在特征图通道数目不一样的情况下，使用padding的方法。</p><figure><img src="/images/%7B6F8AFF72-4C3D-4FAD-8FC9-90F7FED6E8B2%7D.png"alt="{6F8AFF72-4C3D-4FAD-8FC9-90F7FED6E8B2}.png" /><figcaptionaria-hidden="true">{6F8AFF72-4C3D-4FAD-8FC9-90F7FED6E8B2}.png</figcaption></figure><figure><img src="/images/%7B39D210C3-7C1B-4913-AC78-12E71A8FD13B%7D.png"alt="{39D210C3-7C1B-4913-AC78-12E71A8FD13B}.png" /><figcaptionaria-hidden="true">{39D210C3-7C1B-4913-AC78-12E71A8FD13B}.png</figcaption></figure><figure><img src="/images/%7BE3B121C1-5782-4762-A983-042799DA4ED9%7D.png"alt="{E3B121C1-5782-4762-A983-042799DA4ED9}.png" /><figcaptionaria-hidden="true">{E3B121C1-5782-4762-A983-042799DA4ED9}.png</figcaption></figure><p>使用计算量越来越小的卷积核，使用计算量越来越小的卷积核，使用越来越多的层数，使用越来越多的通道数。</p><h4 id="highway-network">Highway Network</h4><p>在平坦的网络通路中，可能有信息通路瓶颈问题。在HighwayNetwork中，引入了门控机制，这样的操作可以使得信息的流动更加顺畅。 <spanclass="math display"><em>y</em> = <em>H</em>(<em>x</em>, <em>W</em>_<em>H</em>)<em>T</em>(<em>x</em>, <em>W</em>_<em>T</em>) + <em>x</em>(1 − <em>T</em>(<em>x</em>, <em>W</em>_<em>T</em>))</span>其中<spanclass="math inline"><em>H</em>(<em>x</em>, <em>W</em>_<em>H</em>)</span>是一个MLP，<spanclass="math inline"><em>T</em>(<em>x</em>, <em>W</em>_<em>T</em>)</span>是一个门控函数，这样的操作可以使得信息的流动更加顺畅。</p><ul><li><p><span class="math inline"><em>x</em></span>: 输入向量。</p></li><li><p><spanclass="math inline"><em>H</em>(<em>x</em>, <em>W</em>_<em>H</em>)</span>:非线性变换（如全连接层或卷积层）。</p></li><li><p><spanclass="math inline"><em>T</em>(<em>x</em>, <em>W</em>_<em>T</em>)</span>:TransformGate（变换门），控制非线性变换的权重，通常用Sigmoid激活（输出值在0到1之间）。</p></li><li><p><spanclass="math inline"><em>C</em>(<em>x</em>, <em>W</em>_<em>C</em>)</span>:Carry Gate（携带门），控制原始输入 x 的权重。通常设定为 <spanclass="math inline"><em>C</em> = 1 − <em>T</em></span>，以减少参数量。</p></li><li><p>当 <spanclass="math inline"><em>T</em>(<em>x</em>) → 0</span> 时，输出<spanclass="math inline"><em>y</em> ≈ <em>x</em></span>，即当前层几乎不进行变换（信息直接跳过该层）。</p></li><li><p>当 <span class="math inline"><em>T</em>(<em>x</em>) → 1</span>时，输出<spanclass="math inline"><em>y</em> ≈ <em>H</em>(<em>x</em>)</span>，即信息完全经过当前层的非线性变换。</p></li><li><p>通过这种方式，网络可以自适应地选择浅层或深层的特征。</p></li></ul><p><span class="math display">$$T(x,W\_T) = \\sigma(W\_T^T x + b\_T)$$</span> 动机是在网络中<strong>提高信息的流动性</strong></p><ul><li><strong>ResNet</strong>：使用恒等跳跃连接<em>Identity SkipConnection</em>，公式为 <spanclass="math inline"><em>y</em> = <em>H</em>(<em>x</em>) + <em>x</em></span>，无门控机制。</li><li><strong>HighwayNetwork</strong>：通过门控动态调节跳跃连接的权重，更灵活地控制信息流。#### ResNet</li></ul><figure><img src="/images/%7B6767A6D4-59B6-4B62-9FE9-427D009BB837%7D.png"alt="{6767A6D4-59B6-4B62-9FE9-427D009BB837}.png" /><figcaptionaria-hidden="true">{6767A6D4-59B6-4B62-9FE9-427D009BB837}.png</figcaption></figure><ul><li>56层的模型比20层的模型效果更差，既然如此，先将20层的模型训练好，然后再增加36层的<em>identity</em>网络，之后再训练整个网络。发现这样的操作的效果反而更差。得出的结论是56层的网络更加难以训练，网络的拟和能力不足。<strong>平坦的网络很难拟和</strong></li><li>残差是更加容易拟和的。</li><li>继续将这些残差网络堆叠在一起，可以得到一个更深的网络。</li><li>卷积、池化是算子<em>operator</em>，残差是一个块<em>block</em>，网络是一个层<em>layer</em></li></ul><figure><img src="/images/%7B8F67BE27-E8CA-4921-99B2-C57FFFAE5E7B%7D.png"alt="{8F67BE27-E8CA-4921-99B2-C57FFFAE5E7B}.png" /><figcaptionaria-hidden="true">{8F67BE27-E8CA-4921-99B2-C57FFFAE5E7B}.png</figcaption></figure><figure><img src="/images/%7B681C5361-F2ED-427F-86C2-48C9EC68AADF%7D.png"alt="{681C5361-F2ED-427F-86C2-48C9EC68AADF}.png" /><figcaptionaria-hidden="true">{681C5361-F2ED-427F-86C2-48C9EC68AADF}.png</figcaption></figure><p><img src="/images/%7B5DE8620F-D5FB-4E82-96D7-0E03CCCCAF82%7D.png"alt="{5DE8620F-D5FB-4E82-96D7-0E03CCCCAF82}.png" />对于网络会有多个维度进行评价： - 准确率top-1、top-5准确率 -VGG网络参数量很大，但是由于是平坦的网络，所以有一定的计算整齐度。在网络有较多的分叉时，计算整齐度会变差。较为整齐的网络计算是较快的。- 目前的显卡对于<span class="math inline">$3 \\times3$</span>的卷积核是有硬件加速的</p><h3 id="landscape-visualization">LandScape Visualization</h3><p><a href="https://arxiv.org/abs/1712.09913">[1712.09913] Visualizingthe Loss Landscape of Neural Nets</a></p><h2 id="lightweight-for-deployment">Lightweight for Deployment</h2><h3 id="pruning">Pruning</h3><p>卷积神经网络是一个很适合做剪枝的网络。说明神经网络的有很多的参数是冗余的。剪枝配合上重训练可以减少网络的参数量，并且在一定的程度上提升网络的性能。</p><h4 id="quantization-and-encoding">Quantization and Encoding</h4><p>k-means算法可以将权重量化，将权重量化为几个值，这样的操作可以减少网络的参数量。在实际的操作中，可以将权重量化为8位，这样的操作可以减少网络的参数量。</p><p>编码操作，如Huffman编码，可以减少网络的参数量。 <ahref="https://arxiv.org/abs/1510.00149">[1510.00149] Deep Compression:Compressing Deep Neural Networks with Pruning, Trained Quantization andHuffman Coding</a></p><p>神经网络一般是在训练的时候使用较大的网络，然后再裁剪为较小的网络，反之效果不一定会好。</p><p>我们相信在裁剪的过程中，保留下来的参数是重要的参数，这样的操作可以提升网络的性能。在实验中</p><p><a href="https://arxiv.org/abs/1803.03635">[1803.03635] The LotteryTicket Hypothesis: Finding Sparse, Trainable Neural Networks</a></p><p>先设计一个较小的网络，之后推广到较大的网络。在实际的操作中，可以使用较小的网络，然后再增加网络的深度。这种设计是硬件友好的，由于这样的设计是计算对齐的。</p><h4 id="group-convolution">Group Convolution</h4><p>标准的卷积层有这样的不足：对于不同的输入通道，后面的输出都使用到了所有的输入通道。在GroupConvolution中，将输入通道分为几个组，然后对于每一个组使用一个卷积核。这样的操作可以减少网络的参数量，减少计算量。</p><p>但是通道之间不交流，这样的操作可能会导致网络的性能下降。</p><h4 id="depthwise-separable-convolution">Depthwise SeparableConvolution</h4><ul><li>引入了<span class="math inline">$1 \\times1$</span>卷积核<em>pointwise</em>，这样的操作可以减少参数量</li><li>令通道数等于分组数目<em>channelwise</em> or <em>depthwise</em></li></ul><p>上述两个算子是轻量化网络的基础。</p><figure><img src="/images/Pasted%20image%2020250316132626.png"alt="Pasted image 20250316132626.png" /><figcaption aria-hidden="true">Pasted image20250316132626.png</figcaption></figure><p><a href="https://arxiv.org/abs/2201.03545">[2201.03545] A ConvNet forthe 2020s</a></p><figure><img src="/images/Pasted%20image%2020250316134751.png"alt="Pasted image 20250316134751.png" /><figcaption aria-hidden="true">Pasted image20250316134751.png</figcaption></figure><p>宏观设计：<strong>减少空间尺寸的衰减有利于学习不同的特征</strong>。</p><h2 id="advanced-modules">Advanced Modules</h2><h3 id="d-modeling">3D Modeling</h3><p>普通的二维卷积网络在处理视频时，会丢失时间信息。在3D卷积网络中，引入了时间维度，这样的操作可以保留时间信息。</p><p>直接使用3D卷积可以进行直接的对应。</p><h4 id="deforamble-convolution">Deforamble Convolution</h4><p>在CNN中，形变较难进行建模，对于形变的物体效果会变差。在识别的过程中，在识别的过程中可能引入噪音。在SpatialTransformerNetwork中，引入了形变的概念，类似于视觉中进行防抖的操作。</p><p>在CNN中引入偏置的概念，在对应的特征图上进行坐标的偏置操作 <spanclass="math display">$$y(p\_0) = \\sum\_{p\_i \\in \\Omega} w\_i x(p\_i+p\_0+\\Delta p\_0)$$</span> #### Attention</p><p>要建模大范围的特征范围，就是high-level的特征，但是在使用CNN的过程中，有效感受野并没有那么大。</p><p><span class="math display">$$y\_i = \\frac{1}{\\mathcal{C}(x)}\\sum\_{\\forall j} f(x\_i,x\_j)g(x\_j)$$</span> 对于注意力函数<spanclass="math inline"><em>f</em>(<em>x</em>_<em>i</em>, <em>x</em>_<em>j</em>)</span>可以计算为两个参数的内积。上述操作实际上是一种全局建模，可以在CNN的最后几步使用。</p><h4 id="cam">CAM</h4><p>作为一种可解释性的工作，CAM可以将CNN的输出映射到输入图像上，这样的操作可以直观地看到CNN的输出。计算不同部分的权重，可以得到不同部分的重要性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-4</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-4/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-4/</url>
    
    <content type="html"><![CDATA[<h2 id="optimization">Optimization</h2><p><strong>Objective Function</strong> <span class="math display">$$\\arg \\min \\mathcal{O(D,\\theta)}=  \\sum\_{i=1}^{N} L(y\_i,f(x\_i,\\theta)) + \\Omega(\\theta)$$</span> 上述目标可以可视化为以<spanclass="math inline">$\\theta$</span>为横坐标、<spanclass="math inline">$\\mathcal{O}$</span>为纵坐标的函数图像，我们的目标是找到函数图像的最低点。这是一个<strong>非凸优化</strong>问题，是初值敏感的。“地形图”是否简单是网络训练是否容易的关键。</p><h3 id="first-order-optimization">First-Order Optimization</h3><p>可以将<span class="math inline">$J(\\theta)$</span>展开为泰勒级数：<span class="math display">$$J(\\theta) = J(\\theta\_0) + \\nabla J(\\theta\_0)^T(\\theta -\\theta\_0) + \\frac{1}{2}(\\theta - \\theta\_0)^T H(\\theta -\\theta\_0)$$</span> 沿着梯度的方向进行更新： <span class="math display">$$J(\\theta - \\eta g) = J(\\theta ) -  \\eta g^Tg \\leq J(\\theta)$$</span></p><p><strong>梯度下降算法的问题在于</strong>： - 容易在鞍点处停滞 -对于较为简单的凸优化问题，学习率的选择不好都会有发散的问题，训练对于学习率是很敏感的</p><p>对于学习率下降的算法，主流使用的是<em>StepStrategy</em>，即损失函数不下降了就减少学习率。</p><h4 id="warm-restarts">Warm Restarts</h4><p>使用的策略为：<em>Cosine Annealing</em>: <spanclass="math display">$$\\eta\_t = \\eta\_{min}^i + \\frac{1}{2}(\\eta\_{max}^i -\\eta\_{min}^i)(1 + \\cos(\\frac{T\_{cur}}{T\_{i}}\\pi))$$</span></p><p>其中<spanclass="math inline"><em>T</em>_<em>c</em><em>u</em><em>r</em></span>为当前的迭代次数，<spanclass="math inline"><em>T</em>_<em>i</em></span>为当前的周期数，<spanclass="math inline">$\\eta\_{min}^i$</span>和<spanclass="math inline">$\\eta\_{max}^i$</span>分别为第<spanclass="math inline"><em>i</em></span>个周期的最小和最大学习率。学习率的衰减不能是线性的，是先快后慢的。</p><h4 id="convergence-rate">Convergence Rate</h4><ul><li>We assume that <span class="math inline">$J(\\theta)$</span> isconvex, differentiable and Lipchitz by constant <spanclass="math inline"><em>L</em></span>. And domain of <spanclass="math inline">$\\theta$</span> is bounded by radius <spanclass="math inline"><em>R</em></span>. With gradient descent update:<span class="math display">$$\\theta^{t+1}=\\theta^t-\\eta \\nabla J\\left(\\theta^t\\right)$$</span></li></ul><p><span class="math display">$$\\begin{aligned}&amp;J\\left(\\theta^t\\right)-J\\left(\\theta^\*\\right)  &amp; \\text{ (Gap btw minimum value } \\left.J\\left(\\theta^\*\\right)\\right)\\\\\\leq &amp; \\nablaJ\\left(\\theta^t\\right)\\left(\\theta^t-\\theta^\*\\right) &amp;\\text { (Convexity) } \\\\= &amp;\\frac{1}{\\eta}\\left(\\theta^t-\\theta^{t+1}\\right)^{\\mathrm{T}}\\left(\\theta^t-\\theta^\*\\right)  &amp;\\text { (Definition of GD) } \\\\=&amp; \\frac{1}{2\\eta}\\left(\\left\\|\\theta^t-\\theta^\*\\right\\|^2+\\left\\|\\theta^{t+1}-\\theta^t\\right\\|^2-\\left\\|\\theta^{t+1}-\\theta^\*\\right\\|^2\\right)&amp; \\text { (Properties of Norm) } \\\\= &amp; \\frac{1}{2\\eta}\\left(\\left\\|\\theta^t-\\theta^\*\\right\\|^2-\\left\\|\\theta^{t+1}-\\theta^\*\\right\\|^2\\right)+\\frac{\\eta}{2}\\left\\|\\nablaJ\\left(\\theta^t\\right)\\right\\|^2 &amp; \\text { (Definition of GD)} \\\\\\leq &amp;\\frac{1}{2\\eta}\\left(\\left\\|\\theta^t-\\theta^\*\\right\\|^2-\\left\\|\\theta^{t+1}-\\theta^\*\\right\\|^2\\right)+\\frac{\\eta}{2}L^2 &amp;  \\text { (Lipchitz Property) }\\end{aligned}$$</span></p><ul><li>From previous computation, we get the following inequality for everystep <span class="math inline"><em>t</em></span> :</li></ul><p><span class="math display">$$J\\left(\\theta^t\\right)-J\\left(\\theta^\*\\right) \\leq \\frac{1}{2\\eta}\\left(\\left\\|\\theta^t-\\theta^\*\\right\\|^2-\\left\\|\\theta^{t+1}-\\theta^\*\\right\\|^2\\right)+\\frac{\\eta}{2}L^2$$</span></p><ul><li>Recall <span class="math inline">$\\max \_{\\theta,\\theta^{\\prime}}\\left(\\left\\|\\theta-\\theta^{\\prime}\\right\\|\\right)\\leq R$</span>. Assume we update parameters for <spanclass="math inline"><em>T</em></span> steps. We add all equations forall <span class="math inline">$t \\in\\{0,1, \\ldots, T-1\\}$</span>:</li></ul><p><span class="math display">$$\\begin{aligned}&amp;\\sum\_t\\left(J\\left(\\theta^t\\right)-J\\left(\\theta^\*\\right)\\right)\\leq \\frac{1}{2\\eta}\\left(\\left\\|\\theta^0-\\theta\_\*^\*\\right\\|^2-\\left\\|\\theta^T-\\theta^\*\\right\\|^2\\right)+\\frac{\\etaL^2 T}{2} \\\\&amp; \\frac{1}{T} \\sum\_tJ\\left(\\theta^t\\right)-J\\left(\\theta^\*\\right) \\leq \\frac{1}{2\\eta T}\\left(R^2+0\\right)+\\frac{\\eta L^2}{2} \\\\&amp; \\frac{1}{T} \\sum\_tJ\\left(\\theta^t\\right)-J\\left(\\theta^\*\\right) \\leq \\frac{R^2}{2\\eta T}+\\frac{\\eta L^2}{2}\\end{aligned}$$</span></p><ul><li>let <span class="math inline">$\\eta =\\frac{R}{L\\sqrt{T}}$</span>: <span class="math display">$$\\frac{1}{T} \\sum\_tJ\\left(\\theta^t\\right)-J\\left(\\theta^\*\\right) \\leq \\frac{L}{R}\\sqrt{T}$$</span></li></ul><h3 id="second-order-optimization">Second-Order Optimization</h3><p>不仅要关注一次的梯度信息，还要关注二次的信息。二阶优化算法的核心是Hessian矩阵，可以分辨是不是鞍点。函数的展开为： <span class="math display">$$J(\\theta) = J(\\theta\_0) + \\nabla J(\\theta\_0)^T(\\theta -\\theta\_0) + \\frac{1}{2}(\\theta - \\theta\_0)^T H(\\theta -\\theta\_0)$$</span> 能感知到“地形图”中的曲率。 对于海森矩阵可以进行特征值分解：<span class="math display">$$H = Q \\Lambda Q^T \\quad \\text{and} \\quad H^{-1} = Q \\Lambda^{-1}Q^T$$</span>特征值中较大和较小的特征值如果相差较大，称为病态矩阵；如果从最大到最小的变化较为平缓，则较为光滑。如果特征值全为正值，那么就是凸函数；如果有正有负，那么就是鞍点。事实上，使用的梯度方法为局部的方法，下降是相对较慢的。 #### Newton’sMethod 牛顿法的计算方法为： <span class="math display">$$\\hat{J}(\\theta) = J(\\theta\_0) + \\nabla J(\\theta\_0)^T(\\theta -\\theta\_0) + \\frac{1}{2}(\\theta - \\theta\_0)^T H(\\theta -\\theta\_0)$$</span> 求导得到： <span class="math display">$$\\nabla\_{\\theta} \\hat{J}(\\theta) = \\nabla\_{\\theta} J(\\theta\_0)+ H(\\theta - \\theta\_0) = 0$$</span> 求解得到： <span class="math display">$$\\theta^{t+1} = \\theta^{t} - H^{-1} \\nabla\_{theta} J(\\theta^{t})$$</span>牛顿法的优点在于收敛速度快，但是缺点在于计算复杂度高，需要计算海森矩阵的逆矩阵。计算复杂度为<spanclass="math inline"><em>O</em>(<em>d</em><sup>3</sup>)</span>，其中<spanclass="math inline"><em>d</em></span>为参数的个数。<strong>在深度学习时代基本上不再使用</strong>。</p><h4 id="quasi-newton-method">Quasi-Newton Method</h4><p>对于海森矩阵的逆矩阵，我们可以使用拟牛顿法进行近似： <spanclass="math display">$$H\_{t+1}^{-1} = H\_t^{-1} + \\frac{y\_t y\_t^T}{y\_t^T s\_t} -\\frac{H\_t^{-1}s\_t s\_t^T H\_t^{-1}}{s\_t^T H\_t^{-1}s\_t}$$</span> 其中<span class="math inline">$y\_t = \\nablaJ(\\theta\_{t+1}) - \\nabla J(\\theta\_t)$</span>，<spanclass="math inline">$s\_t = \\theta\_{t+1} - \\theta\_t$</span>。<strong>在矩阵计算的时候要将较小的矩阵先乘，这样可以计算复杂度</strong></p><h2 id="optimization-in-deep-learning">Optimization in DeepLearning</h2><p>[[Deep Learning Lecture-2#Optimization in Practice]]</p><p>是非凸优化问题，优化的目的在于找到一个较好的局部极值。比较好的局部极值是比较低的、比较平缓的局部极值，对于比较陡峭的局部极值泛化能力比较差（对测试数据的微小变化敏感）。</p><p>好的局部极值有一些特性： - 值比较低 -是”盆地“，这样有利于模型的泛化</p><h3 id="mini-batch">mini-batch</h3><p><strong>mini-batchSGD</strong>：在每一轮遍历<em>epoch</em>后，对数据进行随机的打乱<em>Shuffle</em>，然后分成若干个batch，对每一个batch进行参数的更新。这样可以减少计算的时间，同时可以减少过拟合的问题。</p><ul><li>mini-batch的大小对于训练的影响，一般而言较大的mini-batch会有更好的收敛性，但是计算复杂度更高。</li><li>由于不一样的小样本选择会引入一定的随机性，这样是有利于跳出局部极值的。</li><li>由于mini-batch的选择是有随机性的，不同的batch的难度不一样，所以这时候出现Loss的规律性的震荡是很正常的。</li><li>矩阵最大奇异值与最小奇异值的比值称为矩阵的条件数，条件数越大，矩阵越病态。对于病态矩阵，SGD的收敛速度会变慢。</li></ul><h3 id="learning-rate-decay">Learning Rate Decay</h3><p>初始学习率较大，随着迭代次数的增加，学习率逐渐减小。有相对应的衰减策略。<em>Exponential decay</em>: <span class="math display">$$\\eta\_t = \\eta\_0 \\cdot e^{-\\alpha t}$$</span> <em>Inverse decay</em>: <span class="math display">$$\\eta\_t = \\frac{\\eta\_0}{1+\\alpha t}$$</span> ### SGD Stochastic Gradient Descent #### SGD with Momentum</p><p><strong>SGD with Momentum</strong>:</p><p>对于下面的更新公式：</p><p><span class="math display">$$\\theta\_{ij} = \\theta\_{ij} - \\eta \\Delta$$</span>在高维中，地形是相对而言较为崎岖的，这里的学习率一般是比较小的，否则容易发散。在接近于局部极值的时候。较大的学习率学习的是较为粗糙的特征，较小的学习率学习的是较为细致的特征。</p><p><span class="math display">$$\\Delta = \\beta \\Delta - \\eta \\frac{\\partial J(\\theta)}{\\partial\\theta\_{ij}}$$</span> <spanclass="math inline">$\\beta$</span>是动量参数，可以理解为之前的梯度的累积。</p><p><strong>Nesterov Momentum:</strong> <span class="math display">$$\\begin{aligned}&amp;\\tilde{\\theta}^{t} = \\theta^{t} - \\beta \\Delta^{t-1} \\\\&amp;\\Delta^{t} = \\beta \\Delta^{t-1} + (1-\\beta)\\nabla J^t(\\tilde{\\theta}^t)\\\\&amp;\\theta^{t+1} = \\theta^t - \\eta \\Delta^t\\\\\\end{aligned}$$</span>在深度学习的实现中使用的一般是这种。走动量的方向可以减少震荡，同时可以加速收敛。当到达了比较好的局部极值时候又会在这个值的附近抖动。超参数：<span class="math inline">$\\beta$</span>，一般而言<spanclass="math inline">$\\beta$</span>取0.9是比较好的，越大的值越容易进行震荡。学习率0.01、0.003、0.001一般按照指数变化。</p><p>是在每次更新完<spanclass="math inline">$\\theta$</span>之后（进行试探之后才进行计算）才进行梯度的计算，可以避免一些<em>overshoot</em>。核心的思想为多获取一些二次的信息。</p><h3 id="weight-decay"><em>Weight Decay</em></h3><p>加入正则项，对于参数的更新进行限制，控制假设空间的大小，可以防止过拟合。但是在深度学习中并不够。</p><p><em>L1 regularization</em> <span class="math display">$$\\Omega(\\theta) = \\lambda \\sum\_{l=1}^{L} \\sum\_{i=1}^{n\_l}\\sum\_{j=1}^{n\_{l+1}} |\\theta\_{ij}^{(l)}|$$</span> <em>L2 regularization</em> <span class="math display">$$\\Omega(\\theta) = \\lambda \\sum\_{l=1}^{L} \\sum\_{i=1}^{n\_l}\\sum\_{j=1}^{n\_{l+1}} (\\theta\_{ij}^{(l)})^2$$</span></p><h3 id="adaptive-learning-rate">Adaptive Learning Rate</h3><p>直观理解为在不同的“地形”上需要使用的学习率（步长）是不一样的。对于不同的参数使用不同的学习率。<strong>Adagrad</strong>算法的核心思想为：<span class="math display">$$\\begin{aligned}&amp;r^t = r^{t-1} + \\nabla J^t(\\theta^t) \\odot \\nablaJ^t(\\theta^t)\\\\&amp;h^t = \\frac{1}{\\sqrt{r^t} + \\delta} \\\\&amp;\\Delta^t = h^t \\odot \\nabla J^t(\\theta^t)\\\\&amp;\\theta^{t+1} = \\theta^t - \\eta \\Delta^t\\end{aligned}$$</span> <em>上述公式中的第二行为逐元素操作</em> 其中<spanclass="math inline">$\\odot$</span>为对应元素相乘，<spanclass="math inline">$\\delta$</span>为一个很小的数，防止分母为0。这样可以保证在不同的地形上使用不同的学习率。<strong>本质上为探索”地形图”</strong>。但是Adagrad的问题在于随着迭代次数的增加，分母会变得越来越大，导致学习率会变得越来越小，最终会导致学习率为0，这样就不再更新了。</p><p><strong>RMSprop</strong>算法的核心思想为：对Adagrad的分母进行指数滑动平均：<span class="math display">$$\\begin{aligned}&amp;r^t = \\rho r^{t-1} + (1-\\rho)\\nabla J^t(\\theta^t) \\odot\\nabla J^t(\\theta^t)\\\\&amp;h^t = \\frac{1}{\\sqrt{r^t} + \\delta} \\\\&amp;\\Delta^t = h^t \\odot \\nabla J^t(\\theta^t)\\\\&amp;\\theta^{t+1} = \\theta^t - \\eta \\Delta^t\\end{aligned}$$</span></p><p><strong>Adam</strong>算法的核心思想为：结合了SGD withMomentum和RMSprop： <span class="math display">$$\\begin{aligned}&amp; r^t = \\rho r^{t-1} + (1-\\rho)\\nabla J^t(\\theta^t) \\odot\\nabla J^t(\\theta^t)\\\\&amp; h^t = \\frac{1}{\\sqrt{r^t} + \\delta} \\\\&amp; s^t = \\varepsilon s^{t-1} + (1-\\epsilon)\\nablaJ^t(\\theta^t)\\\\&amp; \\Delta^t = h^t \\odot s^t \\\\&amp; \\theta^{t+1} = \\theta^t - \\eta \\Delta^t\\end{aligned}$$</span> 实际使用的参数为<spanclass="math inline">$\\rho=0.9$</span>，<spanclass="math inline">$\\varepsilon = 0.9$</span>，<spanclass="math inline">$\\rho = 0.999$</span>对于学习率的下降，还是要使用对应的算法，对于实际使用的算法，还需要对<spanclass="math inline"><em>r</em></span>、<spanclass="math inline"><em>s</em></span>进行无偏修正。</p><p>Nadam算法为Adam算法的变种，对于SGD withMomentum的更新进行了修正。</p><p>调参一般而言是，对于一个模型找到对于其最好的优化器。</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-5</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-5/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-5/</url>
    
    <content type="html"><![CDATA[<h2 id="recurrent-network">Recurrent Network</h2><h3 id="sequence-model">Sequence Model</h3><p>序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是<strong>捕捉序列中的上下文</strong>。</p><h3 id="basic-principle">Basic Principle</h3><h4 id="local-dependency">Local Dependency</h4><p><strong>LocalDependency</strong>：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元素的。这种依赖关系是<strong>局部的</strong>。</p><p><span class="math display">$$P(x\_1,x\_2,\\dots,x\_T) = \\prod\_{t=1}^{T}P(x\_t|x\_1,\\dots,x\_{t-1}) =\\prod\_{t=1}^{T} g(s\_{t-2},x\_{t-1})$$</span></p><p>如果引入马尔科夫性，那么损失的信息太多了。因此我们引入一个隐藏状态<spanclass="math inline"><em>s</em>_<em>t</em></span>，<spanclass="math inline"><em>s</em>_<em>t</em></span>的信息是前面的元素的信息的一个编码。假设第<spanclass="math inline"><em>t</em></span>时间的元素的信息都被编码到了<spanclass="math inline"><em>s</em>_<em>t</em> − 2</span>和<spanclass="math inline"><em>x</em>_<em>t</em> − 1</span>中，<spanclass="math inline"><em>s</em>_<em>t</em> − 2</span>是一个向量，<spanclass="math inline"><em>g</em></span>是一个函数，<spanclass="math inline"><em>s</em>_<em>t</em> − 2</span>是一个隐藏状态。我们认为时间上也存在一个感受野。这个思考过程和人也是类似的，在处理序列问题时，也保存早些时候的一些信息。</p><p>另外也有时间上的<strong>平稳性假设</strong>，这个假设是人工智能能预测未来的理论基础。本质是时间上的独立同分布假设。</p><h4 id="parametric-sharing">Parametric Sharing</h4><p>不同时刻使用的参数是一样的，这样可以大大降低参数量。这样的模型是<strong>循环神经网络</strong>。</p><h4 id="language-model">Language Model</h4><p>语言中的建模任务为：给定一个句子的前面的单词，预测下一个单词。这个任务被称为语言模型。语言模型的目标是最大化句子的概率。语言模型的输入是一个句子，输出是一个概率分布，表示下一个单词的概率。</p><p><strong>向量化表达</strong>：可以使用one-hot向量表示单词，也可以使用词向量表示单词。每个词的表达大概需要1000维度。如果使用MLP，要将每个词的向量拼接起来，然后输入到MLP中。这样的模型不适合处理长序列，参数是非常可怕的。在MLP中丢失了一部分的信息（由于MLP的输入维度是可交换的），丢失了前后序关系。</p><h4 id="a-better-mlp">A Better MLP</h4><p><strong>n-gram</strong>：使用n-gram模型，可以考虑前n个单词的信息。</p><p><img src="/images/Pasted%20image%2020250320225821.png"alt="Pasted image 20250320225821.png" />输出的概率分布是一个softmax层，输入是一个向量，这个向量是前n个单词的向量拼接起来的。然后在得到的概率分布上进行采样。<strong>滚动预测</strong>，使用第一个次的预测结果作为第二次的输入，会有<strong>误差累计</strong>的问题。但是上述模型会有一个问题：参数量太大；对于每一个词的预测需要有一个MLP，这样的模型不适合处理长序列。### RNN</p><figure><img src="/images/Pasted%20image%2020250321083321.png"alt="Pasted image 20250321083321.png" /><figcaption aria-hidden="true">Pasted image20250321083321.png</figcaption></figure><p>循环神经网络中最重要的内容是中间的隐藏层，构建学习到的时间程度上的特征。在每个时刻输入的都是向量，称为<em>tokenembedding</em>。 上述模型在纵向方向上，从<spanclass="math inline"><em>x</em>_<em>t</em></span>到<spanclass="math inline"><em>y</em>_<em>t</em></span>就是一个前馈网络<em>feedforwardnetwork</em>（包括MLP和CNN）。在横向方向上，从<spanclass="math inline"><em>s</em>_<em>t</em> − 1</span>到<spanclass="math inline"><em>s</em>_<em>t</em></span>就是一个循环网络<em>recurrentnetwork</em>。 #### Recurrent Layer <spanclass="math inline"><em>h</em>_<em>t</em></span>用来编码<spanclass="math inline"><em>t</em></span>时刻之前的所有信息。对于这样的层，接受的输入是<spanclass="math inline"><em>x</em>_<em>t</em></span>和<spanclass="math inline"><em>h</em>_<em>t</em> − 1</span>，输出是<spanclass="math inline"><em>h</em>_<em>t</em></span>。<spanclass="math inline"><em>h</em>_<em>t</em></span>的计算公式如下： <spanclass="math display"><em>h</em>_<em>t</em> = <em>f</em>_<em>W</em>(<em>h</em>_<em>t</em> − 1, <em>x</em>_<em>t</em>)</span><span class="math display">$$h\_t = \\tanh (Wh\_{t-1} + Ux\_t)$$</span>长期以来使用的是双曲正切作为激活函数，但是使用ReLu可能有更好的梯度性质。<spanclass="math display"><em>y</em>_<em>t</em> = <em>V</em><em>h</em>_<em>t</em></span>可以认为<spanclass="math inline"><em>h</em>_<em>t</em></span>包含了之前的所有信息，所以使用<spanclass="math inline"><em>h</em>_<em>t</em></span>来预测<spanclass="math inline"><em>y</em>_<em>t</em></span>。<strong>通过引入状态变量来使得递推公式在形式上是二阶依赖。</strong></p><h4 id="bidirectionaldeep-rnn">Bidirectional&amp;Deep RNN</h4><figure><img src="/images/Pasted%20image%2020250321091923.png"alt="Pasted image 20250321091923.png" /><figcaption aria-hidden="true">Pasted image20250321091923.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250321091936.png"alt="Pasted image 20250321091936.png" /><figcaption aria-hidden="true">Pasted image20250321091936.png</figcaption></figure><p>这里纵向方向上的前馈网络中的训练难点在前面的MLP与CNN中是一样的，梯度消失和梯度爆炸。上面的图中的<spanclass="math inline"><em>y</em>_<em>t</em>, <em>c</em></span>是真是标签的独热编码，<spanclass="math inline"><em>C</em></span>是此表中的元素个数。</p><p>在横向方向上，梯度消失是很严重的。因为<spanclass="math inline"><em>h</em>_<em>t</em></span>包含了<spanclass="math inline"><em>h</em>_<em>t</em> − 1</span>的信息，所以梯度会在时间上指数级的衰减。解决这个问题的方法是<strong>LSTM</strong>和<strong>GRU</strong>。</p><h4 id="rnn-for-lm">RNN for LM</h4><ul><li>理论上，可以表达没有边界的时间上的依赖，由于将状态变量编码为<spanclass="math inline"><em>h</em>_<em>t</em></span>，<spanclass="math inline"><em>h</em>_<em>t</em></span>包含了之前的所有信息。</li><li>将序列编码到一个向量中，这个向量包含了整个序列的信息。</li><li>参数在时间上是共享的</li><li>但是在实际上，很难建模时间上的长时间依赖。对于较早的信息，后面的权重会很小。</li></ul><p><strong>一个模型是否有效，在于<em>assumptions</em>与实际情况是否匹配。</strong></p><h4 id="architecture">Architecture</h4><figure><img src="/images/Pasted%20image%2020250321102900.png"alt="Pasted image 20250321102900.png" /><figcaption aria-hidden="true">Pasted image20250321102900.png</figcaption></figure><h5 id="many-to-one">Many to One</h5><p>主要实现的是情感识别、文本分类等任务。在最后一个时间步的输出是最终的输出。最后一个时刻的状态不一定包含有重要的信息（比如上下文中的情感词）。</p><h5 id="one-to-many">One to Many</h5><p>可能的输入有，比如输入一个图像的特征向量再输入到这个网络中。对于这个输入，应该是输入每个状态还是输入所有的状态。并没有解决每一个词对应图中的哪一个区域的问题。</p><h5 id="many-to-many">Many to Many</h5><p>有两种情况，输入是每个时刻的语音的因素，输出的是对应的symbol，是输入输出平行<em>parallel</em>的，但是输入和输出是异构的。</p><p>LM输入和输出是基本上平行的，但是滞后一个时刻，是自回归<em>autoregressive</em>的。##### Sequence to Sequence</p><p>输入和输出都是序列，输入和输出的长度不一定相同。比如机器翻译、语音识别等任务。这个任务可以分为两个部分：编码器和解码器。编码器将输入序列编码到一个向量中，解码器将这个向量解码到输出序列中。</p><figure><img src="/images/Pasted%20image%2020250321110238.png"alt="Pasted image 20250321110238.png" /><figcaption aria-hidden="true">Pasted image20250321110238.png</figcaption></figure><p><em>为什么上述序列中的参数<spanclass="math inline"><em>W</em>_1</span>和<spanclass="math inline"><em>W</em>_2</span>是不一样的？</em></p><p>首先将输入变量<spanclass="math inline">$\\{x\_t\\}$</span>编码到状态变量<spanclass="math inline">$\\{h\_t\\}$</span>中，然后再将状态变量<spanclass="math inline">$\\{h\_t\\}$</span>解码到输出变量<spanclass="math inline">$\\{y\_t\\}$</span>中。编码器是没有lossfunction的，因为输入和输出是异构的。解码器接受的输入是编码器的输出，解码器的输出是一个概率分布。解码器的lossfunction是交叉熵损失函数。</p><p><strong>机器翻译任务中的挑战</strong>： - 输入和输出是异构的 -长序列的处理</p><figure><img src="/images/Pasted%20image%2020250321130443.png"alt="Pasted image 20250321130443.png" /><figcaption aria-hidden="true">Pasted image20250321130443.png</figcaption></figure><p><strong>如何从概率中采样</strong>： - 选择概率最大的 -概率较大的有更大的概率被选择 - Beam Search贪心方法进行搜索</p><figure><img src="/images/Pasted%20image%2020250321130842.png"alt="Pasted image 20250321130842.png" /><figcaption aria-hidden="true">Pasted image20250321130842.png</figcaption></figure><p>可以选择概率较大的k个词，然后以这个词为条件计算下一个词的条件概率，类似于构建一个真k叉树，这样的方法是一种贪心的方法。考虑这棵树上所有的路径，选择最大的路径（本质上是一种搜索技术）。</p><h3 id="backpropagation-through-time">Backpropagation Through Time</h3><figure><img src="/images/Pasted%20image%2020250321133148.png"alt="Pasted image 20250321133148.png" /><figcaption aria-hidden="true">Pasted image20250321133148.png</figcaption></figure><p><span class="math display">$$\\frac{\\partial L}{\\partial U}  = \\sum\_{t=0}^T \\frac{\\partialL\_{t}}{\\partial U} = \\sum\_{t=0}^{T} \\sum\_{s=0}^t \\frac{\\partialL\_{t}}{\\partial y\_t} \\frac{\\partial y\_t}{\\partialh\_t}  \\frac{\\partial h\_t}{\\partial h\_s}\\frac{\\partialh\_s}{\\partial U}$$</span>前一个求和的意义是对于损失函数的各个部分求和，后面的求和式是对于<spanclass="math inline"><em>h</em>_<em>t</em></span>的前面的每一个可能的链求和。其中： <span class="math display">$$\\frac{\\partial h\_t}{\\partial h\_s} = \\prod\_{i=s+1}^t\\frac{\\partial h\_i}{\\partial h\_{i-1}}$$</span>这个式子是一个矩阵乘法，是一个矩阵的连乘。这个矩阵是一个雅可比矩阵。</p><figure><img src="/images/Pasted%20image%2020250321135457.png"alt="Pasted image 20250321135457.png" /><figcaption aria-hidden="true">Pasted image20250321135457.png</figcaption></figure><p>用<em>Cauchy-Schwarz</em>不等式可以证明： <spanclass="math display">$$\\| \\frac{\\partial h\_t}{\\partial h\_{t-1}} \\| \\leq \\| W^T \\|\\|diag (f'(h\_{t-1}))\\| \\leq \\sigma\_{max} \\gamma$$</span> 这里<spanclass="math inline">$\\sigma\_{max}$</span>是矩阵<spanclass="math inline"><em>W</em></span>的最大奇异值，<spanclass="math inline">$\\gamma$</span>是激活函数的导数的最大值。 于是：<span class="math display">$$\\| \\frac{\\partial h\_t}{\\partial h\_{s}} \\| =  \\prod\_{i=s+1}^t\\| \\frac{\\partial h\_i}{\\partial h\_{i-1}} \\| \\leq (\\sigma\_{max}\\gamma)^{t-s}$$</span>这个式子说明了梯度消失的问题，梯度消失是指梯度在时间上的指数级衰减。或者梯度爆炸的问题，梯度爆炸是指梯度在时间上的指数级增长。</p><h4 id="truncated-bptt">Truncated BPTT</h4><figure><img src="/images/Pasted%20image%2020250321140454.png"alt="Pasted image 20250321140454.png" /><figcaption aria-hidden="true">Pasted image20250321140454.png</figcaption></figure><p>这个方法是将时间上的梯度截断，这样可以减少梯度消失和梯度爆炸的问题。但是这样的方法会导致梯度的估计不准确，因为梯度的估计是基于一个截断的时间窗口的。</p><h4 id="long-short-term-memory">Long Short-Term Memory</h4><blockquote><p>为什么这样就能实现所谓的LSTM</p></blockquote><ul><li>遗忘，将过去“没用”的信息遗忘</li><li>更新，将新的信息更新到状态变量中</li><li>输出，输出门控制一部分信息用来进行预测 <imgsrc="/images/Pasted%20image%2020250321141243.png"alt="Pasted image 20250321141243.png" /> <spanclass="math inline"><em>t</em></span>时刻的状态变量<spanclass="math inline"><em>h</em>_<em>t</em></span>储存的是<spanclass="math inline"><em>t</em></span>时刻的信息，<spanclass="math inline"><em>c</em>_<em>t</em></span>是<spanclass="math inline"><em>t</em></span>时刻的记忆变量，<spanclass="math inline"><em>h</em>_<em>t</em> − 1</span>和<spanclass="math inline"><em>x</em>_<em>t</em></span>是<spanclass="math inline"><em>t</em></span>时刻的输入，<spanclass="math inline"><em>f</em>_<em>t</em></span>是遗忘门，<spanclass="math inline"><em>i</em>_<em>t</em></span>是输入门，<spanclass="math inline"><em>o</em>_<em>t</em></span>是输出门，<spanclass="math inline"><em>g</em>_<em>t</em></span>是更新门。</li></ul><p><img src="/images/Pasted%20image%2020250321141234.png"alt="Pasted image 20250321141234.png" />上述网络构造了一个信息流高速路径，使得梯度能够进行快速的传播。</p><p>遗忘门和残差网络的思想是类似的，都是将过去的信息和现在的信息进行融合。这样的网络可以更好的处理长序列的问题。[[DeepLearning Lecture-3#ResNet]]</p><h4 id="gradient-clipping">Gradient Clipping</h4><p>梯度的大小是由模长决定的，如果梯度的模长过大，可以将梯度的模长进行截断。这样可以避免梯度爆炸的问题。</p><h4 id="variational-dropout">Variational Dropout</h4><p>在深度网络中，如果是过拟和的，也就是对于一个含有多个参数的网络。也就是说如果输入的参数小于参数的个数，那么相对应的线性方程组是欠定的。</p><p>在RNN中对应的纵向方向上是多层感知机，所以可以采用标准的Dropout方法。但是在横向方向上是一个循环网络，Dropout方法不适用。因为Dropout方法会破坏时间上的连续性，违背了参数共享的原则。采用<strong>步调一致</strong>的方法进行操作，这样可以保持时间上的连续性。这样的方法是<strong>VariationalDropout</strong>。</p><h4 id="layer-normalization">Layer Normalization</h4><figure><img src="/images/Pasted%20image%2020250322202759.png"alt="Pasted image 20250322202759.png" /><figcaption aria-hidden="true">Pasted image20250322202759.png</figcaption></figure><p>在CNN中，对于每一个通道的值进行归一化。在这样的每一个通道中计算均值和方差。还是要加入一个平移变量和伸缩变量。</p><p>在RNN中，主要的原因是门控结构是相对于每一个序列而言的，所以应该引入一种新的归一化方法<em>LayerNormalization</em>，应该在每一条样本（一个序列）在每一个时刻经过之后的值在<spanclass="math inline"><em>C</em></span>个通道上进行归一化操作。最后得到的结果是：将所有的向量放在以原点为球心的单位球面上。</p><h4 id="weight-normalization">Weight Normalization</h4><p>对于每层的参数<spanclass="math inline">$\\mathbf{w}$</span>进行重参数化： <spanclass="math display">$$\\mathbf{w} = \\frac{g}{\\|v\\|}v$$</span> <img src="/images/Pasted%20image%2020250322204925.png"alt="Pasted image 20250322204925.png" /></p><p>对右边的式子<spanclass="math inline">$\\frac{v}{\\|v\\|}$</span>进行优化是更为容易的，重参数化的意思是，在训练和测试的时候使用不同的参数表达形式，这样是更加容易优化的。</p><h4 id="training-inference-shift">Training-Inference Shift</h4><p>滚动预测：在训练的时候，使用真实的标签进行预测；在测试的时候，使用的是推理得到的值进行预测。这是一个自回归任务。</p><h5 id="curriculum-learning">Curriculum Learning</h5><p>在训练的时候，可以先训练一些简单的任务，然后再训练一些复杂的任务。这样可以更好的训练模型。在实际中，可以对所有的样本计算loss，先计算loss较小的样本，然后再计算loss较大的样本。这就是<strong>自步学习</strong>。这里涉及到选择不同的样本顺序的问题。</p><p><em>ScheduledSampling</em>：在学习刚开始的时候，更多地使用真实的标签进行预测；随着学习的进行，更多地使用模型预测的值进行预测。<strong>是RNN中很重要的技术</strong>。</p><h3 id="rnn-with-attention">RNN with Attention</h3><h4 id="human-attention">Human Attention</h4><p>人类的注意力： - 可持续注意力（没有实现） -选择性注意力（人类的选择性注意力复杂得多） - 交替式注意力 -分配式注意力</p><h4 id="attention-in-deep-learning">Attention in Deep Learning</h4><blockquote><p><em>Allowing the model to dynamically pay attention to only certainparts of the input that help in performing the task at handeffectively.</em></p></blockquote><p>存在时间<em>temporal Attention</em>和空间<em>SpatialAttention</em>上的注意力。一般而言指的是时间上的注意力。</p><h4 id="auto-regessive">Auto-Regessive</h4><p>[[#Sequence to Sequence]]</p><p>最重要的问题是编码器和解码器之间的信息沟通太少了，存在有信息瓶颈。并且在翻译任务中，输入和输出的顺序并不是一致的，大部分的语言的语序是不一样的。<strong>希望看到后面的信息</strong>，这和RNN的设计目的是相违背的。<strong>全连接的思想</strong>又回来了，获得全局信息的方法有很多，不只是有MLP的方法。有一种基本思想是<strong><em>Relevance</em></strong>，也就是和当前任务相关的信息。这个思想是在Attention中得到了体现。</p><h4 id="attention">Attention</h4><figure><img src="/images/Pasted%20image%2020250322222251.png"alt="Pasted image 20250322222251.png" /><figcaption aria-hidden="true">Pasted image20250322222251.png</figcaption></figure><p>计算两个东西的相似度有：计算内积、输入<em>relationnetwork</em>。这样的模型在互联网中有很多的应用，比如推荐系统、搜索引擎等。</p><figure><img src="/images/Pasted%20image%2020250323084415.png"alt="Pasted image 20250323084415.png" /><figcaption aria-hidden="true">Pasted image20250323084415.png</figcaption></figure><p>注意力的分配是符合概率分布的，所以可以使用上面计算得到的相关性<spanclass="math inline"><em>e</em>_<em>i</em><em>j</em></span>使用softmax函数进行归一化。这样得到的分布就是注意力的分布：<span class="math display">$$\\alpha\_{ij} = \\frac{\\exp(e\_{ij})}{\\sum\_{k=1}^{T\_x}\\exp(e\_{ik})}$$</span> 上述计算式表达的含义是，在状态<spanclass="math inline"><em>i</em></span>的时刻分配在<spanclass="math inline"><em>j</em></span>上的注意力（对于<spanclass="math inline"><em>j</em></span>的求和为1）。继续计算<spanclass="math inline"><em>c</em>_<em>i</em></span>： <spanclass="math display">$$c\_i = \\sum\_{j=1}^{T\_x} \\alpha\_{ij} x\_j$$</span> 这里<spanclass="math inline"><em>c</em>_<em>i</em></span>是对于状态<spanclass="math inline"><em>i</em></span>的时刻的注意力向量，是对于<spanclass="math inline"><em>x</em></span>的加权和。 <spanclass="math display"><em>s</em>_<em>i</em> = <em>f</em>(<em>s</em>_<em>i</em> − 1, <em>y</em>_<em>i</em> − 1, <em>c</em>_<em>i</em>)</span>这里<spanclass="math inline"><em>s</em>_<em>i</em></span>是状态变量，<spanclass="math inline"><em>y</em>_<em>i</em> − 1</span>是前一个时刻的输出，<spanclass="math inline"><em>c</em>_<em>i</em></span>是当前时刻的注意力向量。这里的函数<spanclass="math inline"><em>f</em></span>是一个GRU orLSTM。这个模型是一个<strong>Seq2Seq</strong>模型。</p><blockquote><p>这里的<span class="math inline"><em>c</em>_<em>i</em></span>和<spanclass="math inline"><em>s</em>_<em>i</em></span>的区别是什么，为什么和LSTM有关</p></blockquote><p><img src="/images/Pasted%20image%2020250323084332.png"alt="Pasted image 20250323084332.png" /> <imgsrc="/images/Pasted%20image%2020250323084535.png"alt="Pasted image 20250323084535.png" /></p><p>在机器翻译的过程中，使用source的上下文信息比直接使用词典来翻译更好。这样的模型可以更好的处理长序列的问题。只要是序列都会使用<strong>滑动窗口</strong>，一般设置为50~100之间。对于较短的情况，可以使用psdding的方法；对于较长的情况会使用截断的方法。希望找一个与序列的长度线性关系的模型。</p><h4 id="attention-vs.-mlp">Attention vs. MLP</h4><p>相同点： - 都是全局模型，是对于长序关系的建模。</p><p>不同点： - Attention是基于概率的，MLP是基于全连接的。 -Attention引入relevance的思想，能大大减小参数量</p><h4 id="hierarchical-attention">Hierarchical Attention</h4><p>先建模词注意力然后再建模句子注意力。</p><h4 id="global-attention">Global Attention</h4><p><span class="math display">$$\\text{score} = \\begin{cases}h\_t^T \\overline{h\_s} \\\\h\_t^T W\_a \\overline{h\_s} \\\\v\_a^T \\tanh(W\_a\[h\_t;\\overline{h\_s}\])\\end{cases}$$</span> 发现上面三种计算方式的效率是差不多的。</p><figure><img src="/images/Pasted%20image%2020250323090740.png"alt="Pasted image 20250323090740.png" /><figcaption aria-hidden="true">Pasted image20250323090740.png</figcaption></figure><h3 id="memory">Memory</h3><h4 id="human-memory">Human Memory</h4><ul><li>Sensory Memory<ul><li>计算机视觉与机器感知</li></ul></li><li>Short-term Memory<ul><li>与计算机中的内存是很相近的，LSTM是一种将短期记忆尽量变长的方法。</li></ul></li><li>Long-term Memory<ul><li>前面的模型中没有实现这个功能</li></ul></li></ul><p>在自然语言中，比较困难的任务是进行对话，这时候需要进行长期记忆。在对话中，需要对话的上下文进行理解。</p><h4 id="neural-turing-machine">Neural Turing Machine</h4><figure><img src="/images/Pasted%20image%2020250323092327.png"alt="Pasted image 20250323092327.png" /><figcaption aria-hidden="true">Pasted image20250323092327.png</figcaption></figure><p>在这个模型中，最重要的是对内存1进行寻址的操作，这个操作是一个注意力的操作。对于读的操作，是按照注意力的大小对地址里面的内容进行加权平均。对于写的操作，类似于LSTM中的Forget Gate，先进行擦除之后才进行写入。</p><p>对于Internal Memory，最大的问题是可能会遗忘，对于ExternalMemory，是一个外部的存储器，这样的存储是比较稳定的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-6</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-6/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-6/</url>
    
    <content type="html"><![CDATA[<h2 id="transformers">Transformers</h2><h3 id="transformers-attention-is-all-you-need">Transformers: Attentionis All You Need</h3><p>[[Deep Learning Lecture-5#Attention]]</p><p>再次理解Attention的概念：类似于”查字典“的操作，对于Query <spanclass="math inline"><em>q</em></span>, Key <spanclass="math inline"><em>k</em></span>和Value <spanclass="math inline"><em>v</em></span>，计算相关性，也就是重要性，对于输出序列中的第<spanclass="math inline"><em>i</em></span>个输出有价值的信息： <spanclass="math display"><em>w</em>_<em>i</em><em>j</em> = <em>a</em>(<em>q</em>_<em>i</em>, <em>k</em>_<em>j</em>)</span>其中<spanclass="math inline"><em>a</em></span>是一个函数，可以是内积、<em>AdditiveAttention</em>等。对于输出序列中的第<spanclass="math inline"><em>i</em></span>个输出，计算当前的输出的<spanclass="math inline"><em>q</em>_<em>i</em></span>，计算与输入序列中的<spanclass="math inline"><em>k</em>_<em>j</em></span>的相关性，然后对于<spanclass="math inline"><em>v</em>_<em>j</em></span>进行加权求和（这是一种寻址操作），得到的<spanclass="math inline"><em>c</em>_<em>i</em></span>是查字典所得到的信息：<span class="math display">$$c\_i = \\sum\_{j=1}^T w\_{ij}v\_j$$</span> <strong>希望找到一种更好的计算方法</strong>。</p><p>在[[Deep Learning Lecture-5#RNN with Attention]]中问题在于： -太复杂的模型 - 某种意义上使用的Attention已经足够使用，不再需要循环网络 -循环网络的计算是串行的，不能有效加速 #### Self-Attention</p><p>计算的是同一条序列中的不同位置之间的相关性，也就是自注意力。对于输入序列中的第<spanclass="math inline"><em>i</em></span>个位置，计算与其他位置的相关性，然后对于所有的位置进行加权求和：规定Query <span class="math inline">$Q = \[q\_1 \\dotsq\_n\]$</span>，Key <span class="math inline">$K = \[k\_1 \\dotsk\_n\]$</span>，Value <span class="math inline">$V = \[v\_1 \\dotsv\_k\]$</span>，则：</p><figure><img src="/images/Pasted%20image%2020250323133751.png"alt="Pasted image 20250323133751.png" /><figcaption aria-hidden="true">Pasted image20250323133751.png</figcaption></figure><h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4><p>我们认为使用一个网络来计算相关性太复杂了，当两个向量是相同维度的时候可以直接计算内积。在这里，在计算先引入参数，使得其维度是一样的，从而可以计算内积：</p><p><em>Scaled Dot-Product</em> : <span class="math display">$$a(q,k) = \\frac{q^T k}{\\sqrt{d\_k}}$$</span>使得变换前后的方差是一样的，这样可以使得梯度更加稳定，否则可能进入激活函数的饱和区。<span class="math display">$$Attention(Q,K,V) =\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\_k}}\\right)V$$</span> 上面的式子将得到的<span class="math inline">$n \\timesn$</span>的矩阵进行softmax操作，在归一化的过程中，<strong>是某一个query在所有的key上的注意力分配一定是<spanclass="math inline">$\\mathbf{1}$</span></strong>。后面是对于Value的加权求和。<strong>在上面的公式中，<spanclass="math inline"><em>Q</em></span>、<spanclass="math inline"><em>K</em></span>和<spanclass="math inline"><em>V</em></span>中的向量都是行向量，进行softmax操作时也是在同一行上操作</strong>。</p><figure><img src="/images/Pasted%20image%2020250323141917.png"alt="Pasted image 20250323141917.png" /><figcaption aria-hidden="true">Pasted image20250323141917.png</figcaption></figure><p>对于同一组输入，经过不同的线性变换得到的不同的Query、Key和Value，在样本数量为<spanclass="math inline"><em>m</em></span>的情况下，可以进行计算：</p><p><span class="math display">$$\\begin{aligned}&amp;W^Q \\in \\mathbb{R}^{d\_k \\times d\_{\\text{input}}}, \\\\&amp;W^K \\in \\mathbb{R}^{d\_k \\times d\_{\\text{input}}}, \\\\&amp;W^V \\in \\mathbb{R}^{d\_v \\times d\_{\\text{input}}} \\\\&amp;Q = X W^Q \\in \\mathbb{R}^{m \\times d\_k}, \\\\&amp;K = X W^K \\in \\mathbb{R}^{m \\times d\_k}, \\\\&amp;V = X W^V \\in \\mathbb{R}^{m \\times d\_v}. \\\\&amp;QK^T \\in \\mathbb{R}^{m \\times m}, \\\\&amp;\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\_k}}\\right) \\in\\mathbb{R}^{m \\times m}, \\\\&amp;\\text{Attention}(Q, K, V) =\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\_k}}\\right)V \\in\\mathbb{R}^{m \\times d\_v}.\\end{aligned}$$</span></p><p><strong>维度总结表</strong></p><table><thead><tr><th>矩阵/操作</th><th>维度</th><th>说明</th></tr></thead><tbody><tr><td>输入矩阵 <span class="math inline"><em>X</em></span></td><td><span class="math inline">$m \\times d\_{\\text{input}}$</span></td><td>包含 <span class="math inline"><em>m</em></span>个样本，每个样本维度为 <spanclass="math inline">$d\_{\\text{input}}$</span></td></tr><tr><td>查询矩阵 <span class="math inline"><em>Q</em></span></td><td><span class="math inline">$m \\times d\_k$</span></td><td>每个样本的查询向量维度为 <spanclass="math inline"><em>d</em>_<em>k</em></span></td></tr><tr><td>键矩阵 <span class="math inline"><em>K</em></span></td><td><span class="math inline">$m \\times d\_k$</span></td><td>每个样本的键向量维度为 <spanclass="math inline"><em>d</em>_<em>k</em></span></td></tr><tr><td>值矩阵 <span class="math inline"><em>V</em></span></td><td><span class="math inline">$m \\times d\_v$</span></td><td>每个样本的值向量维度为 <spanclass="math inline"><em>d</em>_<em>v</em></span></td></tr><tr><td>注意力得分矩阵 <spanclass="math inline"><em>Q</em><em>K</em><sup><em>T</em></sup></span></td><td><span class="math inline">$m \\times m$</span></td><td>样本间的注意力强度矩阵</td></tr><tr><td>最终输出 <spanclass="math inline">$\\text{Attention}(Q,K,V)$</span></td><td><span class="math inline">$m \\times d\_v$</span></td><td>聚合所有样本的加权值信息，输出维度为 <spanclass="math inline"><em>d</em>_<em>v</em></span></td></tr></tbody></table><h4 id="multi-head-attention">Multi-Head Attention</h4><p>注意到上面的注意力的表达能力是相当有限的，在languagemodel同一个词和其他不同的词之间可能有很多种不同的关系，仅仅用一种简单的关系来表示是不够的。所以我们引入多头注意力，希望能在不同的侧面上进行表达。<span class="math display">$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}\_1, \\dots,\\text{head}\_h)W^O$$</span> 其中： <span class="math display">$$\\text{head}\_i = \\text{Attention}(QW\_i^Q, KW\_i^K, VW\_i^V)$$</span> 其中<spanclass="math inline"><em>W</em>_<em>i</em><sup><em>Q</em></sup>, <em>W</em>_<em>i</em><sup><em>K</em></sup>, <em>W</em>_<em>i</em><sup><em>V</em></sup></span>是不同的线性变换，<spanclass="math inline"><em>W</em><sup><em>O</em></sup></span>是最后的线性变换，最后进行的维度的规约操作。与CNN相比，CNN的不同的通道之间与上一层的每一个通道之间都是有连接的；但是在这里，不同的头之间是没有连接的，这样可以使得不同的头可以关注不同的信息。</p><p>不同的头之间是可以并行计算的，这样可以加速计算；但是缺点是内存占用会很大。</p><h4 id="position-wise-feed-forward-networks">Position-wise Feed-ForwardNetworks</h4><p><img src="/images/Pasted%20image%2020250323145847.png"alt="Pasted image 20250323145847.png" /> 在标准 Transformer Block中，注意力层之后会接一个前馈神经网络（FFN），其结构如下：</p><ol type="1"><li><p>输入张量形状：<br /><span class="math display">$$X \\in \\mathbb{R}^{B \\times L \\times H \\times D}$$</span></p><ul><li><span class="math inline"><em>B</em></span>：Batch size<br /></li><li><span class="math inline"><em>L</em></span>：序列长度<br /></li><li><span class="math inline"><em>H</em></span>：Attention 头数<br /></li><li><span class="math inline"><em>D</em></span>：每个头的维度</li></ul></li><li><p>将多头输出合并：<br /><span class="math display">$$X' = \\mathrm{reshape}(X,\\, (B,\\,L,\\,H\\cdot D)) \\in \\mathbb{R}^{B\\times L \\times (H\\!D)}$$</span></p></li><li><p>两层“卷积”全连接结构<br />这里所谓“卷积”，实际上等价于在最后一维上对每个位置独立地做 1×1 卷积（与RNN 中在不同时间步共享参数的思想一致），并在卷积核后加入非线性ReLU。<br /><span class="math display">$$\\begin{aligned}Z\_1 &amp;= \\mathrm{ReLU}\\bigl(X' W\_1 + b\_1\\bigr),\\quadW\_1\\in\\mathbb{R}^{(H\\!D)\\times d\_{ff}},\\;b\_1\\in\\mathbb{R}^{d\_{ff}}\\\\Z\_2 &amp;= Z\_1 W\_2 + b\_2,\\quadW\_2\\in\\mathbb{R}^{d\_{ff}\\times(H\\!D)},\\;b\_2\\in\\mathbb{R}^{(H\\!D)}\\end{aligned}$$</span></p><ul><li>第一层升维到中间维度 <spanclass="math inline"><em>d</em>_<em>f</em><em>f</em></span>（例如 <spanclass="math inline">2048</span>）<br /></li><li>第二层降维回原始维度 <span class="math inline">$H\\!D$</span>（例如<span class="math inline">512</span>）</li></ul></li><li><p>加残差 &amp; LayerNorm<br /><span class="math display">$$Y = \\mathrm{LayerNorm}\\bigl(X' + Z\_2\\bigr)\\in\\mathbb{R}^{B\\timesL\\times(H\\!D)}$$</span></p></li><li><p>（可选）reshape 回多头排列：<br /><span class="math display">$$Y' = \\mathrm{reshape}(Y,\\, (B,\\,L,\\,H,\\,D))$$</span></p></li></ol><ul><li><strong>线性限制</strong>：除去 Attention 中的SoftMax，若只堆线性层，模型表达能力较弱；引入 ReLU后能够拟合更复杂的非线性函数。<br /></li><li><strong>稀疏权重</strong>：Attention 的 SoftMax本质上生成了一组“稀疏”权重，负责学习不同位置间的依赖；FFN则负责在每个位置上“干净”地抽取该词的内部特征，避免无谓的跨词干扰。<br /></li><li><strong>卷积视角</strong>：将 FFN 看作<strong>对最后一维的 1×1卷积</strong>，等同于对每个位置独立但在所有位置共享参数，这与 RNN在时间步上共享权重的假设一致。<br /></li><li><strong>增强表达</strong>：Attention 解决了上下文依赖，FFN则补强了单位置特征提取，两者协同提升了 Transformer 的整体表达能力。</li></ul><hr /><blockquote><p><strong>注意</strong>：以上操作对每个批次（<spanclass="math inline"><em>B</em></span>）中每个序列位置（<spanclass="math inline"><em>L</em></span>）都独立执行，参数在所有位置间共享。</p></blockquote><h4 id="residual-connection">Residual Connection</h4><p>在上面的操作中，这些操作都是有排列不变性。残差是一个标准的操作，这样可以让网络更好地记录位置编码。</p><h4 id="layer-normalization">Layer Normalization</h4><p>目的是使得每一层经过Attention和Feed-Forward之后的输出的分布是一样的，这样可以使得梯度更加稳定。[[Deep Learning Lecture-5#Layer Normalization]]</p><figure><img src="/images/Pasted%20image%2020250323151056.png"alt="Pasted image 20250323151056.png" /><figcaption aria-hidden="true">Pasted image20250323151056.png</figcaption></figure><h4 id="value-embedding">Value Embedding</h4><ul><li><p><strong>目的</strong>：将原始输入特征（如温度、流量、股价等数值）从低维（通常是1或几个通道）映射到高维向量空间，使模型能在更大维度下学习更丰富的特征表示。</p></li><li><p><strong>做法</strong>：通过一个线性层或 1×1卷积（<code>TokenEmbedding</code>）把每个时间步的原始向量映成长度为<code>d_model</code> 的向量。</p></li><li><p><strong>实现方式</strong>：通过一个线性映射或 <spanclass="math inline">$1 \\times 1$</span> 卷积完，对于有<spanclass="math inline"><em>c</em>_<em>i</em><em>n</em></span>个特征维度的输入，使用卷积核的大小是<spanclass="math inline">$\[d\_{model},c\_{in},1\]$</span></p></li><li><p><strong>输入维度</strong>：<spanclass="math inline">$(\\text{batch\_size}, \\text{seq\_len},c\_{in})$</span></p></li><li><p><strong>输出维度</strong>：<spanclass="math inline">$(\\text{batch\_size}, \\text{seq\_len},d\_{model})$</span></p></li></ul><h4 id="positional-encoding-position-embedding">Positional Encoding/Position Embedding</h4><p>位置信息是顺序信息的一种泛化的形式。如果采用独热编码，这是一种类别信息而不是一个顺序信息，不同的是不可以比的。所以引入<em>positionembedding</em>，这是一个矩阵，效果类似于一个查找表。查找操作在这里就是一个矩阵乘上一个独热编码的操作，这是因为GPU在矩阵乘法操作上是非常高效的。但是独热编码会带来下面的问题 - <strong>高维稀疏性</strong>：独热编码的维度等于序列最大长度（如512），导致向量稀疏且计算效率低下（尤其对长序列）。- <strong>无法泛化到未见长度</strong>：若训练时序列最大长度为512，模型无法处理更长的序列</p><p><strong>引入归纳偏好</strong>： -每个位置的编码应该是独一无二的且是确定的 -认为两个位置的距离应该是一致的 -应该生成一个有界的值，位置数随着序列长度的增加而增加</p><p>Google的实现是使用的正弦和余弦函数的组合： <spanclass="math display">$$e\_i(2j) = \\sin\\left(\\frac{i}{10000^{2j/d\_{\\text{model}}}}\\right)$$</span> <span class="math display">$$e\_i(2j+1) =\\cos\\left(\\frac{i}{10000^{2j/d\_{\\text{model}}}}\\right)$$</span> 上述公式中的<spanclass="math inline"><em>i</em></span>指的句子中的第<spanclass="math inline"><em>i</em></span>个位置，<spanclass="math inline"><em>j</em></span>指的是位置编码的维度，<spanclass="math inline">$d\_{\\text{model}}$</span>是位置编码的维度。这样的编码是满足上面的归纳偏好的。</p><h4 id="temporal-embedding">Temporal Embedding</h4><ul><li><strong>目的</strong>：针对时间序列中特有的“时间属性”——小时、星期几、月份、季节等——进行编码，让模型学到周期性（如日周期、周周期、年周期）和节假日效应等信息。</li><li><strong>做法</strong>：通常把每个时间属性（hour-of-day, day-of-week,month 等）也映射到 <code>d_model</code>维度，然后把这些属性向量加起来或拼接后再降维。<code>TemporalEmbedding</code>类会根据 <code>embed_type</code>（如 <code>'fixed'</code> 或<code>'learned'</code>）和 <code>freq</code>（如<code>'h'</code>、<code>'d'</code>）来决定具体细节。</li></ul><h4 id="encoder">Encoder</h4><p>编码器中使用的是多头注意力、逐位置前馈网络和位置编码。在这个编码器中是一个直筒式的网络，好处是调参较为简单。</p><p>缺点： - 二次复杂度 - 参数量过大 - 很多的头是冗余的</p><p>训练阶段要使用多个头，发现有些头的权重较低，可以在推理阶段去掉这些头。</p><h4 id="decoder">Decoder</h4><h5 id="autoregressive">Autoregressive</h5><figure><img src="/images/Pasted%20image%2020250323161530.png"alt="Pasted image 20250323161530.png" /><figcaption aria-hidden="true">Pasted image20250323161530.png</figcaption></figure><p>预测阶段一定要使用滚动预测，这是一个自回归的状态，但是这是一个串行的操作，会比较慢。但是在训练阶段这样是不能接受的，我希望训练的不同阶段可以并行计算，但是这里要求在一开始输入所有的序列，所以这里需要<strong>遮挡</strong>。在算Attention的时候，对于当前的位置，只能看到之前的位置，不能看到之后的位置。</p><figure><img src="/images/Pasted%20image%2020250323163151.png"alt="Pasted image 20250323163151.png" /><figcaption aria-hidden="true">Pasted image20250323163151.png</figcaption></figure><p>在编码器上是不能用的，因为防止解码器在训练时利用未来的目标序列信息（即“作弊”），确保模型逐步生成的能力与推理阶段一致。训练过程中仍然需要真实标签作为目标输出，但掩码限制了模型在生成当前词时对未来的访问。</p><h4 id="encoder-decoder-attention">Encoder-Decoder Attention</h4><p>计算的是解码器的输出和编码器的输出之间的相关性，这里的Query是解码器的输出，Key和Value是编码器的输出。</p><figure><img src="/images/Pasted%20image%2020250323185530.png"alt="Pasted image 20250323185530.png" /><figcaption aria-hidden="true">Pasted image20250323185530.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250323185720.png"alt="Pasted image 20250323185720.png" /><figcaption aria-hidden="true">Pasted image20250323185720.png</figcaption></figure><p>注意这里是将编码器的输出输入到解码器中的每一层的Encoder-DecoderAttention中。这里是神经网络中的<strong>特征重用</strong>思想，并且解码器中的网络是直筒式的，所以这些特征是可以重用的。</p><h4 id="rnn-vs.-transformer">RNN vs. Transformer</h4><ul><li>RNN是串行的，Transformer是并行的</li><li>对于有严格偏序关系的序列，RNN可能更适合</li><li>对于长序列，Transformer更适合</li><li>对于较小的数据量，Transformers参数量较大，表现可能不如RNN</li></ul><figure><img src="/images/Pasted%20image%2020250323190623.png"alt="Pasted image 20250323190623.png" /><figcaption aria-hidden="true">Pasted image20250323190623.png</figcaption></figure><p><img src="/images/Pasted%20image%2020250323190856.png"alt="Pasted image 20250323190856.png" /> ### X-formers Variance withImprovements</p><p><a href="https://arxiv.org/abs/2106.04554">[2106.04554] A Survey ofTransformers</a></p><figure><img src="/images/Pasted%20image%2020250323191632.png"alt="Pasted image 20250323191632.png" /><figcaption aria-hidden="true">Pasted image20250323191632.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250323191637.png"alt="Pasted image 20250323191637.png" /><figcaption aria-hidden="true">Pasted image20250323191637.png</figcaption></figure><h4 id="lineariezd-attention">Lineariezd Attention</h4><h4 id="flow-attention">Flow Attention</h4><h3 id="gpt-generative-pre-trained-transformer">GPT: GenerativePre-trained Transformer</h3><h4 id="transfer-learning">Transfer Learning</h4><p>先将一个模型预训练好，然后在特定的任务上进行微调。一般而言，预训练的过程是无监督的，优点是可以使用大规模数据。</p><h4 id="pre-training">Pre-Training</h4><figure><img src="/images/Pasted%20image%2020250324190821.png"alt="Pasted image 20250324190821.png" /><figcaption aria-hidden="true">Pasted image20250324190821.png</figcaption></figure><ul><li>直接使用的是Transformers中的block，但是这里使用12层</li><li>只使用decoder没有encoder，因为这不是一个机器翻译的任务</li><li>在计算损失函数的过程中，使用的似然函数是最大似然估计，在实际中使用一个参数化的网络来近似需要的概率。</li></ul><h4 id="supervised-fine-tuning">Supervised Fine-Tuning</h4><p>对于不同的任务，需要更换模型的输出头，并且还要使用新的损失函数。关注上下文建模。<img src="/images/Pasted%20image%2020250324193704.png"alt="Pasted image 20250324193704.png" /></p><p>最后是使用无监督训练的损失函数和有监督训练的损失函数的加权和，这是一个<strong>多任务学习</strong>。当微调的数据比较少的时候，可以使用无监督训练的损失函数的权重较大。</p><p>对于不同的下游任务，要进行任务适配<em>Task SpecificAdaptation</em>。对于不同的下游任务，可以使用不同的头。 <imgsrc="/images/Pasted%20image%2020250324194137.png"alt="Pasted image 20250324194137.png" /></p><h4 id="gpt-2-gpt-3">GPT-2 &amp; GPT-3</h4><p>Zero-shotlearning：在没有看到训练数据的情况下，直接在测试集上进行预测。通过在预训练阶段使用大规模的数据，可以使得模型具有更好的泛化能力，这样可以提高在一些常见问题上的表现。</p><h3id="bert-bidirectional-encoder-representations-from-transformers">BERT:Bidirectional Encoder Representations from Transformers</h3><p>与GPT不同的是，BERT是双向的，可以看到上下文的信息。 <imgsrc="/images/Pasted%20image%2020250324195035.png"alt="Pasted image 20250324195035.png" /></p><p>BERT在encoder阶段就使用了mask，这样可以使得模型在训练的时候不会看到未来的信息。在训练的过程中随机地mask掉一些词，然后预测这些词。如果遮挡的词太少，那么模型得到的训练不够，如果遮挡的词太多，那么得到的上下文就很少。在训练的过程中就使用了102种语言。特征工程：使用了更多的特征，引入了更多的embedding是多个任务的联合训练，这样可以使得模型更加通用。</p><h4 id="roberta-a-robustly-optimized-bert-pretraining-approach">RoBERTa:A Robustly Optimized BERT Pretraining Approach</h4><p>经过充分的调参和更长的训练时间，使得模型的表现更好。证明了BERT中的下句预测是没有用的，因为在RoBERTa中去掉了这个任务。mask的pattern可以动态调整</p><h4id="albert-a-lite-bert-for-self-supervised-learning-of-language-representations">ALBERT:A Lite BERT for Self-supervised Learning of LanguageRepresentations</h4><p>低秩分解，减少参数量 <imgsrc="/images/Pasted%20image%2020250324203227.png"alt="Pasted image 20250324203227.png" /></p><p>跨层参数共享：可以让模型更加稳定</p><h4 id="t5-text-to-text-transfer-transformer">T5: Text-to-Text TransferTransformer</h4><p>迁移是泛化的高级形式：可以将多种文本任务统一为文本到文本的形式，这样可以使得模型更加通用。</p><p>架构层面的创新： <imgsrc="/images/Pasted%20image%2020250324203639.png"alt="Pasted image 20250324203639.png" /></p><p>这里使用的是prefix-LM，这样可以使得模型更加通用。</p><h3 id="vision-transformer">Vision Transformer</h3><h4 id="vit">ViT</h4><p>将一个图像变成一个patch 增加一个PositionEmbedding，于是得到各个patch的特征的加权平均。主要的贡献是将图像转换为序列，从而可以使用transformers来进行建模。在这个之前，普遍的观点是transformers只能用于文本数据，而CNN用于图像数据。</p><h4 id="swim-transformer">Swim Transformer</h4><p>将CNN中的一些归纳偏好引入，可以使用局部的注意力，但是在一定程度上能捕捉全局的信息，通过ShiftedWindow Mechanism来实现。 层次化特征： <imgsrc="/images/Pasted%20image%2020250324205304.png"alt="Pasted image 20250324205304.png" /></p><p>密集预测任务对于层次化特征需求更高，于是这个模型的表现是更好地。</p><h4 id="detr">DETR</h4><figure><img src="/images/Pasted%20image%2020250324205845.png"alt="Pasted image 20250324205845.png" /><figcaption aria-hidden="true">Pasted image20250324205845.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250324205921.png"alt="Pasted image 20250324205921.png" /><figcaption aria-hidden="true">Pasted image20250324205921.png</figcaption></figure><h3 id="fundation-models">Fundation Models</h3><p><span class="math display">$$e^{\\alpha+ \\beta i } = e^{\\alpha} e^{\\beta i} = e^{\\alpha}(\\cos\\beta + i \\sin \\beta)$$</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-7</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-7/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-7/</url>
    
    <content type="html"><![CDATA[<h2 id="generative-model">Generative Model</h2><p>数据分布是生成模型的核心。生成模型的目标是学习数据的分布，然后生成新的数据。目标是<strong>学习数据的分布</strong>，然后生成新的数据。对生成模型的评价是通过生成的数据的质量来评价的，生成的数据越接近真实数据，生成模型的质量越好。<span class="math display">$$\\theta^\* = \\arg\\max\_{\\theta} \\mathbb{E}\_{x \\sim p\_{data}}\\log p\_{model}(x|\\theta)$$</span> <img src="/images/Pasted%20image%2020250330094251.png"alt="Pasted image 20250330094251.png" /></p><figure><img src="/images/Pasted%20image%2020250330094404.png"alt="Pasted image 20250330094404.png" /><figcaption aria-hidden="true">Pasted image20250330094404.png</figcaption></figure><h3 id="gan-generative-adversarial-network">GAN: Generative AdversarialNetwork</h3><p>对抗机器学习的思想是通过两个网络之间的对抗来学习。生成器和判别器之间的对抗是GAN的核心思想。使用的博弈问题的思想，使用的是最小化最大的思想。</p><p>GAN的思想为：生成器和判别器之间的对抗，生成器生成数据，判别器判断数据的真实性。</p><p>目标函数为： <span class="math display">$$\\min\_G \\max\_D V(D,G) = \\mathbb{E}\_{x \\sim p\_{data}(x)}\[\\logD(x)\] + \\mathbb{E}\_{z \\sim p\_z(z)}\[\\log(1-D(G(z)))\]$$</span> - 当生成器固定时，判别器的目标是最大化判别器的准确率：<spanclass="math inline">$\\max\_D V(D,G)$</span> -当判别器固定时，生成器的目标是最小化判别器的准确率：<spanclass="math inline">$\\min\_G V(D,G)$</span></p><p>生成器 <span class="math inline"><em>G</em></span> 的实质是将噪声分布<spanclass="math inline"><em>p</em>_<em>z</em>(<em>z</em>)</span>（如高斯分布）映射到数据分布<spanclass="math inline"><em>p</em>_<em>g</em>(<em>x</em>)</span>。根据概率密度变换定理，若<span class="math inline"><em>G</em></span>是可逆且光滑的函数，则生成数据的分布为：</p><p><span class="math display">$$p\_g(x) = p\_z(z) \\cdot \\left| \\det \\left( \\frac{\\partialG^{-1}(x)}{\\partial x} \\right) \\right|$$</span></p><p>尽管深度神经网络通常不可逆，但通过足够复杂的函数逼近（如多层非线性变换），生成器可以隐式地学习到从<span class="math inline"><em>p</em>_<em>z</em>(<em>z</em>)</span> 到<span class="math inline"><em>p</em>_<em>g</em>(<em>x</em>)</span>的映射，覆盖真实分布 <spanclass="math inline"><em>p</em>_<em>d</em><em>a</em><em>t</em><em>a</em>(<em>x</em>)</span>。</p><p><strong>关键点</strong>：<br />- 噪声输入的随机性确保了生成数据分布的多样性。<br />-网络的非线性能力允许从简单分布（如高斯分布）逼近复杂分布（如图像像素分布）。</p><p>但是GAN的<strong>训练过程是非常困难</strong>的，梯度性质是不好的，因为在比较好的样本中，由于梯度性质的问题会进行较大的更新。</p><figure><img src="/images/Pasted%20image%2020250330100434.png"alt="Pasted image 20250330100434.png" /><figcaption aria-hidden="true">Pasted image20250330100434.png</figcaption></figure><p><strong>训练GAN的技巧</strong> <imgsrc="/images/Pasted%20image%2020250330100506.png"alt="Pasted image 20250330100506.png" /></p><h4 id="dcgan-deep-convolutional-generative-adversarial-networks">DCGAN:Deep Convolutional Generative Adversarial Networks</h4><p>对于生成器和判别器，使用卷积神经网络来进行训练。对于判别器，使用的是较为标准的CNN网络，对于生成器，使用的是转置卷积，先将特征图进行padding，然后进行卷积操作，这样可以获得一个较大的特征图。</p><ul><li><strong>生成器</strong>：使用转置卷积（反卷积）逐步上采样，生成高分辨率图像。</li><li><strong>判别器</strong>：使用标准CNN逐步下采样。</li><li><strong>关键技巧</strong>：批量归一化、Leaky ReLU、全卷积结构。</li></ul><figure><img src="/images/Pasted%20image%2020250330102359.png"alt="Pasted image 20250330102359.png" /><figcaption aria-hidden="true">Pasted image20250330102359.png</figcaption></figure><p>证明了泛化定理：在有限的训练样本之下，可以通过训练得到一个泛化的模型。</p><h4 id="inception-score">Inception Score</h4><p>IS 用于衡量生成模型的性能，重点关注两点：<br />-<strong>类别明确性</strong>：单个生成样本应明确属于某个类别（对应分类概率尖锐）。<br />- <strong>多样性</strong>：生成样本应覆盖多个类别（类别分布均匀）。</p><p><span class="math display">$$\\text{IS} = \\exp\\left(\\mathbb{E}\_{x \\sim p\_g} \\left\[\\text{KL}\\left(p(y|x) \\parallel p(y)\\right) \\right\]\\right)$$</span></p><ul><li>Class Probability Distribution: <spanclass="math inline"><em>p</em>(<em>y</em>|<em>x</em>)</span>，生成样本<span class="math inline"><em>x</em></span>属于各个类别的概率，度量的是生成的数据的类别分布和真实数据的类别分布的相似性。</li><li>Marginal Distribution of Generated Data: <spanclass="math inline"><em>p</em>(<em>y</em>)</span>，度量的是生成的数据的类别分布的多样性，如果生成的数据是单一的称为模式坍塌。</li></ul><h5 id="kl-divergence">KL Divergence</h5><p>使用KL散度来度量两个分布的相似性： <span class="math display">$$KL(p||q) =\\mathbb{E}\_X \\left(  \\log \\frac{p(x)}{q(x)}\\right)=\\sum\_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)}$$</span> 需要上面的值尽可能偏大</p><ul><li><strong>KL散度的意义</strong>：<ul><li><strong><spanclass="math inline"><em>p</em>(<em>y</em>|<em>x</em>)</span>尖锐</strong> → 分类概率集中（如某类概率接近 1），此时 <spanclass="math inline">$\\text{KL}(p(y|x) \\parallel p(y))$</span>值大。<br /></li><li><strong><span class="math inline"><em>p</em>(<em>y</em>)</span>均匀</strong> → 生成样本覆盖所有类别，<spanclass="math inline">$\\text{KL}$</span> 值的期望更大。<br /></li></ul></li><li><strong>取指数的作用</strong>：将对数空间的值转换为正数，放大差异便于比较。</li></ul><h4 id="fid-frechet-inception-distance">FID: Frechet InceptionDistance</h4><p>将生成的数据输入一个网络来提取特征，用得到的特征来拟和两个高斯分布，然后计算两个高斯分布的<em>FrechetInception Distance</em>。这是有显式表达式的。</p><p><span class="math display">$$\\text{FID} = \\|\\mu\_r - \\mu\_g\\|^2 + \\text{Tr}\\left(\\Sigma\_r +\\Sigma\_g - 2(\\Sigma\_r \\Sigma\_g)^{1/2}\\right)$$</span> - <strong><span class="math inline">$\\mu\_r,\\mu\_g$</span></strong>：真实数据和生成数据特征的均值向量。<br />- <strong><span class="math inline">$\\Sigma\_r,\\Sigma\_g$</span></strong>：真实数据和生成数据特征的协方差矩阵。<br />- <strong><spanclass="math inline">$\\text{Tr}(\\cdot)$</span></strong>：矩阵的迹（对角线元素之和）。</p><h4 id="mode-collapse">Mode Collapse</h4><p>指生成的数据的多样性不够，类别分布是单一的，这是GAN的一个问题。</p><p>原始GAN使用JS散度（<span class="math inline">$JSD(p\_{data}\\parallel p\_g)$</span>）衡量分布距离，存在： -<strong>梯度消失</strong>：当 <spanclass="math inline"><em>p</em>_<em>g</em></span> 和 <spanclass="math inline"><em>p</em>_<em>d</em><em>a</em><em>t</em><em>a</em></span>无重叠时，<span class="math inline">$JSD = \\log 2$</span>，梯度为零。 -<strong>模式坍塌</strong>：生成器倾向于生成少数样本。</p><h5 id="wasserstein-distance">Wasserstein Distance</h5><p><ahref="https://zh.wikipedia.org/wiki/%E6%B2%83%E7%91%9F%E6%96%AF%E5%9D%A6%E5%BA%A6%E9%87%8F">沃瑟斯坦度量- 维基百科，自由的百科全书</a></p><p>有可能统计距离是不是一个好的距离度量。Wasserstein距离是一个好的距离度量，用推土机距离来度量两个分布的距离。</p><p><span class="math display">$$ c(x,y) \\mapsto \[0, \\infty),$$</span> 表示从点<spanclass="math inline"><em>x</em></span>运输质量到点<spanclass="math inline"><em>y</em></span>的代价。一个从<spanclass="math inline">$\\mu$</span>到<spanclass="math inline">$\\nu$</span>的运输方案可以用函数<spanclass="math inline">$\\gamma(x,y)$</span>来描述，该函数表明从<spanclass="math inline"><em>x</em></span>移动到<spanclass="math inline"><em>y</em></span>的质量。一个运输方案<spanclass="math inline">$\\gamma(x,y)$</span>必须满足以下性质： <spanclass="math display">$$\\begin{aligned}\\int \\gamma(x,y) \\, \\mathrm{d}y = \\mu(x), \\\\\\int \\gamma(x,y) \\, \\mathrm{d}x = \\nu(y),\\end{aligned}$$</span></p><p>前者表示从某一点<spanclass="math inline"><em>x</em></span>移到其他所有点的土堆总质量必须等于最初该点<spanclass="math inline"><em>x</em></span>上的土堆质量，后者则表示从所有点移到某一点<spanclass="math inline"><em>y</em></span>的土堆总质量必须等于最终该点<spanclass="math inline"><em>y</em></span>上的土堆质量。 <spanclass="math display">$$\\iint c(x,y) \\gamma(x,y) \\, \\mathrm{d}x \\, \\mathrm{d}y = \\intc(x,y) \\, \\mathrm{d}\\gamma(x,y).$$</span></p><p>方案<spanclass="math inline">$\\gamma$</span>并不是唯一的，所有可能的运输方案中代价最低的方案即为最优运输方案。最优运输方案的代价为：</p><p><span class="math display">$$C = \\inf\_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int c(x,y) \\,\\mathrm{d}\\gamma(x,y).$$</span> ##### Wasserstein GAN</p><figure><img src="/images/Pasted%20image%2020250330141642.png"alt="Pasted image 20250330141642.png" /><figcaption aria-hidden="true">Pasted image20250330141642.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250330142308.png"alt="Pasted image 20250330142308.png" /><figcaption aria-hidden="true">Pasted image20250330142308.png</figcaption></figure><p>对于模式坍塌进一步的理解，如果空间上有一个较大的利普希茨系数，那么说明发生了模式坍塌。</p><figure><img src="/images/Pasted%20image%2020250330144259.png"alt="Pasted image 20250330144259.png" /><figcaption aria-hidden="true">Pasted image20250330144259.png</figcaption></figure><h5 id="spectral-normalization">Spectral Normalization</h5><figure><img src="/images/Pasted%20image%2020250330144556.png"alt="Pasted image 20250330144556.png" /><figcaption aria-hidden="true">Pasted image20250330144556.png</figcaption></figure><table><thead><tr><th>指标</th><th>JS散度</th><th>Wasserstein距离</th></tr></thead><tbody><tr><td><strong>连续性</strong></td><td>不连续（梯度消失）</td><td>连续</td></tr><tr><td><strong>对称性</strong></td><td>对称</td><td>对称</td></tr><tr><td><strong>计算复杂度</strong></td><td>低</td><td>高（需约束判别器）</td></tr></tbody></table><h4 id="conditional-gan">Conditional GAN</h4><p>给定一个类来进行生成数据，这样可以生成不同类别的数据，这样可以生成更加多样的数据。并且可以在一定程度上避免模式坍塌。除了接受高斯噪声还接受一个标签/图像等等作为输入。</p><h4 id="acgan-auxiliary-classifier-gan">ACGAN: Auxiliary ClassifierGAN</h4><p>是多任务学习的一种方法，除了生成数据，还可以进行分类。在生成数据的过程中还生成标签，所以可以一定程度上避免模式坍塌。</p><h4 id="cycle-gan">Cycle GAN</h4><figure><img src="/images/Pasted%20image%2020250330153629.png"alt="Pasted image 20250330153629.png" /><figcaption aria-hidden="true">Pasted image20250330153629.png</figcaption></figure><p>无配对数据下的图像转换（如马→斑马），通过循环一致性损失<spanclass="math inline"><em>C</em><em>y</em><em>c</em></span>保证生成的图像在两个方向上的转换是一致的。### Self-Attention GAN</p><p>将<em>Self-Attention</em>机制应用到GAN中，这样可以使得生成的数据更加真实。</p><h4 id="adaptive-instance-normalization">Adaptive InstanceNormalization</h4><p><span class="math display">$$AdaIN(u ,v) = \\sigma(v) \\left(\\frac{u - \\mu(u)}{\\sigma(u)}\\right)+ \\mu(v)$$</span>本质上为重新着色的操作，将一个图像的风格转移到另一个图像上。</p><h4 id="stylegan">StyleGAN</h4><p>通过控制风格来生成数据，这样可以生成更加多样的数据。</p><h3 id="vae">VAE</h3><h4 id="encoder">Encoder</h4><ol type="1"><li><strong>推断潜在变量分布</strong><br />编码器将输入数据（如图像、文本）映射到潜在空间<em>latentspace</em>，输出潜在变量<spanclass="math inline"><em>z</em></span>的概率分布参数（通常是高斯分布的均值和方差）。这一步称为<strong>变分推断</strong>，目的是找到输入数据在低维潜在空间中的概率表示。</li><li><strong>数据压缩与特征提取</strong><br />编码器将高维输入数据压缩到低维潜在变量<spanclass="math inline"><em>z</em></span>，提取数据的关键特征（如形状、颜色等抽象属性），同时去除冗余信息。</li><li><strong>引入不确定性</strong><br />不同于传统自编码器的确定性编码，VAE的编码器输出的是分布的参数，通过随机采样生成 zz，使得潜在空间具有连续性，便于生成新样本。</li></ol><h4 id="decoder">Decoder</h4><ol type="1"><li><strong>数据生成</strong><br />解码器从潜在变量<spanclass="math inline"><em>z</em></span>出发，重构输入数据<spanclass="math inline"><em>x</em></span>的分布（如像素值的伯努利分布或高斯分布），生成与原始数据相似的新样本。</li><li><strong>潜在空间映射到数据空间</strong><br />解码器学习如何将低维潜在变量<spanclass="math inline"><em>z</em></span>解码为高维数据空间中的样本，捕捉数据生成过程的规律（如像素间的依赖关系）。</li><li><strong>生成多样性</strong><br />由于潜在变量<spanclass="math inline"><em>z</em></span>是连续且概率化的，解码器可以在潜在空间中插值或随机采样，生成多样化且合理的新数据。</li></ol><h4 id="why-variational">Why Variational</h4><p>其核心目标是通过学习数据分布<spanclass="math inline"><em>p</em>(<em>x</em>)</span>，生成与训练数据类似的新样本。具体来说，VAE旨在解决以下问题：- <strong>生成新数据</strong>：例如生成图像、文本或音频。 -<strong>学习潜在表示</strong>：将高维数据映射到低维潜在空间，同时保持数据的语义特征。- <strong>概率建模</strong>：显式定义数据的生成过程<spanclass="math inline">$p\_{\\theta} (x|z)$</span>，并引入潜在变量<spanclass="math inline"><em>z</em></span>表示数据的隐含因素</p><p>在VAE中，我们引入一个潜在变量 <spanclass="math inline"><em>z</em></span>，假设数据 <spanclass="math inline"><em>x</em></span> 是由某个先验分布 <spanclass="math inline">$p\_\\theta(z)$</span> 生成，然后通过条件分布 <spanclass="math inline">$p\_\\theta(x|z)$</span> 生成可观测数据 <spanclass="math inline"><em>x</em></span>。 <span class="math display">$$p\_\\theta (x) = \\int p\_\\theta(x|z) p\_\\theta(z) \\, \\mathrm{d}z$$</span> <spanclass="math inline"><em>z</em></span>的分布是一个高斯分布，对于<spanclass="math inline"><em>z</em></span>采样得到的实例，通过一个网络生成<spanclass="math inline"><em>x</em></span>。困难在于上面的反常积分，这是一个高维积分，不可行。可以使用<em>蒙特卡洛</em>。</p><p>对于后验分布： <span class="math display">$$p\_\\theta(z|x) = \\frac{p\_\\theta(x|z) p\_\\theta(z)}{p\_\\theta(x)}$$</span>在积分中是经常使用的，但是计算是NP-hard的，因此引入<strong>变分推断</strong>，通过优化下界（ELBO）间接逼近。直接求后验往往是不可行的。因此，我们用<strong>变分推断</strong>的方式，去学习一个近似后验分布<spanclass="math inline">$q\_{\\phi}(z∣x)$</span>，并用它来逼近真正的后验分布<spanclass="math inline">$p\_{\\theta}(z∣x)$</span>，从而得到一个变分下界。</p><p>变分的意思为变量的替换，在概率中为分布率的替换。对于这个问题，可以使用一个神经网络来实现，对于这种显式的分布，需要假设分布率，可以指定为高斯分布，用网络来学习均值和协方差，作为<em>EncoderNetwork</em>。 对于条件分布<spanclass="math inline">$p\_\\theta(x|z)$</span>，也可以假定为高斯分布，这样可以计算均值和协方差，作为<em>DecoderNetwork</em>。</p><figure><img src="/images/Pasted%20image%2020250330161028.png"alt="Pasted image 20250330161028.png" /><figcaption aria-hidden="true">Pasted image20250330161028.png</figcaption></figure><p>但是VAE是<em>Intractable</em>的，这个问题是NP-hard的。对于上面的条件概率，是比较困难的。所以采用一个近似的方法来进行求解。比如假设服从一个高斯分布，之后计算这个分布的均值和协方差矩阵。</p><h4 id="elbo-evidence-lower-bound">ELBO: Evidence Lower Bound</h4><p>训练的目的是使得<span class="math inline">$\\log p\_\\theta(x\_i)$</span>尽可能大，但是很难计算，这里引入“参考系”，也就是引入一个近似的分布<spanclass="math inline">$q\_\\phi(z|x\_i)$</span>。</p><figure><img src="/images/Pasted%20image%2020250330164549.png"alt="Pasted image 20250330164549.png" /><figcaption aria-hidden="true">Pasted image20250330164549.png</figcaption></figure><p>这里是将最大化似然的过程简化为最大化ELBO的过程。 <spanclass="math display">$$\\log p\_\\theta (x\_i) = \\mathbb{E}\_{q\_\\phi(z|x\_i)} \\left\[ \\logp\_\\theta(x\_i|z) \\right\] - KL (q\_\\phi(z|x\_i) || p\_\\theta(z))$$</span></p><h4 id="reparameterization-trick">Reparameterization Trick</h4><figure><img src="/images/Pasted%20image%2020250403144444.png"alt="Pasted image 20250403144444.png" /><figcaption aria-hidden="true">Pasted image20250403144444.png</figcaption></figure><p>在VAE的训练过程中，通常需要从潜在变量的分布中采样。直接从分布中采样可能导致梯度无法传播到编码器网络，因此引入了重参数化技巧。这是的采样变量是<spanclass="math inline">$\\epsilon$</span>，这样的话采样的过程就在计算图的旁路上，这样就可以进行梯度的传播。#### VAE Inference</p><p>在训练的时候是从<spanclass="math inline"><em>x</em></span>中采样得到的，然后由后验分布<spanclass="math inline">$q\_\\phi(z|x)$</span>来进行采样。而在推理过程中，是从潜在变量<spanclass="math inline"><em>z</em></span>中采样得到的，然后由条件分布<spanclass="math inline">$p\_\\theta(x|z)$</span>来进行进行推理。上述过程的合理性在于目标函数ELBO中的第二项：<spanclass="math inline">$KL (q\_\\phi(z|x\_i) ||p\_\\theta(z))$</span>在训练中被最小化了。</p><figure><img src="/images/Pasted%20image%2020250403152841.png"alt="Pasted image 20250403152841.png" /><figcaption aria-hidden="true">Pasted image20250403152841.png</figcaption></figure><h4 id="vq-vae">VQ-VAE</h4><p>VQ-VAE是对VAE的一个改进，使用了向量量化的方法来进行训练。通过对潜在变量进行离散化来进行训练，这样可以避免模式坍塌的问题。</p><h3 id="diffusion-probabilistic-models">Diffusion ProbabilisticModels</h3><h4 id="denoising-diffusion-probabilistic-models">Denoising DiffusionProbabilistic Models</h4><p>Diffusion模型是通过对数据进行逐步添加噪声来训练模型的。通过对数据进行逐步添加噪声，然后再通过一个网络来进行去噪声的操作。这样可以生成新的数据。</p><p>Markovian Process: 逐步添加噪声的过程，通常是一个高斯分布的过程。<span class="math display">$$q(x\_t | x\_{t-1}) = \\mathcal{N}(x\_t; \\sqrt{1-\\beta\_t}  x\_{t-1},\\beta\_t I)$$</span> 联合分布： <span class="math display">$$q(x\_{1:T} | x\_0) = \\prod\_{t=1}^{T} q(x\_t | x\_{t-1})$$</span></p><p>重参数化表达： <span class="math display">$$x\_t = \\sqrt{1-\\beta\_t} x\_{t-1} + \\sqrt{\\beta\_t} \\epsilon \\quad\\epsilon \\sim \\mathcal{N}(0, I)$$</span></p><h5 id="diffusion-kernel">Diffusion Kernel</h5><p><span class="math display">$$\\begin{aligned}x\_t &amp; = \\sqrt{1-\\beta\_t} x\_{t-1} + \\sqrt{\\beta\_t} \\epsilon\\\\&amp;= \\sqrt{\\alpha\_t} x\_{t-1} + \\sqrt{1-\\alpha\_t} \\epsilon \\\\&amp;= \\sqrt{\\alpha\_t \\alpha\_{t-1}} x\_{t-2} + \\sqrt{1-\\alpha\_t}\\epsilon + \\sqrt{\\alpha\_t} \\sqrt{1-\\alpha\_{t-1}} \\epsilon' \\\\&amp; = \\sqrt{\\alpha\_t \\alpha\_{t-1}} x\_{t-2} + \\sqrt{1-  \\alpha\_t \\alpha\_{t-1}} \\epsilon \\\\&amp; \\dots \\\\&amp; = \\sqrt{\\overline{\\alpha\_t}} x\_0 + \\sqrt{1 -\\overline{\\alpha\_t}} \\epsilon\\end{aligned}$$</span> - 其中 <span class="math inline">$\\overline{\\alpha\_t} =\\prod\_{s=1}^{t} \\alpha\_s$</span></p><h5 id="generation-by-denoising">Generation by Denoising</h5><ul><li>首先对于<spanclass="math inline"><em>x</em>_<em>T</em></span>进行采样，得到一个高斯分布的样本。</li><li>然后利用后验分布得到<spanclass="math inline"><em>x</em>_<em>t</em> − 1</span>，使用Bayes公式：<span class="math inline">$x\_{t-1} \\sim q(x\_{t-1}|x\_t) \\proptoq(x\_{t-1}) q(x\_t |x\_{t-1})$</span></li></ul><p>采用一个变分方法来实现： <span class="math display">$$p\_{\\theta}(x\_{t-1}|x\_t) = \\mathcal{N} \\left(x\_{t-1};\\mu\_{\\theta}(x\_t, t), \\sigma\_t^2 \\mathbf{I} \\right)$$</span> <span class="math display">$$p\_\\theta(x\_{0:T}) = p\_\\theta (x\_T)\\prod\_{t=1}^{T}p\_\\theta(x\_{t-1}|x\_t)$$</span></p><p>上述过程可以使用一个网络来实现对于均值和方差的输出。</p><figure><img src="/images/Pasted%20image%2020250403164408.png"alt="Pasted image 20250403164408.png" /><figcaption aria-hidden="true">Pasted image20250403164408.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250403165731.png"alt="Pasted image 20250403165731.png" /><figcaption aria-hidden="true">Pasted image20250403165731.png</figcaption></figure><p><ahref="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models? | Lil’Log</a></p><figure><img src="/images/Pasted%20image%2020250405094553.png"alt="Pasted image 20250405094553.png" /><figcaption aria-hidden="true">Pasted image20250405094553.png</figcaption></figure><h5 id="diffusion-parameters">Diffusion Parameters</h5><figure><img src="/images/Pasted%20image%2020250405161801.png"alt="Pasted image 20250405161801.png" /><figcaption aria-hidden="true">Pasted image20250405161801.png</figcaption></figure><h5 id="acceleration-strategies">Acceleration Strategies</h5><h5 id="re-design-forward-sampling">Re-design forward Sampling</h5><ul><li>Striding Sampling<ul><li>在每个时间步长中跳过多个时间步进行采样，从而减少采样次数。</li><li>问题在于在相邻的时间戳中反向传播的后验分布可以近似为高斯分布，但是在较大的步长下不一定是这样的。</li></ul></li><li>DDIM: Denoising Diffusion Implicit Models<ul><li>通过设计一个新的前向采样过程来加速采样。</li><li>通过引入一个新的参数<spanclass="math inline">$\\alpha\_t$</span>来控制前向采样的过程。</li><li>通过设计一个新的后验分布来进行采样。</li><li>通过设计一个新的损失函数来进行训练。</li><li><figure><img src="/images/Pasted%20image%2020250405163042.png"alt="Pasted image 20250405163042.png" /><figcaption aria-hidden="true">Pasted image20250405163042.png</figcaption></figure></li></ul></li><li>Denosing Diffusion Models<ul><li><figure><img src="/images/Pasted%20image%2020250405163350.png"alt="Pasted image 20250405163350.png" /><figcaption aria-hidden="true">Pasted image20250405163350.png</figcaption></figure></li></ul></li><li>Latent Diffusion Models<ul><li><figure><img src="/images/Pasted%20image%2020250405163425.png"alt="Pasted image 20250405163425.png" /><figcaption aria-hidden="true">Pasted image20250405163425.png</figcaption></figure></li></ul></li></ul><h5 id="conditonal-diffusion-models">Conditonal Diffusion Models</h5><figure><img src="/images/Pasted%20image%2020250405163914.png"alt="Pasted image 20250405163914.png" /><figcaption aria-hidden="true">Pasted image20250405163914.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250405163941.png"alt="Pasted image 20250405163941.png" /><figcaption aria-hidden="true">Pasted image20250405163941.png</figcaption></figure><figure><img src="/images/Pasted%20image%2020250405164110.png"alt="Pasted image 20250405164110.png" /><figcaption aria-hidden="true">Pasted image20250405164110.png</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Lecture-8</title>
    <link href="/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-8/"/>
    <url>/2025/06/21/Deep%20Learning/Deep%20Learning%20Lecture-8/</url>
    
    <content type="html"><![CDATA[<h2 id="transfer-learning">Transfer Learning</h2><p>模型在较大规模中的数据集中得到成功，在较小的数据集中进进行学习。<strong>利用先验知识</strong>是实现强人工智能的一个前提条件。</p><h2 id="pre-training">Pre-Training</h2><p>预训练的结果是一个模型，然后在小数据集上进行微调。预训练提供的是一个先验知识，然后再进行微调。这已经是深度学习中的共识，在小数据集中做微调是一个非常好的方法。</p><h3 id="supervised-pre-training">Supervised Pre-Training</h3><ul><li>BiT: Big Transfer<ul><li>在大规模数据集上做预训练非常重要</li></ul></li><li>DAT: Domain Adaptative Transfer<ul><li>在预训练的数据集上进行筛选对于模型的表现会更好</li></ul></li><li>迁移性受到数据间距离的影响</li><li>在微调的过程中需要对很多层进行调整</li><li>下游的任务和预训练的任务之间的距离越近，迁移的效果越好</li></ul><p>假设空间的解释： - 预训练的模型能在较小的较好的假设空间上寻找</p><h4 id="multi-task-architecture">Multi-Task Architecture</h4><ul><li>Hard Parameter Sharing<ul><li>直接共享参数</li><li>共享的参数是所有任务都需要的</li></ul></li><li>Soft Parameter Sharing<ul><li>每个任务使用一个模型</li></ul></li><li>TASKONOMY<ul><li>首先进行训练，之后构建任务的关系图，在训练的过程中找一些更相近的任务进行共享</li></ul></li></ul><p>多任务学习的损失函数： <span class="math display">$$\\min\_{\\theta} \\sum\_{i=1}^{T} w\_i \\mathcal{L}\_i(\\theta)$$</span> 其中 <span class="math inline"><em>w</em>_<em>i</em></span>是每个任务的权重 - GradNorm：使得每个任务的梯度相同 - Taskuncertainty：每个任务的损失函数的权重是不同的 - Pareto optimalsolution： - Optimize for the worst task - Regularization：</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
